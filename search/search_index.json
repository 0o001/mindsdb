{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","social":[{"link":"https://github.com/mindsdb/mindsdb","type":"github"},{"link":"https://twitter.com/mindsdb","type":"twitter"},{"link":"https://www.mindsdb.com","type":"link"}]},"docs":[{"location":"","text":"Quickstart \u00b6 Follow the following steps to start predicting in SQL straight away. Check out our Getting Started Guide for trying MindsDB with your data or model. 1. Create an Account \u00b6 Create your free MindsDB Cloud account . Local Installation Follow our Docker instructions . if you prefer to proceed with a local installation. 2. Connect MindsDB to a MySQL Client \u00b6 You can use the MindsDB SQL Editor or open your preferred MySQL client and connect it to MindsDB. Using the MindsDB SQL Editor Connecting to a Third-party MySQL Client Just log in to your account, and you will be automatically directed to the Editor . To connect to MindsDB from another SQL client use cloud.mindsdb.com as a host, 3306 port and your MindsDB Cloud credentials for username/password. \"user\":[your_mindsdb_cloud_username], \"password:\"[your_mindsdb_cloud_password]\", \"host\":\"cloud.mindsdb.com\", \"port\":\"3306\" If you do not already have a preferred SQL client, we recommend DBeaver Community Edition . 3. Connecting a Database CREATE DATABASE \u00b6 For this quickstart, we have already prepared some example data for you. To add it to your account, use the CREATE DATABASE syntax by copying and pasting this command into your SQL client: CREATE DATABASE example_data WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } On execution, you should get: Query OK , 0 rows affected ( 3 . 22 sec ) 4. Previewing Available Data \u00b6 You can now preview the available data with a standard SELECT . To preview the Home Rentals dataset, copy and paste this command into your SQL client: SELECT * FROM example_data . demo_data . home_rentals LIMIT 10 ; On execution, you should get: + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | 0 . 0 | 1 . 0 | 484 | great | 10 | 2271 | south_side | 2271 | | 1 . 0 | 1 . 0 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 . 0 | 1 . 0 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 . 0 | 1 . 0 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 . 0 | 2 . 0 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 . 0 | 1 . 0 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 . 0 | 2 . 0 | 1190 | poor | 58 | 4463 | westbrae | 4124 | | 1 . 0 | 1 . 0 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 . 0 | 1 . 0 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 . 0 | 1 . 0 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ 5. Creating a Predictor CREATE PREDICTOR \u00b6 Now you are ready to create your first predictor. Use the CREATE PREDICTOR syntax by copying and pasting this command into your SQL client: CREATE PREDICTOR mindsdb . home_rentals_predictor FROM example_data ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; Query OK , 0 rows affected ( 9 . 79 sec ) 6. Checking the Status of a Predictor \u00b6 A predictor may take a couple of minutes for the training to complete. You can monitor the status of your predictor by copying and pasting this command into your SQL client: SELECT status FROM mindsdb . predictors WHERE name = 'home_rentals_predictor' ; On execution, you should get: + ----------+ | status | + ----------+ | training | + ----------+ Or: + ----------+ | status | + ----------+ | complete | + ----------+ Predictor Status Must be 'complete' Before Making a Prediction 7. Making a Prediction via SELECT \u00b6 The SELECT syntax will allow you to make a prediction based on features. Make your first prediction by copying and pasting this command into your SQL client: SELECT rental_price FROM mindsdb . home_rentals_predictor WHERE number_of_bathrooms = 2 AND sqft = 1000 ; On execution, you should get: + --------------+ | rental_price | + --------------+ | 1130 | + --------------+ Congratulations If you got this far, you have trained a predictive model using SQL and have used it to tell the future!","title":"Quickstart"},{"location":"#quickstart","text":"Follow the following steps to start predicting in SQL straight away. Check out our Getting Started Guide for trying MindsDB with your data or model.","title":"Quickstart"},{"location":"#1-create-an-account","text":"Create your free MindsDB Cloud account . Local Installation Follow our Docker instructions . if you prefer to proceed with a local installation.","title":"1. Create an Account"},{"location":"#2-connect-mindsdb-to-a-mysql-client","text":"You can use the MindsDB SQL Editor or open your preferred MySQL client and connect it to MindsDB. Using the MindsDB SQL Editor Connecting to a Third-party MySQL Client Just log in to your account, and you will be automatically directed to the Editor . To connect to MindsDB from another SQL client use cloud.mindsdb.com as a host, 3306 port and your MindsDB Cloud credentials for username/password. \"user\":[your_mindsdb_cloud_username], \"password:\"[your_mindsdb_cloud_password]\", \"host\":\"cloud.mindsdb.com\", \"port\":\"3306\" If you do not already have a preferred SQL client, we recommend DBeaver Community Edition .","title":"2. Connect MindsDB to a MySQL Client"},{"location":"#3-connecting-a-database-create-database","text":"For this quickstart, we have already prepared some example data for you. To add it to your account, use the CREATE DATABASE syntax by copying and pasting this command into your SQL client: CREATE DATABASE example_data WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } On execution, you should get: Query OK , 0 rows affected ( 3 . 22 sec )","title":"3. Connecting a Database CREATE DATABASE"},{"location":"#4-previewing-available-data","text":"You can now preview the available data with a standard SELECT . To preview the Home Rentals dataset, copy and paste this command into your SQL client: SELECT * FROM example_data . demo_data . home_rentals LIMIT 10 ; On execution, you should get: + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | 0 . 0 | 1 . 0 | 484 | great | 10 | 2271 | south_side | 2271 | | 1 . 0 | 1 . 0 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 . 0 | 1 . 0 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 . 0 | 1 . 0 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 . 0 | 2 . 0 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 . 0 | 1 . 0 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 . 0 | 2 . 0 | 1190 | poor | 58 | 4463 | westbrae | 4124 | | 1 . 0 | 1 . 0 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 . 0 | 1 . 0 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 . 0 | 1 . 0 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+","title":"4. Previewing Available Data"},{"location":"#5-creating-a-predictor-create-predictor","text":"Now you are ready to create your first predictor. Use the CREATE PREDICTOR syntax by copying and pasting this command into your SQL client: CREATE PREDICTOR mindsdb . home_rentals_predictor FROM example_data ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; Query OK , 0 rows affected ( 9 . 79 sec )","title":"5. Creating a Predictor CREATE PREDICTOR"},{"location":"#6-checking-the-status-of-a-predictor","text":"A predictor may take a couple of minutes for the training to complete. You can monitor the status of your predictor by copying and pasting this command into your SQL client: SELECT status FROM mindsdb . predictors WHERE name = 'home_rentals_predictor' ; On execution, you should get: + ----------+ | status | + ----------+ | training | + ----------+ Or: + ----------+ | status | + ----------+ | complete | + ----------+ Predictor Status Must be 'complete' Before Making a Prediction","title":"6. Checking the Status of a Predictor"},{"location":"#7-making-a-prediction-via-select","text":"The SELECT syntax will allow you to make a prediction based on features. Make your first prediction by copying and pasting this command into your SQL client: SELECT rental_price FROM mindsdb . home_rentals_predictor WHERE number_of_bathrooms = 2 AND sqft = 1000 ; On execution, you should get: + --------------+ | rental_price | + --------------+ | 1130 | + --------------+ Congratulations If you got this far, you have trained a predictive model using SQL and have used it to tell the future!","title":"7. Making a Prediction via SELECT"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/","text":"Getting Started with MindsDB on Digital Ocean \u00b6 Introduction \u00b6 MindsDB has the vision to enforce advanced predictive capabilities directly in our Database. With this, anyone who knows SQL can utilize the built-in machine learning techniques without needing any other outsourced tools or training. In short, MindsDB aims to provide intelligent databases to make better data-driven decisions in the existing database without the effort of creating a new one. Deployment on Digital Ocean \u00b6 Digital Ocean is a Cloud computing platform that provides cloud computing services with predictable pricing, developer-friendly features, and scalability as you need. Today, we will be deploying the MindsDB server on a Digital Ocean Droplet (a scalable virtual machine on DO Platform) and trying to connect to it using our local MySQL client or Dbeaver. Step 1: Sign up for a Digital Ocean account if you don't have one yet. They provide 100$ Credits when you sign up so that you can try it for free. Step 2: Once you've created an account, you should see a dashboard like this. If you look towards the left panel, you can find the Droplets option. Now click on Droplets. Step 3: Once you are in the Droplets Dashboard, hit Create Droplet . Step 4: Now the Create Droplet dashboard opens. Here you need to choose the configurations you want for your virtual machine and can also select any additional applications you want to install on your VM from the Marketplace. For now, we will move ahead with the following configurations that would boot up a standard VM for us with the required performance. Distributions : Ubuntu 20.04(LTS) x64 Choose a Plan : Shared CPU (Basic) CPU Options : $20/mo (4GB/2CPUs, 80GB SSD Disk, 4TB Transfer) Datacenter Region : Closest Region to you to reduce Latency (Bangalore for me) Authentication : SSH Key (If you have one) or Root Password (Set your Root Password below to login to the VM) Select Additional Option : Choose Monitoring and IPv6 which are free services Finalize and Create : Set the number of Droplets you want (1 for now) and the Name of Droplet Select a Project : Leave it at default for now Or you can simply go to the top and select Marketplace and search for Docker there and select Docker on Ubuntu directly, as we would be using Docker to install the MindsDB server on our Droplet, instead of choosing a Ubuntu LTS distribution and then installing docker on top of it manually. Hit Create Droplet and wait for a while till it spins one for us in the cloud. Step 5: Once your Droplet is created, you can always click on the Droplets in the left Navigation pane and Click on the Droplet name to go into the Droplet Dashboard details. Now click on the Console from the top right corner. This should open up a terminal for you where you can interact with your VM. Step 6: Now while you are inside the console, simply check whether Docker is installed or not using the following command. docker run hello-world If the command successfully returns Hello From Docker , then you're good to go. Now we start with the installation of MindsDB on our Droplet. Step 7: Now we will use the following commands in sequence to get done with the installation of MindDB Sever. docker pull mindsdb/mindsdb This should pull the latest Production image to our Droplet. You can also use the latest Beta version as well using the following command. docker pull mindsdb/mindsdb_beta Step 8: After that, we need to publish the ports so that we can hit the endpoints and communicate with the MindsDB Server in our Droplet. So, we will first expose the MindsDB GUI Port and then the MySQL Port that we will be using for now with the commands below. MindsDB GUI with MySQL API docker run -p 47334:47334 -p 47335:47335 mindsdb/mindsdb You can always follow the MindsDB Getting Started documentation if you need further information or want to install MindsDB by any other means other than Docker. Step 9: You might see a warning on the console after you run the above command that some data sources may not be available and you can always find details on how to add the specific data source on the MindDB Repo . For now, we will add the data source for MSSQL using the command below. Open another console for the droplet and run it if MindsDB is still running in the first console. pip install mindsdb-datasources[mssql] NOTE: You have to install python first to be able to use pip . Run the following to get it installed. apt install python3-pip You may encounter a few errors with regards to the versioning of pandas and scramp which you can ignore. Step 10: Now you can access the MindsDB GUI using the Droplet IPv4:47334 . Make sure you have turned off any VPN service you're using or else you may not be able to connect to the GUI using the URL. Step 11: This step enables us to connect a database client to our MindsDB server. For now, I would be using DBeaver to do the same. You can also connect your local MySQL server following the steps here . NOTE: You should follow the local deployment steps instead of cloud as this is a local instance hosted on the cloud and not the official MindsDB cloud edition. The default username is mindsdb and the default password is empty. We will first need to define the connection settings in Dbeaver before we can connect it. Always make sure you're using MySQL only or MySQL8 or higher from the available options. Click on Next and fill in the following details in the upcoming screen. Hostname : Your Droplet IPv4 Port : 47335 Username : mindsdb Password : Leave it empty. Now hit the Test Connection and once it returns success, click on Finish . NOTE: If you're running it for the first time, it may ask you to download some additional driver files. Check Force Download/Overwrite and hit `Download to proceed. Tables in MindsDB \u00b6 Once connected to the mindsdb database, it will contain 3 tables predictors , commands and datasources . You can simply run a query show tables; to see these as shown in the snippet above. The Predictors table contains all the newly trained ML models as a new record. Each column in the Predictors table contains information about each of these models. You can always find more information by visiting MindsDB Documentation page. This concludes the tutorial on how to deploy the MindsDB server on a Digital Ocean Droplet and connect to it using a local client. Please drop a like let me know in the comments if you found this useful or have any other suggestions. NOTE: The dashboard images belong to the official platforms of Digital Ocean and MindsDB respectively and are used here only for illustrative purposes. This tutorial has already been published on Dev.to and you find it here","title":"Getting Started with MindsDB on Digital Ocean"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/#getting-started-with-mindsdb-on-digital-ocean","text":"","title":"Getting Started with MindsDB on Digital Ocean"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/#introduction","text":"MindsDB has the vision to enforce advanced predictive capabilities directly in our Database. With this, anyone who knows SQL can utilize the built-in machine learning techniques without needing any other outsourced tools or training. In short, MindsDB aims to provide intelligent databases to make better data-driven decisions in the existing database without the effort of creating a new one.","title":"Introduction"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/#deployment-on-digital-ocean","text":"Digital Ocean is a Cloud computing platform that provides cloud computing services with predictable pricing, developer-friendly features, and scalability as you need. Today, we will be deploying the MindsDB server on a Digital Ocean Droplet (a scalable virtual machine on DO Platform) and trying to connect to it using our local MySQL client or Dbeaver. Step 1: Sign up for a Digital Ocean account if you don't have one yet. They provide 100$ Credits when you sign up so that you can try it for free. Step 2: Once you've created an account, you should see a dashboard like this. If you look towards the left panel, you can find the Droplets option. Now click on Droplets. Step 3: Once you are in the Droplets Dashboard, hit Create Droplet . Step 4: Now the Create Droplet dashboard opens. Here you need to choose the configurations you want for your virtual machine and can also select any additional applications you want to install on your VM from the Marketplace. For now, we will move ahead with the following configurations that would boot up a standard VM for us with the required performance. Distributions : Ubuntu 20.04(LTS) x64 Choose a Plan : Shared CPU (Basic) CPU Options : $20/mo (4GB/2CPUs, 80GB SSD Disk, 4TB Transfer) Datacenter Region : Closest Region to you to reduce Latency (Bangalore for me) Authentication : SSH Key (If you have one) or Root Password (Set your Root Password below to login to the VM) Select Additional Option : Choose Monitoring and IPv6 which are free services Finalize and Create : Set the number of Droplets you want (1 for now) and the Name of Droplet Select a Project : Leave it at default for now Or you can simply go to the top and select Marketplace and search for Docker there and select Docker on Ubuntu directly, as we would be using Docker to install the MindsDB server on our Droplet, instead of choosing a Ubuntu LTS distribution and then installing docker on top of it manually. Hit Create Droplet and wait for a while till it spins one for us in the cloud. Step 5: Once your Droplet is created, you can always click on the Droplets in the left Navigation pane and Click on the Droplet name to go into the Droplet Dashboard details. Now click on the Console from the top right corner. This should open up a terminal for you where you can interact with your VM. Step 6: Now while you are inside the console, simply check whether Docker is installed or not using the following command. docker run hello-world If the command successfully returns Hello From Docker , then you're good to go. Now we start with the installation of MindsDB on our Droplet. Step 7: Now we will use the following commands in sequence to get done with the installation of MindDB Sever. docker pull mindsdb/mindsdb This should pull the latest Production image to our Droplet. You can also use the latest Beta version as well using the following command. docker pull mindsdb/mindsdb_beta Step 8: After that, we need to publish the ports so that we can hit the endpoints and communicate with the MindsDB Server in our Droplet. So, we will first expose the MindsDB GUI Port and then the MySQL Port that we will be using for now with the commands below. MindsDB GUI with MySQL API docker run -p 47334:47334 -p 47335:47335 mindsdb/mindsdb You can always follow the MindsDB Getting Started documentation if you need further information or want to install MindsDB by any other means other than Docker. Step 9: You might see a warning on the console after you run the above command that some data sources may not be available and you can always find details on how to add the specific data source on the MindDB Repo . For now, we will add the data source for MSSQL using the command below. Open another console for the droplet and run it if MindsDB is still running in the first console. pip install mindsdb-datasources[mssql] NOTE: You have to install python first to be able to use pip . Run the following to get it installed. apt install python3-pip You may encounter a few errors with regards to the versioning of pandas and scramp which you can ignore. Step 10: Now you can access the MindsDB GUI using the Droplet IPv4:47334 . Make sure you have turned off any VPN service you're using or else you may not be able to connect to the GUI using the URL. Step 11: This step enables us to connect a database client to our MindsDB server. For now, I would be using DBeaver to do the same. You can also connect your local MySQL server following the steps here . NOTE: You should follow the local deployment steps instead of cloud as this is a local instance hosted on the cloud and not the official MindsDB cloud edition. The default username is mindsdb and the default password is empty. We will first need to define the connection settings in Dbeaver before we can connect it. Always make sure you're using MySQL only or MySQL8 or higher from the available options. Click on Next and fill in the following details in the upcoming screen. Hostname : Your Droplet IPv4 Port : 47335 Username : mindsdb Password : Leave it empty. Now hit the Test Connection and once it returns success, click on Finish . NOTE: If you're running it for the first time, it may ask you to download some additional driver files. Check Force Download/Overwrite and hit `Download to proceed.","title":"Deployment on Digital Ocean"},{"location":"Getting%20Started%20with%20MindsDB%20on%20Digital%20Ocean/#tables-in-mindsdb","text":"Once connected to the mindsdb database, it will contain 3 tables predictors , commands and datasources . You can simply run a query show tables; to see these as shown in the snippet above. The Predictors table contains all the newly trained ML models as a new record. Each column in the Predictors table contains information about each of these models. You can always find more information by visiting MindsDB Documentation page. This concludes the tutorial on how to deploy the MindsDB server on a Digital Ocean Droplet and connect to it using a local client. Please drop a like let me know in the comments if you found this useful or have any other suggestions. NOTE: The dashboard images belong to the official platforms of Digital Ocean and MindsDB respectively and are used here only for illustrative purposes. This tutorial has already been published on Dev.to and you find it here","title":"Tables in MindsDB"},{"location":"community/","text":"Join our community \u00b6 If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace . MindsDB newsletter \u00b6 To get updates on MindsDB\u2019s latest announcements, releases and events, sign up for our newsletter . Become a MindsDB Beta tester \u00b6 If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community . Talk to our engineers \u00b6 If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button. Get in touch for collaboration \u00b6 Contact us by submitting this form .","title":"Join our community"},{"location":"community/#join-our-community","text":"If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace .","title":"Join our community"},{"location":"community/#mindsdb-newsletter","text":"To get updates on MindsDB\u2019s latest announcements, releases and events, sign up for our newsletter .","title":"MindsDB newsletter"},{"location":"community/#become-a-mindsdb-beta-tester","text":"If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community .","title":"Become a MindsDB Beta tester"},{"location":"community/#talk-to-our-engineers","text":"If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button.","title":"Talk to our engineers"},{"location":"community/#get-in-touch-for-collaboration","text":"Contact us by submitting this form .","title":"Get in touch for collaboration"},{"location":"connect/","text":"Connect your data \u00b6 Connect to database \u00b6 From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select the Supported Database that you want to connect to. Add the Integration name(how you want to name the integration between your database and MindsDB). Add the Database name. Add the Hostname. Add Port. Add the database user. Add Password for the user. Click on CONNECT . Required inputs Note: For different type of database there could be different required inputs you need to provide. After connecting MindsDB and the database, use your SQL client to connect to MindsDB as a database .","title":"Connect your data"},{"location":"connect/#connect-your-data","text":"","title":"Connect your data"},{"location":"connect/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select the Supported Database that you want to connect to. Add the Integration name(how you want to name the integration between your database and MindsDB). Add the Database name. Add the Hostname. Add Port. Add the database user. Add Password for the user. Click on CONNECT . Required inputs Note: For different type of database there could be different required inputs you need to provide. After connecting MindsDB and the database, use your SQL client to connect to MindsDB as a database .","title":"Connect to database"},{"location":"contribute/","text":"Contribute to MindsDB \u00b6 Thank you for your interest in contributing to MindsDB. MindsDB is free, open source software, and all types of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes. Contribution issues \u00b6 Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . A great place to start looking will be our GitHub projects for: Community writers dashboard . Community code contributors dashboard . Also, we are always open to suggestions so feel free to open new issues with your ideas and we can give you guidance! After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA. Report issue \u00b6 We use GitHub issues to track bugs and features. Report them by opening a new issue and complete out all of the required inputs: Your Python version , MindsDB version , Describe the bug and Steps to reproduce . Documentation \u00b6 We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the documentation tagged issues and help us. Write for us \u00b6 Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you .","title":"How to contribute"},{"location":"contribute/#contribute-to-mindsdb","text":"Thank you for your interest in contributing to MindsDB. MindsDB is free, open source software, and all types of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes.","title":"Contribute to MindsDB"},{"location":"contribute/#contribution-issues","text":"Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . A great place to start looking will be our GitHub projects for: Community writers dashboard . Community code contributors dashboard . Also, we are always open to suggestions so feel free to open new issues with your ideas and we can give you guidance! After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA.","title":"Contribution issues"},{"location":"contribute/#report-issue","text":"We use GitHub issues to track bugs and features. Report them by opening a new issue and complete out all of the required inputs: Your Python version , MindsDB version , Describe the bug and Steps to reproduce .","title":"Report issue"},{"location":"contribute/#documentation","text":"We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the documentation tagged issues and help us.","title":"Documentation"},{"location":"contribute/#write-for-us","text":"Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you .","title":"Write for us"},{"location":"getting-started/","text":"Getting Started \u00b6 MindsDB integrates with the most popular databases and also with the DBT and MLflow workflow you already have. To try MindsDB right away, without bringing your own data or model, check out our Quick Start Guide . Choose your MindsDB installation path. MindsDB Cloud Docker Create your free MindsDB Cloud account . To get started with a Docker installation, begin with our Docker instructions . Open your SQL client and connect to MindsDB. If you do not already have a preferred SQL client, we recommend DBeaver Community Edition MindsDB Cloud Docker Create a new MySQL connection. Configure it using the following parameters, as well as the username and password you created above: Host: mysql.mindsdb.com Port: 3306 Database: mindsdb Create a new MySQL connection. Configure it using the following parameters. Password remains empty. Host: localhost Port: 47335 Database: mindsdb Username: mindsdb Connect your data to MindsDB using the CREATE DATABASE syntax . Example taken from our Quick Start Guide . You can now preview the available data with a standard SELECT . Example taken from our Quick Start Guide . Now you are ready to create your model, using the CREATE PREDICTOR syntax . If you already have a model in MLFlow, you can connect to your model as well. MindsDB is creating my model My model is in MLflow Example taken from our Quick Start Guide . Example taken from our Docker Guide . #create-predictor code { background-color: #353535; color: #f5f5f5 } mysql> CREATE PREDICTOR mindsdb.home_rentals_predictor -> FROM example_data (select * from demo_data.home_rentals) -> PREDICT rental_price -> USING url.predict='http://host.docker.internal:1234/invocations', -> format='mlflow', -> dtype_dict={\"alcohol\": \"integer\", \"chlorides\": \"integer\", \"citric acid\": \"integer\", \"density\": \"integer\", \"fixed acidity\": \"integer\", \"free sulfur dioxide\": \"integer\", \"pH\": \"integer\", \"residual sugar\": \"integer\", \"sulphates\": \"integer\", \"total sulfur dioxide\": \"integer\", \"volatile acidity\": \"integer\"}; Query OK, 0 rows affected (0.21 sec) The SELECT syntax will allow you to make a prediction based on features. Example taken from our Quick Start Guide . To integrate your predictions into your DBT workflow, you will need to make four changes: profiles.yml schema.yml predicted_rentals.sql dbt_project.yml mindsdb: type: mysql host: mysql.mindsdb.com user: mindsdb.user@example.com password: mindsdbpassword port: 3306 dbname: mindsdb schema: example_data threads: 1 keepalives_idle: 0 # default 0, indicating the system default connect_timeout: 10 # default 10 seconds version: 2 models: - name: predicted_rentals description: \"Integrating MindsDB predictions and historical data\" with predictions as ( SELECT hrp.rental_price as predicted_price, hr.rental_price as actual_price FROM mindsdb.home_rentals_predictor hrp JOIN exampleData.demo_data.home_rentals hr WHERE hr.number_of_bathrooms=2 AND hr.sqft=1000; ) select * from predictions; models: home_rentals: +materialized: view","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"MindsDB integrates with the most popular databases and also with the DBT and MLflow workflow you already have. To try MindsDB right away, without bringing your own data or model, check out our Quick Start Guide . Choose your MindsDB installation path. MindsDB Cloud Docker Create your free MindsDB Cloud account . To get started with a Docker installation, begin with our Docker instructions . Open your SQL client and connect to MindsDB. If you do not already have a preferred SQL client, we recommend DBeaver Community Edition MindsDB Cloud Docker Create a new MySQL connection. Configure it using the following parameters, as well as the username and password you created above: Host: mysql.mindsdb.com Port: 3306 Database: mindsdb Create a new MySQL connection. Configure it using the following parameters. Password remains empty. Host: localhost Port: 47335 Database: mindsdb Username: mindsdb Connect your data to MindsDB using the CREATE DATABASE syntax . Example taken from our Quick Start Guide . You can now preview the available data with a standard SELECT . Example taken from our Quick Start Guide . Now you are ready to create your model, using the CREATE PREDICTOR syntax . If you already have a model in MLFlow, you can connect to your model as well. MindsDB is creating my model My model is in MLflow Example taken from our Quick Start Guide . Example taken from our Docker Guide . #create-predictor code { background-color: #353535; color: #f5f5f5 } mysql> CREATE PREDICTOR mindsdb.home_rentals_predictor -> FROM example_data (select * from demo_data.home_rentals) -> PREDICT rental_price -> USING url.predict='http://host.docker.internal:1234/invocations', -> format='mlflow', -> dtype_dict={\"alcohol\": \"integer\", \"chlorides\": \"integer\", \"citric acid\": \"integer\", \"density\": \"integer\", \"fixed acidity\": \"integer\", \"free sulfur dioxide\": \"integer\", \"pH\": \"integer\", \"residual sugar\": \"integer\", \"sulphates\": \"integer\", \"total sulfur dioxide\": \"integer\", \"volatile acidity\": \"integer\"}; Query OK, 0 rows affected (0.21 sec) The SELECT syntax will allow you to make a prediction based on features. Example taken from our Quick Start Guide . To integrate your predictions into your DBT workflow, you will need to make four changes: profiles.yml schema.yml predicted_rentals.sql dbt_project.yml mindsdb: type: mysql host: mysql.mindsdb.com user: mindsdb.user@example.com password: mindsdbpassword port: 3306 dbname: mindsdb schema: example_data threads: 1 keepalives_idle: 0 # default 0, indicating the system default connect_timeout: 10 # default 10 seconds version: 2 models: - name: predicted_rentals description: \"Integrating MindsDB predictions and historical data\" with predictions as ( SELECT hrp.rental_price as predicted_price, hr.rental_price as actual_price FROM mindsdb.home_rentals_predictor hrp JOIN exampleData.demo_data.home_rentals hr WHERE hr.number_of_bathrooms=2 AND hr.sqft=1000; ) select * from predictions; models: home_rentals: +materialized: view","title":"Getting Started"},{"location":"info/","text":"Follow the below steps to get up and running with MindsDB. Steps: \u00b6 Deploy MindsDB: Docker OR Pip OR MindsDB Cloud Connect your Data to MindsDB: Connect to MindsDB like it was a Database using your preferred DB Client Using MindsDB Cloud OR Using a Local Deployment CREATE a predictor (model) trained on your data Make predictions using your model","title":"Getting started"},{"location":"info/#steps","text":"Deploy MindsDB: Docker OR Pip OR MindsDB Cloud Connect your Data to MindsDB: Connect to MindsDB like it was a Database using your preferred DB Client Using MindsDB Cloud OR Using a Local Deployment CREATE a predictor (model) trained on your data Make predictions using your model","title":"Steps:"},{"location":"what-is-mindsdb/","text":"What is MindsDB? \u00b6 Data is your single most important asset and your data lives in a database. By bringing machine learning to the database, MindsDB accelerates the speed of machine learning development. With MindsDB, you will easily build, train, optimize and deploy your models. Your predictions will be available via the queries you already use. What are AI Tables? \u00b6 MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database. Deep Dive into the AI Tables \u00b6 The problem \u00b6 Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; + ------+-----+ | income | debt | + ------+-----+ | 60000 | 20000 | | 80000 | 25100 | | 100000 | 30040 | | 120000 | 36010 | + ------+-----+ A simple visualization of the data present in the income table is as follows: Querying the income table to get the debt value for a particular income value results in the following: SELECT income , debt FROM income_table WHERE income = 80000 ; + ------+-----+ | income | debt | + ------+-----+ | 80000 | 25100 | + ------+-----+ Green dot and dashed line as query result But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; Empty set ( 0 . 00 sec ) In other words, Nothing! no valuable information at all. Dashed red line describing the absense of a value (blue dot) for the query When a table doesn\u2019t have an exact match the query will return an empty set or null value. This is where the AI Tables come into play! The solution \u00b6 Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE PREDICTOR mindsdb . debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows: Green line describing the Predictor (model) created Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the mindsdb . debt_model instead of the income_table . SELECT income , debt FROM mindsdb . debt_model WHERE income = 90000 ; + ------+-----+ | income | debt | + ------+-----+ | 90000 | 27820 | + ------+-----+ Dashed blue line describing the query of the model with the predicted value (dark blue dot) What is MindsDB, why is MindsDB important? \u00b6 Shift on Data Analysis Paradigm \u00b6 There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence. The Machine Learning (ML) Lifecycle \u00b6 The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. Why is it called MindsDB? \u00b6 Well, as most names, we needed one, we like science fiction, and the culture series , where there are these AI super-smart entities called Minds. How about the DB part? Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? We wanted to honor the open source tradition of animals related to projects! We went for a bear because of UC Berkeley where this all was initially coded. But we liked a cooler bear so went for a Polar Bear. How Can You Help Democratize Machine Learning? \u00b6 You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation, tutorial and examples. Tell your friends, to spread the word, i.e write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions .","title":"What is MindsDB?"},{"location":"what-is-mindsdb/#what-is-mindsdb","text":"Data is your single most important asset and your data lives in a database. By bringing machine learning to the database, MindsDB accelerates the speed of machine learning development. With MindsDB, you will easily build, train, optimize and deploy your models. Your predictions will be available via the queries you already use.","title":"What is MindsDB?"},{"location":"what-is-mindsdb/#what-are-ai-tables","text":"MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database.","title":"What are AI Tables?"},{"location":"what-is-mindsdb/#deep-dive-into-the-ai-tables","text":"","title":"Deep Dive into the AI Tables"},{"location":"what-is-mindsdb/#the-problem","text":"Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; + ------+-----+ | income | debt | + ------+-----+ | 60000 | 20000 | | 80000 | 25100 | | 100000 | 30040 | | 120000 | 36010 | + ------+-----+ A simple visualization of the data present in the income table is as follows: Querying the income table to get the debt value for a particular income value results in the following: SELECT income , debt FROM income_table WHERE income = 80000 ; + ------+-----+ | income | debt | + ------+-----+ | 80000 | 25100 | + ------+-----+ Green dot and dashed line as query result But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; Empty set ( 0 . 00 sec ) In other words, Nothing! no valuable information at all. Dashed red line describing the absense of a value (blue dot) for the query When a table doesn\u2019t have an exact match the query will return an empty set or null value. This is where the AI Tables come into play!","title":"The problem"},{"location":"what-is-mindsdb/#the-solution","text":"Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE PREDICTOR mindsdb . debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows: Green line describing the Predictor (model) created Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the mindsdb . debt_model instead of the income_table . SELECT income , debt FROM mindsdb . debt_model WHERE income = 90000 ; + ------+-----+ | income | debt | + ------+-----+ | 90000 | 27820 | + ------+-----+ Dashed blue line describing the query of the model with the predicted value (dark blue dot)","title":"The solution"},{"location":"what-is-mindsdb/#what-is-mindsdb-why-is-mindsdb-important","text":"","title":"What is MindsDB, why is MindsDB important?"},{"location":"what-is-mindsdb/#shift-on-data-analysis-paradigm","text":"There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence.","title":"Shift on Data Analysis Paradigm"},{"location":"what-is-mindsdb/#the-machine-learning-ml-lifecycle","text":"The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications.","title":"The Machine Learning (ML) Lifecycle"},{"location":"what-is-mindsdb/#why-is-it-called-mindsdb","text":"Well, as most names, we needed one, we like science fiction, and the culture series , where there are these AI super-smart entities called Minds. How about the DB part? Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? We wanted to honor the open source tradition of animals related to projects! We went for a bear because of UC Berkeley where this all was initially coded. But we liked a cooler bear so went for a Polar Bear.","title":"Why is it called MindsDB?"},{"location":"what-is-mindsdb/#how-can-you-help-democratize-machine-learning","text":"You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation, tutorial and examples. Tell your friends, to spread the word, i.e write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions .","title":"How Can You Help Democratize Machine Learning?"},{"location":"connect/dbeaver/","text":"Connect from DBeaver \u00b6 From the navigation menu, click Connect to database Search MySQL 8+ Select the MySQL 8+ or MySQL Click on Next Add the Hostname: cloud.mindsdb.com - for MindsDB cloud 127.0.0.1 - for local deployment Add the Database name (leave empty) Add Port 3306 - for MindsDB cloud 47335 - for local deployment Add the database user MindsDB Cloud username mindsdb - for local deployment Add Password for the user MindsDB Cloud password empty - for local deployment Click on OK . MindsDB Database \u00b6 At startup mindsdb database will contain 3 tables predictors , commands and databases . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). predict - The name of the target variable column. accuracy - The model accuracy. update_status - Training update status(up_to_date, updating). mindsdb_version - The MindsDB version used. error - Error message info in case of an error. select_data_query - SQL select query to create the datasource. training options - Additional training parameters. Whitelist MindsDB Cloud IP address If you need to whitelist MindsDB Cloud IP address to have access to your database, reach out to MindsDB team so we can share the Cloud static IP with you.","title":"DBeaver"},{"location":"connect/dbeaver/#connect-from-dbeaver","text":"From the navigation menu, click Connect to database Search MySQL 8+ Select the MySQL 8+ or MySQL Click on Next Add the Hostname: cloud.mindsdb.com - for MindsDB cloud 127.0.0.1 - for local deployment Add the Database name (leave empty) Add Port 3306 - for MindsDB cloud 47335 - for local deployment Add the database user MindsDB Cloud username mindsdb - for local deployment Add Password for the user MindsDB Cloud password empty - for local deployment Click on OK .","title":"Connect from DBeaver"},{"location":"connect/dbeaver/#mindsdb-database","text":"At startup mindsdb database will contain 3 tables predictors , commands and databases . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). predict - The name of the target variable column. accuracy - The model accuracy. update_status - Training update status(up_to_date, updating). mindsdb_version - The MindsDB version used. error - Error message info in case of an error. select_data_query - SQL select query to create the datasource. training options - Additional training parameters. Whitelist MindsDB Cloud IP address If you need to whitelist MindsDB Cloud IP address to have access to your database, reach out to MindsDB team so we can share the Cloud static IP with you.","title":"MindsDB Database"},{"location":"connect/dbt/","text":"To integrate your predictions into your DBT workflow, use the dbt-mindsdb adapter: Adapter for Documentation Install from PyPi MindsDB ( dbt-mindsdb ) Profile Setup pip install dbt-mindsdb Usage \u00b6 Initialization \u00b6 Create dbt project: dbt init [ project_name ] Configure your profiles.yml , currently MindsDB only supports user/password authentication, as shown below on: ~/.dbt/profiles.yml Self-Hosted Local Deployment MindsDB Cloud mindsdb: outputs: dev: type: mindsdb database: 'mindsdb' host: '127.0.0.1' port: 47335 schema: 'mindsdb' password: '' username: 'mindsdb' target: dev mindsdb: outputs: dev: type: mindsdb database: 'mindsdb' host: 'cloud.mindsdb.com' port: 47335 schema: '[dbt schema]' username: '[mindsdb cloud username]' password: '[mindsdb cloud password]' target: dev Key Required Description Example type \u2714\ufe0f The specific adapter to use mindsdb host \u2714\ufe0f The MindsDB (hostname) to connect to cloud.mindsdb.com port \u2714\ufe0f The port to use 3306 or 47335 schema \u2714\ufe0f Specify the schema (database) to build models into The MindsDB datasource username \u2714\ufe0f The username to use to connect to the server mindsdb or mindsdb cloud user password \u2714\ufe0f The password to use for authenticating to the server pass Create predictor \u00b6 Create table_name.sql ( table_name will be used as the name of the predictor): Parameter Required Description Example materialized \u2714\ufe0f Always predictor predictor integration \u2714\ufe0f Name of integration to get data from and save result to. It must be created in MindsDB beforehand. photorep predict \u2714\ufe0f Field to be predicted name predict_alias Alias for predicted field predicted_name using Configuration options for trained model ... {{ config ( materialized = 'predictor' , integration = 'photorep' , predict = 'name' , predict_alias = 'predicted_name' , using = { 'encoders.location.module' : 'CategoricalAutoEncoder' , 'encoders.rental_price.module' : 'NumericEncoder' } ) }} SELECT * FROM stores Create predictions table \u00b6 Create table_name .sql (If you need to specify schema, you can do it with a dot separator: schema_name. table_name .sql): Parameter Required Description Example materialized \u2714\ufe0f Always table table predictor_name \u2714\ufe0f Name of predictor model from Create predictor store_predictor integration \u2714\ufe0f Name of integration to get data from and save result to. It must be created in MindsDB beforehand. photorep {{ config ( materialized = 'table' , predictor_name = 'store_predictor' , integration = 'photorep' ) }} SELECT a , bc FROM ddd WHERE name > latest Note that each time dbt is run, the results table will be rewritten. Testing \u00b6 Install dev requirements pip install -r dev_requirements.txt Run pytest python -m pytest tests/","title":"DBT"},{"location":"connect/dbt/#usage","text":"","title":"Usage"},{"location":"connect/dbt/#initialization","text":"Create dbt project: dbt init [ project_name ] Configure your profiles.yml , currently MindsDB only supports user/password authentication, as shown below on: ~/.dbt/profiles.yml Self-Hosted Local Deployment MindsDB Cloud mindsdb: outputs: dev: type: mindsdb database: 'mindsdb' host: '127.0.0.1' port: 47335 schema: 'mindsdb' password: '' username: 'mindsdb' target: dev mindsdb: outputs: dev: type: mindsdb database: 'mindsdb' host: 'cloud.mindsdb.com' port: 47335 schema: '[dbt schema]' username: '[mindsdb cloud username]' password: '[mindsdb cloud password]' target: dev Key Required Description Example type \u2714\ufe0f The specific adapter to use mindsdb host \u2714\ufe0f The MindsDB (hostname) to connect to cloud.mindsdb.com port \u2714\ufe0f The port to use 3306 or 47335 schema \u2714\ufe0f Specify the schema (database) to build models into The MindsDB datasource username \u2714\ufe0f The username to use to connect to the server mindsdb or mindsdb cloud user password \u2714\ufe0f The password to use for authenticating to the server pass","title":"Initialization"},{"location":"connect/dbt/#create-predictor","text":"Create table_name.sql ( table_name will be used as the name of the predictor): Parameter Required Description Example materialized \u2714\ufe0f Always predictor predictor integration \u2714\ufe0f Name of integration to get data from and save result to. It must be created in MindsDB beforehand. photorep predict \u2714\ufe0f Field to be predicted name predict_alias Alias for predicted field predicted_name using Configuration options for trained model ... {{ config ( materialized = 'predictor' , integration = 'photorep' , predict = 'name' , predict_alias = 'predicted_name' , using = { 'encoders.location.module' : 'CategoricalAutoEncoder' , 'encoders.rental_price.module' : 'NumericEncoder' } ) }} SELECT * FROM stores","title":"Create predictor"},{"location":"connect/dbt/#create-predictions-table","text":"Create table_name .sql (If you need to specify schema, you can do it with a dot separator: schema_name. table_name .sql): Parameter Required Description Example materialized \u2714\ufe0f Always table table predictor_name \u2714\ufe0f Name of predictor model from Create predictor store_predictor integration \u2714\ufe0f Name of integration to get data from and save result to. It must be created in MindsDB beforehand. photorep {{ config ( materialized = 'table' , predictor_name = 'store_predictor' , integration = 'photorep' ) }} SELECT a , bc FROM ddd WHERE name > latest Note that each time dbt is run, the results table will be rewritten.","title":"Create predictions table"},{"location":"connect/dbt/#testing","text":"Install dev requirements pip install -r dev_requirements.txt Run pytest python -m pytest tests/","title":"Testing"},{"location":"connect/deepnote/","text":"Deepnote and MindsDB \u00b6 Deepnote integration We have worked with the team at Deepnote, and built native integration to Deepnote notebooks. Please check: Deepnote Demo Guide Deepnote Integration Docs","title":"Deepnote"},{"location":"connect/deepnote/#deepnote-and-mindsdb","text":"Deepnote integration We have worked with the team at Deepnote, and built native integration to Deepnote notebooks. Please check: Deepnote Demo Guide Deepnote Integration Docs","title":"Deepnote and MindsDB"},{"location":"connect/kafka/","text":"MindsDB and Kafka \u00b6 MindsDB provides a kafka connector plugin to connect to kafka cluster. At first, please visit Kafka Connect Mindsdb page on official confluent site. It contains all instructions how to install the connector from the confluent hub. In addition it wouldn't be a mistake to briefly review instructions described below. You may use official connector docker image: docker pull mindsdb/mindsdb-kafka-connector Also a source code of the connector is located here . Please read the instruction first before building the connector from scratch. It is possible to integrate and use the connector as part of your own kafka cluster or you may try this one in our test docker environment . Just execute: docker-compose up -d from the root of repository to build the connector and launch it in the test environment locally. Please note, there are two types of connector config (do not forget to set a real value for each parameter before using it): For MindsDB Cloud For a separate MindsDB installation Example \u00b6 Prerequisites: \u00b6 Launch MindsDB instance where HTTP API interface is running on docker network interface inet ip (it is 172.17.0.1 in general) Train a new model. You may use this tutorial as example run test kafka environment - docker-compose up -d (see details above) Create the connector instance: \u00b6 To create a connector need to send POST request to specific connectors url with connector configuration in JSON format: import requests MINDSDB_URL = \"http://172.17.0.1:47334\" CONNECTOR_NAME = \"MindsDBConnector\" INTEGRATION_NAME = 'test_kafka' KAFKA_PORT = 9092 KAFKA_HOST = \"127.0.0.1\" CONNECTOR_NAME = \"MindsDBConnector\" CONNECTORS_URL = \"http://127.0.0.1:9021/api/connect/connect-default/connectors\" STREAM_IN = \"topic_in\" STREAM_OUT = \"topic_out\" PREDICTOR_NAME = \"YOUR_MODEL_NAME\" # set actual model name here params = { \"name\" : CONNECTOR_NAME , \"config\" : { \"connector.class\" : \"com.mindsdb.kafka.connect.MindsDBConnector\" , \"topics\" : STREAM_IN , \"mindsdb.url\" : MINDSDB_URL , \"kafka.api.host\" : KAFKA_HOST , \"kafka.api.port\" : KAFKA_PORT , \"kafka.api.name\" : INTEGRATION_NAME , \"predictor.name\" : PREDICTOR_NAME , \"output.forecast.topic\" : STREAM_OUT , \"security.protocol\" : \"SASL_PLAINTEXT\" , \"sasl.mechanism\" : \"PLAIN\" , \"sasl.plain.username\" : \"admin\" , \"sasl.plain.password\" : \"admin-secret\" , } } headers = { \"Content-Type\" : \"application/json\" } res = requests . post ( CONNECTORS_URL , json = params , headers = headers ) The code above creates a MindsDB Kafka connector which uses PREDICTOR_NAME model, receives source data from STREAM_IN kafka topic and sends prediction results to STREAM_OUT kafka topic. Send source data and receive prediction results: \u00b6 There are many kafka clients implementations , and you may find the best one for your goals. import sys import json import kafka connection_info = { \"bootstrap_servers\" : \"127.0.0.1:9092\" , \"security_protocol\" : \"SASL_PLAINTEXT\" , \"sasl_mechanism\" : \"PLAIN\" , \"sasl_plain_username\" : \"admin\" , \"sasl_plain_password\" : \"admin-secret\" } producer = kafka . KafkaProducer ( ** connection_info ) if __name__ == '__main__' : print ( json . dumps ( connection_info )) if len ( sys . argv ) == 1 : topic = \"topic_in\" else : topic = sys . argv [ 1 ] for x in range ( 1 , 4 ): data = { \"Age\" : x + 20 , \"Weight\" : x * x * 0.8 + 200 , \"Height\" : x * x * 0.5 + 65 } to_send = json . dumps ( data ) producer . send ( topic , to_send . encode ( 'utf-8' )) producer . close () This piece of code generates and sends for source records to topic_in (by default) or any other topic (if it is provided and cmd parameter). import sys import json import kafka connection_info = { \"bootstrap_servers\" : \"127.0.0.1:9092\" , \"security_protocol\" : \"SASL_PLAINTEXT\" , \"sasl_mechanism\" : \"PLAIN\" , \"sasl_plain_username\" : \"admin\" , \"sasl_plain_password\" : \"admin-secret\" , \"auto_offset_reset\" : 'earliest' , \"consumer_timeout_ms\" : 1000 } consumer = kafka . KafkaConsumer ( ** connection_info ) if __name__ == '__main__' : print ( json . dumps ( connection_info )) if len ( sys . argv ) == 1 : topic = \"topic_out\" else : topic = sys . argv [ 1 ] consumer . subscribe ( topics = [ topic ]) for msg in consumer : print ( msg . value ) print ( \"-\" * 100 ) While this one shows how to read prediction results from topic_out (by default) or any other topic (if it is provided and cmd parameter).","title":"Kafka"},{"location":"connect/kafka/#mindsdb-and-kafka","text":"MindsDB provides a kafka connector plugin to connect to kafka cluster. At first, please visit Kafka Connect Mindsdb page on official confluent site. It contains all instructions how to install the connector from the confluent hub. In addition it wouldn't be a mistake to briefly review instructions described below. You may use official connector docker image: docker pull mindsdb/mindsdb-kafka-connector Also a source code of the connector is located here . Please read the instruction first before building the connector from scratch. It is possible to integrate and use the connector as part of your own kafka cluster or you may try this one in our test docker environment . Just execute: docker-compose up -d from the root of repository to build the connector and launch it in the test environment locally. Please note, there are two types of connector config (do not forget to set a real value for each parameter before using it): For MindsDB Cloud For a separate MindsDB installation","title":"MindsDB and Kafka"},{"location":"connect/kafka/#example","text":"","title":"Example"},{"location":"connect/kafka/#prerequisites","text":"Launch MindsDB instance where HTTP API interface is running on docker network interface inet ip (it is 172.17.0.1 in general) Train a new model. You may use this tutorial as example run test kafka environment - docker-compose up -d (see details above)","title":"Prerequisites:"},{"location":"connect/kafka/#create-the-connector-instance","text":"To create a connector need to send POST request to specific connectors url with connector configuration in JSON format: import requests MINDSDB_URL = \"http://172.17.0.1:47334\" CONNECTOR_NAME = \"MindsDBConnector\" INTEGRATION_NAME = 'test_kafka' KAFKA_PORT = 9092 KAFKA_HOST = \"127.0.0.1\" CONNECTOR_NAME = \"MindsDBConnector\" CONNECTORS_URL = \"http://127.0.0.1:9021/api/connect/connect-default/connectors\" STREAM_IN = \"topic_in\" STREAM_OUT = \"topic_out\" PREDICTOR_NAME = \"YOUR_MODEL_NAME\" # set actual model name here params = { \"name\" : CONNECTOR_NAME , \"config\" : { \"connector.class\" : \"com.mindsdb.kafka.connect.MindsDBConnector\" , \"topics\" : STREAM_IN , \"mindsdb.url\" : MINDSDB_URL , \"kafka.api.host\" : KAFKA_HOST , \"kafka.api.port\" : KAFKA_PORT , \"kafka.api.name\" : INTEGRATION_NAME , \"predictor.name\" : PREDICTOR_NAME , \"output.forecast.topic\" : STREAM_OUT , \"security.protocol\" : \"SASL_PLAINTEXT\" , \"sasl.mechanism\" : \"PLAIN\" , \"sasl.plain.username\" : \"admin\" , \"sasl.plain.password\" : \"admin-secret\" , } } headers = { \"Content-Type\" : \"application/json\" } res = requests . post ( CONNECTORS_URL , json = params , headers = headers ) The code above creates a MindsDB Kafka connector which uses PREDICTOR_NAME model, receives source data from STREAM_IN kafka topic and sends prediction results to STREAM_OUT kafka topic.","title":"Create the connector instance:"},{"location":"connect/kafka/#send-source-data-and-receive-prediction-results","text":"There are many kafka clients implementations , and you may find the best one for your goals. import sys import json import kafka connection_info = { \"bootstrap_servers\" : \"127.0.0.1:9092\" , \"security_protocol\" : \"SASL_PLAINTEXT\" , \"sasl_mechanism\" : \"PLAIN\" , \"sasl_plain_username\" : \"admin\" , \"sasl_plain_password\" : \"admin-secret\" } producer = kafka . KafkaProducer ( ** connection_info ) if __name__ == '__main__' : print ( json . dumps ( connection_info )) if len ( sys . argv ) == 1 : topic = \"topic_in\" else : topic = sys . argv [ 1 ] for x in range ( 1 , 4 ): data = { \"Age\" : x + 20 , \"Weight\" : x * x * 0.8 + 200 , \"Height\" : x * x * 0.5 + 65 } to_send = json . dumps ( data ) producer . send ( topic , to_send . encode ( 'utf-8' )) producer . close () This piece of code generates and sends for source records to topic_in (by default) or any other topic (if it is provided and cmd parameter). import sys import json import kafka connection_info = { \"bootstrap_servers\" : \"127.0.0.1:9092\" , \"security_protocol\" : \"SASL_PLAINTEXT\" , \"sasl_mechanism\" : \"PLAIN\" , \"sasl_plain_username\" : \"admin\" , \"sasl_plain_password\" : \"admin-secret\" , \"auto_offset_reset\" : 'earliest' , \"consumer_timeout_ms\" : 1000 } consumer = kafka . KafkaConsumer ( ** connection_info ) if __name__ == '__main__' : print ( json . dumps ( connection_info )) if len ( sys . argv ) == 1 : topic = \"topic_out\" else : topic = sys . argv [ 1 ] consumer . subscribe ( topics = [ topic ]) for msg in consumer : print ( msg . value ) print ( \"-\" * 100 ) While this one shows how to read prediction results from topic_out (by default) or any other topic (if it is provided and cmd parameter).","title":"Send source data and receive prediction results:"},{"location":"connect/mindsdb_editor/","text":"MindsDB SQL Editor \u00b6 MindsDB provides a SQL Editor so you don't need to download additional SQL clients to connect to MindsDB. There are two ways you can use the Editor: Self-Hosted Local Deployment MindsDB Cloud After any of the Self-Hosted Setups - Linux , Windows , MacOs or directly from source code - go to your terminal and execute: python -m mindsdb On execution, you should get: ... 2022 -05-06 14 :07:04,599 - INFO - - GUI available at http://127.0.0.1:47334/ ... Immediately after, your browser will automatically open the MindsDB SQL Editor. In case it doesn't, just visit the URL http://127.0.0.1:47334/ in your browser of preference. Go to MindsDB Cloud Just log in to your account, and you will be automatically directed to the Editor. What is next? We recommend you to follow one of our tutorials or jump more into detail understanding the MindsDB Database","title":"MindsDB SQL Editor"},{"location":"connect/mindsdb_editor/#mindsdb-sql-editor","text":"MindsDB provides a SQL Editor so you don't need to download additional SQL clients to connect to MindsDB. There are two ways you can use the Editor: Self-Hosted Local Deployment MindsDB Cloud After any of the Self-Hosted Setups - Linux , Windows , MacOs or directly from source code - go to your terminal and execute: python -m mindsdb On execution, you should get: ... 2022 -05-06 14 :07:04,599 - INFO - - GUI available at http://127.0.0.1:47334/ ... Immediately after, your browser will automatically open the MindsDB SQL Editor. In case it doesn't, just visit the URL http://127.0.0.1:47334/ in your browser of preference. Go to MindsDB Cloud Just log in to your account, and you will be automatically directed to the Editor. What is next? We recommend you to follow one of our tutorials or jump more into detail understanding the MindsDB Database","title":"MindsDB SQL Editor"},{"location":"connect/mysql-client/","text":"Connect MindsDB MySQL API to MySQL CLI \u00b6 MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command-Line Client . Connecting to MySQL API is the same as connecting to a MySQL database: How to Connect \u00b6 mysql -h [ hostname ] --port [ TCP/IP port number ] -u [ user ] -p [ password ] You can either connect locally or to a MindsDB Cloud instance, depending on the case; open your terminal and run: Self-Hosted Local Deployment MindsDB Cloud mysql -h 127 .0.0.1 --port 47335 -u mindsdb mysql -h cloud.mindsdb.com --port 3307 -u [ mindsdb_cloud_email ] -p [ mindsdb_cloud_password ] On execution, you should get: Welcome to the MariaDB monitor. Commands end with ; or \\g . Server version: 5 .7.1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [( none )] > Example \u00b6 mysql -h cloud.mindsdb.com --port 3306 -u zoran@mindsdb.com -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g . Server version: 5 .7.1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [( none )] > What is next? We recommend you to follow one of our tutorials or jump more into detail understanding the MindsDB Database","title":"MySQL CLI"},{"location":"connect/mysql-client/#connect-mindsdb-mysql-api-to-mysql-cli","text":"MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command-Line Client . Connecting to MySQL API is the same as connecting to a MySQL database:","title":"Connect MindsDB MySQL API to MySQL CLI"},{"location":"connect/mysql-client/#how-to-connect","text":"mysql -h [ hostname ] --port [ TCP/IP port number ] -u [ user ] -p [ password ] You can either connect locally or to a MindsDB Cloud instance, depending on the case; open your terminal and run: Self-Hosted Local Deployment MindsDB Cloud mysql -h 127 .0.0.1 --port 47335 -u mindsdb mysql -h cloud.mindsdb.com --port 3307 -u [ mindsdb_cloud_email ] -p [ mindsdb_cloud_password ] On execution, you should get: Welcome to the MariaDB monitor. Commands end with ; or \\g . Server version: 5 .7.1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [( none )] >","title":"How to Connect"},{"location":"connect/mysql-client/#example","text":"mysql -h cloud.mindsdb.com --port 3306 -u zoran@mindsdb.com -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g . Server version: 5 .7.1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [( none )] > What is next? We recommend you to follow one of our tutorials or jump more into detail understanding the MindsDB Database","title":"Example"},{"location":"connect/sql-alchemy/","text":"SQL Alchemy and MindsDB \u00b6 Note: MindsDB has a working integration with SQL Alchemy. To connect to MindsDB, please proceed as you would if you were connecting to a standard MySQL Database. Connect to MindsDB cloud \u00b6 If you have a MindsDB cloud, please use the following information: Hostname: cloud.mindsdb.com Database name (leave empty) Port: 3306 Credentials: MindsDB Cloud username and password On premise Deployment \u00b6 Hostname: 127.0.0.1 or the IP address where you are hosting your MindsDB server. Database name (leave empty) Port: 47335 Credentials: user: mindsdb password: empty","title":"SQL Alchemy"},{"location":"connect/sql-alchemy/#sql-alchemy-and-mindsdb","text":"Note: MindsDB has a working integration with SQL Alchemy. To connect to MindsDB, please proceed as you would if you were connecting to a standard MySQL Database.","title":"SQL Alchemy and MindsDB"},{"location":"connect/sql-alchemy/#connect-to-mindsdb-cloud","text":"If you have a MindsDB cloud, please use the following information: Hostname: cloud.mindsdb.com Database name (leave empty) Port: 3306 Credentials: MindsDB Cloud username and password","title":"Connect to MindsDB cloud"},{"location":"connect/sql-alchemy/#on-premise-deployment","text":"Hostname: 127.0.0.1 or the IP address where you are hosting your MindsDB server. Database name (leave empty) Port: 47335 Credentials: user: mindsdb password: empty","title":"On premise Deployment"},{"location":"connect/tableau/","text":"MindsDB and Tableau \u00b6 Work in progress Tableau requires the MySQL binary protocol, which we are in the process of supporting.If you want to collaborate on this part, please connect with us on slack.","title":"Tableau"},{"location":"connect/tableau/#mindsdb-and-tableau","text":"Work in progress Tableau requires the MySQL binary protocol, which we are in the process of supporting.If you want to collaborate on this part, please connect with us on slack.","title":"MindsDB and Tableau"},{"location":"custom-model/huggingface/","text":"MindsDB and HuggingFace \u00b6 You can bring your Hugging face models as tables to mindsDB, here is an example: CREATE PREDICTOR mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING format = 'huggingface' , APITOKEN = 'yourapitoken' data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" }","title":"HuggingFace"},{"location":"custom-model/huggingface/#mindsdb-and-huggingface","text":"You can bring your Hugging face models as tables to mindsDB, here is an example: CREATE PREDICTOR mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING format = 'huggingface' , APITOKEN = 'yourapitoken' data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" }","title":"MindsDB and HuggingFace"},{"location":"custom-model/mlflow/","text":"MLFLow and MindsDB \u00b6 Simple example - Logistic Regression \u00b6 MLFlow is a tool that you can use to train and serve models, among other features like organizing experiments, tracking metrics, etc. Given there is no way to train an MLflow-wrapped model using its API, you will have to train your models outside of MindsDB by pulling your data manually (i.e. with a script), ideally using a MLflow run or experiment. The first step would be to create a script where you train a model and save it using one of the saving methods that MLflow exposes. For this example, we will use the model in this simple tutorial where the method is mlflow.sklearn.log_model ( here ), given that the model is built with scikit-learn. Once trained, you need to make sure the model is served and listening for input in a URL of your choice (note, this can mean your model can run on a different machine than the one executing MindsDB). Let's assume this URL to be http://localhost:5000/invocations for now. This means you would execute the following command in your terminal, from the directory where the model was stored: mlflow models serve --model-uri runs:/<run-id>/model With <run-id> given in the output of the command python train.py used for actually training the model. Next, we're going to bring this model into MindsDB: CREATE PREDICTOR mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } We can now run predictions as usual, by using the WHERE statement or joining on a data table with an appropriate schema: SELECT ` 1 ` FROM byom_mlflow WHERE ` 0 `= 2 ; Advanced example - Keras NLP model \u00b6 Same use case as in section 1.2, be sure to download the dataset to reproduce the steps here. In this case, we will take a look at the best practices when your model needs custom data preprocessing code (which, realistically, will be fairly common). The key difference is that we now need to use the mlflow.pyfunc module to both 1) save the model using mlflow.pyfunc.save_model and 2) subclass mlflow.pyfunc.PythonModel to wrap the model in an MLflow-compatible way that will enable our custom inference logic to be called. Saving the model \u00b6 In the same script where you train the model (which you can find in the final section of 2.2) there should be a call at the end where you actually use mlflow to save every produced artifact: mlflow . pyfunc . save_model ( path = \"nlp_kaggle\" , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) Here, artifacts will be a dictionary with all expected produced outputs when running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. On the other hand, conda_env specifies the environment under which your model should be executed once served in a self-contained conda environment, so it should include all required packages and dependencies. For this example, they look like this: # these will be accessible inside the Model() wrapper artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } # specs for environment that will be created when serving the model conda_env = { 'name' : 'nlp_keras_env' , 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow' , 'cloudpickle' , 'nltk' , 'pandas' , 'numpy' , 'scikit-learn' , 'tqdm' , ], }, ], } Finally, to actually store the model you need to provide the wrapper class that will 1) load all produced artifacts into an accessible \"context\" and 2) implement all required inference logic: class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): # we use paths in the context to load everything self . model_path = context . artifacts [ 'model' ] self . model = load_model ( self . model_path ) with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) def predict ( self , context , model_input ): # preprocess input, tokenize, pad, and call the model df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) As you can see, here we are loading multiple artifacts and using them to guarantee the input data will be in the same format that was used when training. Ideally, you would abstract this even further into a single preprocess method that is called both at training time and inference time. Finally, serving is simple. Go to the directory where you called the above script, and execute mlflow models serve --model-uri ./nlp_kaggle . At this point, the rest is essentially the same as in the previous example. You can link the MLflow model with these SQL statements: CREATE PREDICTOR mindsdb . byom_mlflow_nlp PREDICT ` target ` USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , dtype_dict = { \"text\" : \"rich text\" , \"target\" : \"binary\" } ; To get predictions, you can directly pass input data using the WHERE clause: SELECT target FROM mindsdb . byom_mlflow_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Or you can JOIN with a data table. For this, you should ensure the table actually exists and that the database it belongs to has been connected to your MindsDB instance. For more details, refer to the same steps in the Ray Serve example (section 1.2). SELECT ta . text , tb . target as predicted FROM db_byom . test . nlp_kaggle_test as ta JOIN mindsdb . byom_mlflow_nlp as tb ; Full Script \u00b6 Finally, for reference, here's the full script that trains and saves the model. The model is exactly the same as in section 1.2, so it may seem familiar. import re import pickle import string import mlflow.pyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy as np import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from tensorflow.keras.initializers import Constant from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.models import load_model stop = set ( stopwords . words ( 'english' )) MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def preprocess_df ( df ): df = df [[ 'text' ]] funcs = [ remove_URL , remove_html , remove_emoji , remove_punct ] for fn in funcs : df [ 'text' ] = df [ 'text' ] . apply ( lambda x : fn ( x )) return df def remove_URL ( text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) def remove_html ( text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) def remove_punct ( text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) def create_corpus ( df ): corpus = [] for tweet in tqdm . tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): self . model_path = context . artifacts [ 'model' ] with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) self . model = load_model ( self . model_path ) def predict ( self , context , model_input ): df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) if __name__ == '__main__' : train_model = True model_path = './' tokenizer_path = './tokenizer.pkl' run_name = 'test_run' mlflow_pyfunc_model_path = \"nlp_kaggle\" mlflow . set_tracking_uri ( \"sqlite:///mlflow.db\" ) if train_model : # preprocess data df = pd . read_csv ( './train.csv' ) target = df [[ 'target' ]] target_arr = target . values df = preprocess_df ( df ) train_corpus = create_corpus ( df ) # load embeddings embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) embedding_dict [ word ] = vectors f . close () # generate and save tokenizer tokenizer_obj = Tokenizer () tokenizer_obj . fit_on_texts ( train_corpus ) with open ( tokenizer_path , 'wb' ) as f : pickle . dump ( tokenizer_obj , f ) # tokenize and pad sequences = tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] word_index = tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , GLOVE_DIM )) # fill embedding matrix for word , i in tqdm . tqdm ( word_index . items ()): if i > num_words : continue emb_vec = embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) # generate model model = Sequential () embedding = Embedding ( num_words , GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = MAX_LEN , trainable = False ) model . add ( embedding ) model . add ( SpatialDropout1D ( 0.2 )) model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) optimizer = Adam ( learning_rate = 1e-5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) # train and save model . fit ( X_train , y_train , batch_size = 4 , epochs = EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) model . save ( model_path ) # save in mlflow format artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } conda_env = { 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow== {} ' . format ( tensorflow . __version__ ), 'cloudpickle== {} ' . format ( cloudpickle . __version__ ), 'nltk== {} ' . format ( nltk . __version__ ), 'pandas== {} ' . format ( pd . __version__ ), 'numpy== {} ' . format ( np . __version__ ), 'scikit-learn== {} ' . format ( sklearn . __version__ ), 'tqdm== {} ' . format ( tqdm . __version__ ) ], }, ], 'name' : 'nlp_keras_env' } # Save and register the MLflow Model with mlflow . start_run ( run_name = run_name ) as run : mlflow . pyfunc . save_model ( path = mlflow_pyfunc_model_path , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) result = mlflow . register_model ( f \"runs:/ { run . info . run_id } / { mlflow_pyfunc_model_path } \" , f \" { mlflow_pyfunc_model_path } \" )","title":"MLFlow"},{"location":"custom-model/mlflow/#mlflow-and-mindsdb","text":"","title":"MLFLow and MindsDB"},{"location":"custom-model/mlflow/#simple-example-logistic-regression","text":"MLFlow is a tool that you can use to train and serve models, among other features like organizing experiments, tracking metrics, etc. Given there is no way to train an MLflow-wrapped model using its API, you will have to train your models outside of MindsDB by pulling your data manually (i.e. with a script), ideally using a MLflow run or experiment. The first step would be to create a script where you train a model and save it using one of the saving methods that MLflow exposes. For this example, we will use the model in this simple tutorial where the method is mlflow.sklearn.log_model ( here ), given that the model is built with scikit-learn. Once trained, you need to make sure the model is served and listening for input in a URL of your choice (note, this can mean your model can run on a different machine than the one executing MindsDB). Let's assume this URL to be http://localhost:5000/invocations for now. This means you would execute the following command in your terminal, from the directory where the model was stored: mlflow models serve --model-uri runs:/<run-id>/model With <run-id> given in the output of the command python train.py used for actually training the model. Next, we're going to bring this model into MindsDB: CREATE PREDICTOR mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } We can now run predictions as usual, by using the WHERE statement or joining on a data table with an appropriate schema: SELECT ` 1 ` FROM byom_mlflow WHERE ` 0 `= 2 ;","title":"Simple example - Logistic Regression"},{"location":"custom-model/mlflow/#advanced-example-keras-nlp-model","text":"Same use case as in section 1.2, be sure to download the dataset to reproduce the steps here. In this case, we will take a look at the best practices when your model needs custom data preprocessing code (which, realistically, will be fairly common). The key difference is that we now need to use the mlflow.pyfunc module to both 1) save the model using mlflow.pyfunc.save_model and 2) subclass mlflow.pyfunc.PythonModel to wrap the model in an MLflow-compatible way that will enable our custom inference logic to be called.","title":"Advanced example - Keras NLP model"},{"location":"custom-model/mlflow/#saving-the-model","text":"In the same script where you train the model (which you can find in the final section of 2.2) there should be a call at the end where you actually use mlflow to save every produced artifact: mlflow . pyfunc . save_model ( path = \"nlp_kaggle\" , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) Here, artifacts will be a dictionary with all expected produced outputs when running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. On the other hand, conda_env specifies the environment under which your model should be executed once served in a self-contained conda environment, so it should include all required packages and dependencies. For this example, they look like this: # these will be accessible inside the Model() wrapper artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } # specs for environment that will be created when serving the model conda_env = { 'name' : 'nlp_keras_env' , 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow' , 'cloudpickle' , 'nltk' , 'pandas' , 'numpy' , 'scikit-learn' , 'tqdm' , ], }, ], } Finally, to actually store the model you need to provide the wrapper class that will 1) load all produced artifacts into an accessible \"context\" and 2) implement all required inference logic: class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): # we use paths in the context to load everything self . model_path = context . artifacts [ 'model' ] self . model = load_model ( self . model_path ) with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) def predict ( self , context , model_input ): # preprocess input, tokenize, pad, and call the model df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) As you can see, here we are loading multiple artifacts and using them to guarantee the input data will be in the same format that was used when training. Ideally, you would abstract this even further into a single preprocess method that is called both at training time and inference time. Finally, serving is simple. Go to the directory where you called the above script, and execute mlflow models serve --model-uri ./nlp_kaggle . At this point, the rest is essentially the same as in the previous example. You can link the MLflow model with these SQL statements: CREATE PREDICTOR mindsdb . byom_mlflow_nlp PREDICT ` target ` USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , dtype_dict = { \"text\" : \"rich text\" , \"target\" : \"binary\" } ; To get predictions, you can directly pass input data using the WHERE clause: SELECT target FROM mindsdb . byom_mlflow_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Or you can JOIN with a data table. For this, you should ensure the table actually exists and that the database it belongs to has been connected to your MindsDB instance. For more details, refer to the same steps in the Ray Serve example (section 1.2). SELECT ta . text , tb . target as predicted FROM db_byom . test . nlp_kaggle_test as ta JOIN mindsdb . byom_mlflow_nlp as tb ;","title":"Saving the model"},{"location":"custom-model/mlflow/#full-script","text":"Finally, for reference, here's the full script that trains and saves the model. The model is exactly the same as in section 1.2, so it may seem familiar. import re import pickle import string import mlflow.pyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy as np import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from tensorflow.keras.initializers import Constant from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.models import load_model stop = set ( stopwords . words ( 'english' )) MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def preprocess_df ( df ): df = df [[ 'text' ]] funcs = [ remove_URL , remove_html , remove_emoji , remove_punct ] for fn in funcs : df [ 'text' ] = df [ 'text' ] . apply ( lambda x : fn ( x )) return df def remove_URL ( text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) def remove_html ( text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) def remove_punct ( text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) def create_corpus ( df ): corpus = [] for tweet in tqdm . tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): self . model_path = context . artifacts [ 'model' ] with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) self . model = load_model ( self . model_path ) def predict ( self , context , model_input ): df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) if __name__ == '__main__' : train_model = True model_path = './' tokenizer_path = './tokenizer.pkl' run_name = 'test_run' mlflow_pyfunc_model_path = \"nlp_kaggle\" mlflow . set_tracking_uri ( \"sqlite:///mlflow.db\" ) if train_model : # preprocess data df = pd . read_csv ( './train.csv' ) target = df [[ 'target' ]] target_arr = target . values df = preprocess_df ( df ) train_corpus = create_corpus ( df ) # load embeddings embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) embedding_dict [ word ] = vectors f . close () # generate and save tokenizer tokenizer_obj = Tokenizer () tokenizer_obj . fit_on_texts ( train_corpus ) with open ( tokenizer_path , 'wb' ) as f : pickle . dump ( tokenizer_obj , f ) # tokenize and pad sequences = tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] word_index = tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , GLOVE_DIM )) # fill embedding matrix for word , i in tqdm . tqdm ( word_index . items ()): if i > num_words : continue emb_vec = embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) # generate model model = Sequential () embedding = Embedding ( num_words , GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = MAX_LEN , trainable = False ) model . add ( embedding ) model . add ( SpatialDropout1D ( 0.2 )) model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) optimizer = Adam ( learning_rate = 1e-5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) # train and save model . fit ( X_train , y_train , batch_size = 4 , epochs = EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) model . save ( model_path ) # save in mlflow format artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } conda_env = { 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow== {} ' . format ( tensorflow . __version__ ), 'cloudpickle== {} ' . format ( cloudpickle . __version__ ), 'nltk== {} ' . format ( nltk . __version__ ), 'pandas== {} ' . format ( pd . __version__ ), 'numpy== {} ' . format ( np . __version__ ), 'scikit-learn== {} ' . format ( sklearn . __version__ ), 'tqdm== {} ' . format ( tqdm . __version__ ) ], }, ], 'name' : 'nlp_keras_env' } # Save and register the MLflow Model with mlflow . start_run ( run_name = run_name ) as run : mlflow . pyfunc . save_model ( path = mlflow_pyfunc_model_path , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) result = mlflow . register_model ( f \"runs:/ { run . info . run_id } / { mlflow_pyfunc_model_path } \" , f \" { mlflow_pyfunc_model_path } \" )","title":"Full Script"},{"location":"custom-model/openai/","text":"MindsDB and OpenAI \u00b6 You can bring OpenAI models as tables to mindsDB, here is an example: CREATE PREDICTOR mindsdb . byom_mlflow PREDICT target_text_column -- `1` is the target column name USING format = 'openai' , APITOKEN = 'yourapitoken' data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" }","title":"OpenAI"},{"location":"custom-model/openai/#mindsdb-and-openai","text":"You can bring OpenAI models as tables to mindsDB, here is an example: CREATE PREDICTOR mindsdb . byom_mlflow PREDICT target_text_column -- `1` is the target column name USING format = 'openai' , APITOKEN = 'yourapitoken' data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" }","title":"MindsDB and OpenAI"},{"location":"custom-model/ray-serve/","text":"MindsDB and Ray Serve \u00b6 Simple example - Logistic regression \u00b6 Ray serve is a simple high-throughput service that can wrap over your own ml models. In this example, we will train and predict with an external scikit-learn model. First, let's look at the actual model wrapped inside a class that complies with the above requirements: import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI () ray . init () serve . start ( detached = True ) async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns )) - set ([ target ])) self . feature_cols = feature_cols X = df . loc [:, self . feature_cols ] Y = list ( df [ target ]) self . model = LogisticRegression () self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) X = df . loc [:, self . feature_cols ] predictions = self . model . predict ( X ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ]} return pred_dict MyModel . deploy () while True : time . sleep ( 1 ) The important bits here are having train and predict endpoints. The train endpoint accept two parameters in the JSON sent via POST: - df -- a serialized dictionary that can be converted into a pandas dataframe - target -- the name of the target column The predict endpoint needs only one parameter: - df -- a serialized dictionary that can be converted into a pandas dataframe The training endpoints must return a JSON that contains the keys status set to ok . The predict endpoint must return a dictionary containing the prediction key, storing the predictions. Additional keys can be returned for confidence and confidence intervals. Once you start this RayServe-wrapped model you can train it using a query like this one: CREATE PREDICTOR mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; And you can query predictions as usual, either by conditioning on a subset of input columns: SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or by JOINING to do batch predictions: SELECT tb . number_of_rooms , t . rental_price FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Please note that, if your model is behind a reverse proxy (e.g. nginx) you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB itself can send as much as you'd like and has been stress-tested with over a billion rows. Example - Keras NLP model \u00b6 For this example, we will consider a natural language processing (NLP) task where we want to train a neural network with Keras to detect if a tweet is related to a natural disaster (fires, earthquakes, etc.). Please download this dataset to follow the example. The code for the model here is a bit more complex than in section 1.1, but the same rules apply: we create a Ray Server based service that wraps around a Kaggle NLP Model which can be trained and then used for predictions: import re import time import json import string import requests from collections import Counter , defaultdict \u200b import ray from ray import serve \u200b import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk.util import ngrams from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer \u200b from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.initializers import Constant from tensorflow.keras.optimizers import Adam \u200b app = FastAPI () stop = set ( stopwords . words ( 'english' )) \u200b \u200b async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target \u200b \u200b @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 \u200b def __init__ ( self ): self . model = None \u200b @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) \u200b target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) \u200b self . embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) self . embedding_dict [ word ] = vectors f . close () \u200b self . tokenizer_obj = Tokenizer () self . tokenizer_obj . fit_on_texts ( train_corpus ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , self . __class__ . GLOVE_DIM )) \u200b for word , i in tqdm ( word_index . items ()): if i > num_words : continue \u200b emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec \u200b self . model = Sequential () embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 )) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) self . model . add ( Dense ( 1 , activation = 'sigmoid' )) \u200b optimizer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) \u200b X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) \u200b return { 'status' : 'ok' } \u200b @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) \u200b df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () sub = pd . DataFrame ({ 'target' : y_pre }) \u200b pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ]} return pred_dict \u200b def preprocess_df ( self , df ): df = df [[ 'text' ]] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x )) return df \u200b def remove_URL ( self , text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) \u200b def remove_html ( self , text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) \u200b def remove_punct ( self , text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) \u200b def remove_emoji ( self , text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) \u200b def create_corpus ( self , df ): corpus = [] for tweet in tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus \u200b \u200b if __name__ == '__main__' : \u200b ray . init () serve . start ( detached = True ) \u200b Model . deploy () \u200b while True : time . sleep ( 1 ) We need access to the training data, so we'll create a table called nlp_kaggle_train to load the dataset that the original model uses. And ingest it into a table with the following schema: id INT , keyword VARCHAR ( 255 ), location VARCHAR ( 255 ), text VARCHAR ( 5000 ), target INT Note: specifics of the schema and how to ingest the csv will vary depending on your database. Next, we can register and train the above custom model using the following query: CREATE PREDICTOR mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; Training will take a while given that this model is a neural network rather than a simple logistic regression. You can check its status with the query SELECT * FROM mindsdb.predictors WHERE name = 'byom_ray_serve_nlp'; , much like you'd do with a \"normal\" MindsDB predictor. Once the predictor's status becomes trained we can query it for predictions as usual: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Which would, hopefully, output 1 . Alternatively, we can try out this tweet to expect 0 as an output: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; If your results do not match this example, it could help to train the model for a longer amount of epochs.","title":"Ray Serve"},{"location":"custom-model/ray-serve/#mindsdb-and-ray-serve","text":"","title":"MindsDB and Ray Serve"},{"location":"custom-model/ray-serve/#simple-example-logistic-regression","text":"Ray serve is a simple high-throughput service that can wrap over your own ml models. In this example, we will train and predict with an external scikit-learn model. First, let's look at the actual model wrapped inside a class that complies with the above requirements: import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI () ray . init () serve . start ( detached = True ) async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns )) - set ([ target ])) self . feature_cols = feature_cols X = df . loc [:, self . feature_cols ] Y = list ( df [ target ]) self . model = LogisticRegression () self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) X = df . loc [:, self . feature_cols ] predictions = self . model . predict ( X ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ]} return pred_dict MyModel . deploy () while True : time . sleep ( 1 ) The important bits here are having train and predict endpoints. The train endpoint accept two parameters in the JSON sent via POST: - df -- a serialized dictionary that can be converted into a pandas dataframe - target -- the name of the target column The predict endpoint needs only one parameter: - df -- a serialized dictionary that can be converted into a pandas dataframe The training endpoints must return a JSON that contains the keys status set to ok . The predict endpoint must return a dictionary containing the prediction key, storing the predictions. Additional keys can be returned for confidence and confidence intervals. Once you start this RayServe-wrapped model you can train it using a query like this one: CREATE PREDICTOR mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; And you can query predictions as usual, either by conditioning on a subset of input columns: SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or by JOINING to do batch predictions: SELECT tb . number_of_rooms , t . rental_price FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Please note that, if your model is behind a reverse proxy (e.g. nginx) you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB itself can send as much as you'd like and has been stress-tested with over a billion rows.","title":"Simple example - Logistic regression"},{"location":"custom-model/ray-serve/#example-keras-nlp-model","text":"For this example, we will consider a natural language processing (NLP) task where we want to train a neural network with Keras to detect if a tweet is related to a natural disaster (fires, earthquakes, etc.). Please download this dataset to follow the example. The code for the model here is a bit more complex than in section 1.1, but the same rules apply: we create a Ray Server based service that wraps around a Kaggle NLP Model which can be trained and then used for predictions: import re import time import json import string import requests from collections import Counter , defaultdict \u200b import ray from ray import serve \u200b import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk.util import ngrams from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer \u200b from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.initializers import Constant from tensorflow.keras.optimizers import Adam \u200b app = FastAPI () stop = set ( stopwords . words ( 'english' )) \u200b \u200b async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target \u200b \u200b @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 \u200b def __init__ ( self ): self . model = None \u200b @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) \u200b target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) \u200b self . embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) self . embedding_dict [ word ] = vectors f . close () \u200b self . tokenizer_obj = Tokenizer () self . tokenizer_obj . fit_on_texts ( train_corpus ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , self . __class__ . GLOVE_DIM )) \u200b for word , i in tqdm ( word_index . items ()): if i > num_words : continue \u200b emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec \u200b self . model = Sequential () embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 )) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) self . model . add ( Dense ( 1 , activation = 'sigmoid' )) \u200b optimizer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) \u200b X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) \u200b return { 'status' : 'ok' } \u200b @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) \u200b df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () sub = pd . DataFrame ({ 'target' : y_pre }) \u200b pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ]} return pred_dict \u200b def preprocess_df ( self , df ): df = df [[ 'text' ]] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x )) return df \u200b def remove_URL ( self , text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) \u200b def remove_html ( self , text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) \u200b def remove_punct ( self , text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) \u200b def remove_emoji ( self , text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) \u200b def create_corpus ( self , df ): corpus = [] for tweet in tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus \u200b \u200b if __name__ == '__main__' : \u200b ray . init () serve . start ( detached = True ) \u200b Model . deploy () \u200b while True : time . sleep ( 1 ) We need access to the training data, so we'll create a table called nlp_kaggle_train to load the dataset that the original model uses. And ingest it into a table with the following schema: id INT , keyword VARCHAR ( 255 ), location VARCHAR ( 255 ), text VARCHAR ( 5000 ), target INT Note: specifics of the schema and how to ingest the csv will vary depending on your database. Next, we can register and train the above custom model using the following query: CREATE PREDICTOR mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; Training will take a while given that this model is a neural network rather than a simple logistic regression. You can check its status with the query SELECT * FROM mindsdb.predictors WHERE name = 'byom_ray_serve_nlp'; , much like you'd do with a \"normal\" MindsDB predictor. Once the predictor's status becomes trained we can query it for predictions as usual: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Which would, hopefully, output 1 . Alternatively, we can try out this tweet to expect 0 as an output: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; If your results do not match this example, it could help to train the model for a longer amount of epochs.","title":"Example - Keras NLP model"},{"location":"mongo/mongo/","text":"Train a model from the MongoDB API \u00b6 Note: This is work in progress, please join our slack channel if you have any questions. Train new model \u00b6 To train a new model, you will need to insert() a new document inside the mindsdb.predictors collection. The object sent to the insert() for training the new model should contain: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a list of features. connection(string) -- The connection string for connecting to MongoDB. If you have used GUI to connect to MongoDB, that connection will be used. select_data_query (object) -- The object that contains info about getting the data to train the model. database(string) - The name of the database collection(string) - The name of the collection find(dict) - The dict that selects the documents from the collection, must be valid JSON format. Same as db.collection.find({...}) training_options (dict) -- Optional value that contains additional training parameters. To train timeseries model you need to provide training_options . db . predictors . insert ( { 'name' : str , 'predict' : str | list of fields , 'connection' : str , # optional 'select_data_query' : { 'database' : str , 'collection' : str , 'find' : dict } , 'training_options' : dict # optional } ) For the timeseries model: db.predictors.insert({ 'name': str, 'predict': str | list of fields, 'connection': str, # optional 'select_data_query':{ 'database': str, 'collection': str, 'find': dict }, 'training_options': { \"timeseries_settings\": { \"order_by\": list of fields, \"group_by\": list of fields, #optional \"horizon\": int, #optional \"use_previous_target\": Boolean, \"window\": int } } }) Train new model example \u00b6 The following example shows you how to train a new model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. db . predictors . insert ( { 'name' : 'churn' , 'predict' : 'Churn' , 'select_data_query' : { 'database' : 'test_data' , 'collection' : 'customer_churn' , 'find' : {} } } ) This INSERT query will train a new model called churn that predicts the customer Churn value. Model training status \u00b6 To check that the training finished successfully, you can find() the model status inside mindsdb.predictors collection e.g.: db . predictors . find () That's it You have successfully trained a new model from a mongo shell. The next step is to get predictions by querying the model . Delete model \u00b6 To delete the model run remove function on predictors collection and send the name of the model to delete as: db.predictors.remove({name: 'model_name'}) Query the model from MongoDB API \u00b6 To get the predictions from the model, you will need to call find() method on the model collection and provide values for which you want to get prediction as an object: db . model_name . find ( { 'key' : 'value' , 'key' : 'value' } ) Note The object provided to find() method must be valid JSON format. Query example \u00b6 The following example shows you how to query the model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. MindsDB will predict the customer Churn value based on the object values sent to find() method. db . churn . find ( { 'PhoneService' : 'Yes' , 'InternetService' : 'DSL' , 'OnlineService' : 'No' , 'MonthlyCharges' : 53 . 85 , 'TotalCharges' : 108 . 15 , 'tenure' : 2 , 'PaperlessBilling' : 'Yes' } ) You should get a response from MindsDB similar to: predicted_value confidence info Yes 0.8 Check JSON below { \"Churn\" : \"Yes\" , \"Churn_confidence\" : 0.8 , \"Churn_explain\" : { \"class_distribution\" : { \"No\" : 0.44513007027299717 , \"Yes\" : 0.5548699297270028 }, \"predicted_value\" : \"Yes\" , \"confidence\" : 0.8 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Contract\" ] } }","title":"Train a model from the MongoDB API"},{"location":"mongo/mongo/#train-a-model-from-the-mongodb-api","text":"Note: This is work in progress, please join our slack channel if you have any questions.","title":"Train a model from the MongoDB API"},{"location":"mongo/mongo/#train-new-model","text":"To train a new model, you will need to insert() a new document inside the mindsdb.predictors collection. The object sent to the insert() for training the new model should contain: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features, include a list of features. connection(string) -- The connection string for connecting to MongoDB. If you have used GUI to connect to MongoDB, that connection will be used. select_data_query (object) -- The object that contains info about getting the data to train the model. database(string) - The name of the database collection(string) - The name of the collection find(dict) - The dict that selects the documents from the collection, must be valid JSON format. Same as db.collection.find({...}) training_options (dict) -- Optional value that contains additional training parameters. To train timeseries model you need to provide training_options . db . predictors . insert ( { 'name' : str , 'predict' : str | list of fields , 'connection' : str , # optional 'select_data_query' : { 'database' : str , 'collection' : str , 'find' : dict } , 'training_options' : dict # optional } ) For the timeseries model: db.predictors.insert({ 'name': str, 'predict': str | list of fields, 'connection': str, # optional 'select_data_query':{ 'database': str, 'collection': str, 'find': dict }, 'training_options': { \"timeseries_settings\": { \"order_by\": list of fields, \"group_by\": list of fields, #optional \"horizon\": int, #optional \"use_previous_target\": Boolean, \"window\": int } } })","title":"Train new model"},{"location":"mongo/mongo/#train-new-model-example","text":"The following example shows you how to train a new model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. db . predictors . insert ( { 'name' : 'churn' , 'predict' : 'Churn' , 'select_data_query' : { 'database' : 'test_data' , 'collection' : 'customer_churn' , 'find' : {} } } ) This INSERT query will train a new model called churn that predicts the customer Churn value.","title":"Train new model example"},{"location":"mongo/mongo/#model-training-status","text":"To check that the training finished successfully, you can find() the model status inside mindsdb.predictors collection e.g.: db . predictors . find () That's it You have successfully trained a new model from a mongo shell. The next step is to get predictions by querying the model .","title":"Model training status"},{"location":"mongo/mongo/#delete-model","text":"To delete the model run remove function on predictors collection and send the name of the model to delete as: db.predictors.remove({name: 'model_name'})","title":"Delete model"},{"location":"mongo/mongo/#query-the-model-from-mongodb-api","text":"To get the predictions from the model, you will need to call find() method on the model collection and provide values for which you want to get prediction as an object: db . model_name . find ( { 'key' : 'value' , 'key' : 'value' } ) Note The object provided to find() method must be valid JSON format.","title":"Query the model from MongoDB API"},{"location":"mongo/mongo/#query-example","text":"The following example shows you how to query the model from a mongo client. The collection used for training the model is the Telcom Customer Churn dataset. MindsDB will predict the customer Churn value based on the object values sent to find() method. db . churn . find ( { 'PhoneService' : 'Yes' , 'InternetService' : 'DSL' , 'OnlineService' : 'No' , 'MonthlyCharges' : 53 . 85 , 'TotalCharges' : 108 . 15 , 'tenure' : 2 , 'PaperlessBilling' : 'Yes' } ) You should get a response from MindsDB similar to: predicted_value confidence info Yes 0.8 Check JSON below { \"Churn\" : \"Yes\" , \"Churn_confidence\" : 0.8 , \"Churn_explain\" : { \"class_distribution\" : { \"No\" : 0.44513007027299717 , \"Yes\" : 0.5548699297270028 }, \"predicted_value\" : \"Yes\" , \"confidence\" : 0.8 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Contract\" ] } }","title":"Query example"},{"location":"setup/cloud/","text":"MindsDB Cloud \u00b6 You can sign up for a quick jumpstart using MindsDB in our Cloud. MindsDB Cloud is hosted by the MindsDB team and has all of the latest updates. You can start using it immediately for free by following these steps: Create a MindsDB Cloud free account \u00b6 Create an account by visiting the sign-up page and filling out the sign-up form. You can see MindsDB Cloud's terms and conditions here Log into MindsDB Cloud \u00b6 Now you can go to the log-in page and input your credentials. Email Validation \u00b6 After sign up, you will get a confirmation email to validate your account. Use MindsDB Cloud \u00b6 Now, you are ready to use MindsDB Cloud. What is next? Now you can use MindsDB Cloud as a SQL database","title":"MindsDB Cloud"},{"location":"setup/cloud/#mindsdb-cloud","text":"You can sign up for a quick jumpstart using MindsDB in our Cloud. MindsDB Cloud is hosted by the MindsDB team and has all of the latest updates. You can start using it immediately for free by following these steps:","title":"MindsDB Cloud"},{"location":"setup/cloud/#create-a-mindsdb-cloud-free-account","text":"Create an account by visiting the sign-up page and filling out the sign-up form. You can see MindsDB Cloud's terms and conditions here","title":"Create a MindsDB Cloud free account"},{"location":"setup/cloud/#log-into-mindsdb-cloud","text":"Now you can go to the log-in page and input your credentials.","title":"Log into MindsDB Cloud"},{"location":"setup/cloud/#email-validation","text":"After sign up, you will get a confirmation email to validate your account.","title":"Email Validation"},{"location":"setup/cloud/#use-mindsdb-cloud","text":"Now, you are ready to use MindsDB Cloud. What is next? Now you can use MindsDB Cloud as a SQL database","title":"Use MindsDB Cloud"},{"location":"setup/self-hosted/docker/","text":"Setup for Docker \u00b6 Install Docker \u00b6 Install Docker on your machine. To make sure Docker is successfully installed on your machine, run: docker run hello-world You should see the Hello from Docker! message displayed. If not, check Docker's Get Started documentation. Docker for Mac users, RAM Allocation issues By default, Docker for Mac allocates 2.00GB of RAM . This is insufficient for deploying MindsDB with docker. We recommend increasing the default RAM limit to 4.00GB . Please refer to Docker's Docker Desktop for Mac user manual for more information on how to increase the allocated memory. Start MindsDB \u00b6 Run the below command to start MindsDB in Docker: docker run -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb The -p flag allows access to MindsDB by two different methods: -p 47334 :47334 - Publishes port 47334 to access MindsDB GUI and HTTP API. -p 47335 :47335 - Publishes port 47335 to access MindsDB MySQL API. Optional Additional Configuration \u00b6 Default Configuration \u00b6 The default configuration for MindsDB's Docker image is represented as a JSON block: { \"config_version\" : \"1.4\" , \"storage_dir\" : \"/root/mdb_storage\" , \"log\" : { \"level\" : { \"console\" : \"ERROR\" , \"file\" : \"WARNING\" , \"db\" : \"WARNING\" } }, \"debug\" : false , \"integrations\" : {}, \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"0.0.0.0\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"mindsdb\" , \"database\" : \"mindsdb\" , \"ssl\" : true }, \"mongodb\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } } To override the default configuration, you can provide values via the MDB_CONFIG_CONTENT environment variable. Example \u00b6 docker run -e MDB_CONFIG_CONTENT = '{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"8080\"}}}' mindsdb/mindsdb Known Issues \u00b6 MKL Issues If you experience any issues related to MKL or if your training process does not complete, please add the MKL_SERVICE_FORCE_INTEL environment variable: docker run -e MKL_SERVICE_FORCE_INTEL = 1 -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb","title":"Docker"},{"location":"setup/self-hosted/docker/#setup-for-docker","text":"","title":"Setup for Docker"},{"location":"setup/self-hosted/docker/#install-docker","text":"Install Docker on your machine. To make sure Docker is successfully installed on your machine, run: docker run hello-world You should see the Hello from Docker! message displayed. If not, check Docker's Get Started documentation. Docker for Mac users, RAM Allocation issues By default, Docker for Mac allocates 2.00GB of RAM . This is insufficient for deploying MindsDB with docker. We recommend increasing the default RAM limit to 4.00GB . Please refer to Docker's Docker Desktop for Mac user manual for more information on how to increase the allocated memory.","title":"Install Docker"},{"location":"setup/self-hosted/docker/#start-mindsdb","text":"Run the below command to start MindsDB in Docker: docker run -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb The -p flag allows access to MindsDB by two different methods: -p 47334 :47334 - Publishes port 47334 to access MindsDB GUI and HTTP API. -p 47335 :47335 - Publishes port 47335 to access MindsDB MySQL API.","title":"Start MindsDB"},{"location":"setup/self-hosted/docker/#optional-additional-configuration","text":"","title":"Optional Additional Configuration"},{"location":"setup/self-hosted/docker/#default-configuration","text":"The default configuration for MindsDB's Docker image is represented as a JSON block: { \"config_version\" : \"1.4\" , \"storage_dir\" : \"/root/mdb_storage\" , \"log\" : { \"level\" : { \"console\" : \"ERROR\" , \"file\" : \"WARNING\" , \"db\" : \"WARNING\" } }, \"debug\" : false , \"integrations\" : {}, \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"0.0.0.0\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"mindsdb\" , \"database\" : \"mindsdb\" , \"ssl\" : true }, \"mongodb\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } } To override the default configuration, you can provide values via the MDB_CONFIG_CONTENT environment variable.","title":"Default Configuration"},{"location":"setup/self-hosted/docker/#example","text":"docker run -e MDB_CONFIG_CONTENT = '{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"8080\"}}}' mindsdb/mindsdb","title":"Example"},{"location":"setup/self-hosted/docker/#known-issues","text":"MKL Issues If you experience any issues related to MKL or if your training process does not complete, please add the MKL_SERVICE_FORCE_INTEL environment variable: docker run -e MKL_SERVICE_FORCE_INTEL = 1 -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb","title":"Known Issues"},{"location":"setup/self-hosted/pip/linux/","text":"Setup for Linux via pip \u00b6 Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions . Suggestions Install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python>=3.7 and pip>=19.3 . Using the Python venv Module \u00b6 Create new virtual environment called mindsdb: python -m venv mindsdb And, activate it: source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Using Anaconda \u00b6 You will need Anaconda or Conda installed and Python 64-bit version. Open Anaconda Prompt and create new virtual environment conda create -n mindsdb conda activate mindsdb Install mindsdb in recently created virtual environment: pip install mindsdb To verify that Mindsdb was installed run: conda list You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Troubleshooting \u00b6 Common Issues \u00b6 Installation fail Note that Python 64 bit version is required. Installation fail If you are using Python 3.9 you may get installation errors. Some of MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this, and Python 3.9 will be supported soon. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies. No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. Still Having problems? \u00b6 Don't worry! Try to replicate the issue using the official docker setup , and please create an issue on our Github repository as detailed as possible. We'll review it and give you a response within a few hours.","title":"Linux via pip"},{"location":"setup/self-hosted/pip/linux/#setup-for-linux-via-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions . Suggestions Install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python>=3.7 and pip>=19.3 .","title":"Setup for Linux via pip"},{"location":"setup/self-hosted/pip/linux/#using-the-python-venv-module","text":"Create new virtual environment called mindsdb: python -m venv mindsdb And, activate it: source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using the Python venv Module"},{"location":"setup/self-hosted/pip/linux/#using-anaconda","text":"You will need Anaconda or Conda installed and Python 64-bit version. Open Anaconda Prompt and create new virtual environment conda create -n mindsdb conda activate mindsdb Install mindsdb in recently created virtual environment: pip install mindsdb To verify that Mindsdb was installed run: conda list You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using Anaconda"},{"location":"setup/self-hosted/pip/linux/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"setup/self-hosted/pip/linux/#common-issues","text":"Installation fail Note that Python 64 bit version is required. Installation fail If you are using Python 3.9 you may get installation errors. Some of MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this, and Python 3.9 will be supported soon. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies. No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated.","title":"Common Issues"},{"location":"setup/self-hosted/pip/linux/#still-having-problems","text":"Don't worry! Try to replicate the issue using the official docker setup , and please create an issue on our Github repository as detailed as possible. We'll review it and give you a response within a few hours.","title":"Still Having problems?"},{"location":"setup/self-hosted/pip/macos/","text":"Setup for MacOS via pip \u00b6 Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions . Suggestions Install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python>=3.7 and pip>=19.3 . Using the Python venv Module \u00b6 Create new virtual environment called mindsdb: python -m venv mindsdb And, activate it: source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Using Anaconda \u00b6 You will need Anaconda or Conda installed and Python 64-bit version. Open Anaconda Prompt and create new virtual environment conda create -n mindsdb conda activate mindsdb Install mindsdb in recently created virtual environment: pip install mindsdb To verify that Mindsdb was installed run: conda list You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Troubleshooting \u00b6 Common Issues \u00b6 Installation fail Note that Python 64 bit version is required. Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt . IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies. No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. Some of MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this, and Python 3.9 will be supported soon. Still Having problems? \u00b6 Don't worry! Try to replicate the issue using the official docker setup , and please create an issue on our Github repository as detailed as possible. We'll review it and give you a response within a few hours.","title":"MacOS via pip"},{"location":"setup/self-hosted/pip/macos/#setup-for-macos-via-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions . Suggestions Install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python>=3.7 and pip>=19.3 .","title":"Setup for MacOS via pip"},{"location":"setup/self-hosted/pip/macos/#using-the-python-venv-module","text":"Create new virtual environment called mindsdb: python -m venv mindsdb And, activate it: source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using the Python venv Module"},{"location":"setup/self-hosted/pip/macos/#using-anaconda","text":"You will need Anaconda or Conda installed and Python 64-bit version. Open Anaconda Prompt and create new virtual environment conda create -n mindsdb conda activate mindsdb Install mindsdb in recently created virtual environment: pip install mindsdb To verify that Mindsdb was installed run: conda list You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using Anaconda"},{"location":"setup/self-hosted/pip/macos/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"setup/self-hosted/pip/macos/#common-issues","text":"Installation fail Note that Python 64 bit version is required. Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt . IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies. No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. Some of MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this, and Python 3.9 will be supported soon.","title":"Common Issues"},{"location":"setup/self-hosted/pip/macos/#still-having-problems","text":"Don't worry! Try to replicate the issue using the official docker setup , and please create an issue on our Github repository as detailed as possible. We'll review it and give you a response within a few hours.","title":"Still Having problems?"},{"location":"setup/self-hosted/pip/source/","text":"Build From Source Using pip \u00b6 This section describes how to deploy MindsDB from the source code. It is the preferred way to use MindsDB if you want to contribute to our code or debug MindsDB. Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions . Suggestions Install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python>=3.7 and pip>=19.3 . Installation \u00b6 We recommend installing MindsDB inside a virtual environment to avoid dependency issues. Clone the repository: git clone git@github.com:mindsdb/mindsdb.git Create new virtual environment called mindsdb-venv: python -m venv mindsdb-venv And, activate it: source mindsdb-venv/bin/activate Install MindsDB prerequisites: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop To verify everything works, start the MindsDB server: python -m mindsdb Now you should be able to access: MindsDB APIs MindsDB Studio MindsDB Studio Using mySQL http://127.0.0.1:47334/api http://127.0.0.1:47334/ mysql -h 127 .0.0.1 --port 3306 -u mindsdb -p Troubleshooting \u00b6 Common Issues \u00b6 No module named mindsdb If you get this error, make sure that your virtual environment is activated. ImportError: No module named {dependency name} This type of error can occur if you skipped the 3rd step. Make sure that you install all of the MindsDB requirements. This site can\u2019t be reached. 127.0.0.1 refused to connect. Please check the MindsDB server console in case the server is still in the starting phase. If the server has started and there is an error displayed, please report it on our GitHub Still Having problems? \u00b6 Don't worry! Try to replicate the issue using the official docker setup , and please create an issue on our Github repository as detailed as possible. We'll review it and give you a response within a few hours.","title":"Sourcecode via pip"},{"location":"setup/self-hosted/pip/source/#build-from-source-using-pip","text":"This section describes how to deploy MindsDB from the source code. It is the preferred way to use MindsDB if you want to contribute to our code or debug MindsDB. Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions . Suggestions Install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python>=3.7 and pip>=19.3 .","title":"Build From Source Using pip"},{"location":"setup/self-hosted/pip/source/#installation","text":"We recommend installing MindsDB inside a virtual environment to avoid dependency issues. Clone the repository: git clone git@github.com:mindsdb/mindsdb.git Create new virtual environment called mindsdb-venv: python -m venv mindsdb-venv And, activate it: source mindsdb-venv/bin/activate Install MindsDB prerequisites: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop To verify everything works, start the MindsDB server: python -m mindsdb Now you should be able to access: MindsDB APIs MindsDB Studio MindsDB Studio Using mySQL http://127.0.0.1:47334/api http://127.0.0.1:47334/ mysql -h 127 .0.0.1 --port 3306 -u mindsdb -p","title":"Installation"},{"location":"setup/self-hosted/pip/source/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"setup/self-hosted/pip/source/#common-issues","text":"No module named mindsdb If you get this error, make sure that your virtual environment is activated. ImportError: No module named {dependency name} This type of error can occur if you skipped the 3rd step. Make sure that you install all of the MindsDB requirements. This site can\u2019t be reached. 127.0.0.1 refused to connect. Please check the MindsDB server console in case the server is still in the starting phase. If the server has started and there is an error displayed, please report it on our GitHub","title":"Common Issues"},{"location":"setup/self-hosted/pip/source/#still-having-problems","text":"Don't worry! Try to replicate the issue using the official docker setup , and please create an issue on our Github repository as detailed as possible. We'll review it and give you a response within a few hours.","title":"Still Having problems?"},{"location":"setup/self-hosted/pip/windows/","text":"Setup for Windows via pip \u00b6 Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions . Suggestions Install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python>=3.7 and pip>=19.3 . Using the Python venv Module \u00b6 Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate.bat Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Using Anaconda \u00b6 You will need Anaconda or Conda installed and Python 64-bit version. Open Anaconda Prompt and create new virtual environment conda create -n mindsdb conda activate mindsdb Install mindsdb in recently created virtual environment: pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ... Troubleshooting \u00b6 Common Issues \u00b6 Installation fail Note that Python 64 bit version is required. Installation fail If you are using Python 3.9 you may get installation errors. Some of MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this, and Python 3.9 will be supported soon. Installation fail If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt . pip command not found fail Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb Still Having problems? \u00b6 Don't worry! Try to replicate the issue using the official docker setup , and please create an issue on our Github repository as detailed as possible. We'll review it and give you a response within a few hours.","title":"Windows via pip"},{"location":"setup/self-hosted/pip/windows/#setup-for-windows-via-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions . Suggestions Install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python>=3.7 and pip>=19.3 .","title":"Setup for Windows via pip"},{"location":"setup/self-hosted/pip/windows/#using-the-python-venv-module","text":"Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate.bat Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using the Python venv Module"},{"location":"setup/self-hosted/pip/windows/#using-anaconda","text":"You will need Anaconda or Conda installed and Python 64-bit version. Open Anaconda Prompt and create new virtual environment conda create -n mindsdb conda activate mindsdb Install mindsdb in recently created virtual environment: pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages included but not limited to: ... alembic == 1 .7.7 aniso8601 == 9 .0.1 appdirs == 1 .4.4 lightgbm == 3 .3.0 lightwood == 22 .4.1.0 MindsDB == 22 .4.5.0 mindsdb-datasources == 1 .8.2 mindsdb-sql == 0 .3.3 mindsdb-streams == 0 .0.5 ...","title":"Using Anaconda"},{"location":"setup/self-hosted/pip/windows/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"setup/self-hosted/pip/windows/#common-issues","text":"Installation fail Note that Python 64 bit version is required. Installation fail If you are using Python 3.9 you may get installation errors. Some of MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this, and Python 3.9 will be supported soon. Installation fail If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt . pip command not found fail Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb","title":"Common Issues"},{"location":"setup/self-hosted/pip/windows/#still-having-problems","text":"Don't worry! Try to replicate the issue using the official docker setup , and please create an issue on our Github repository as detailed as possible. We'll review it and give you a response within a few hours.","title":"Still Having problems?"},{"location":"sql/table-structure/","text":"Table Structure \u00b6 General Structure \u00b6 On startup the mindsdb database will contain 2 tables: predictors and datasources SHOW TABLES ; On execution, you should get: + ---------------------------+ | Tables_in_mindsdb | + ---------------------------+ | predictors | | databases | | integration_name | + ---------------------------+ The predictors TABLE \u00b6 All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: Column name Description name The name of the model. status Training status(training, complete, error). predict The name of the target variable column. accuracy The model accuracy. update_status Trainig update status(up_to_date, updating). mindsdb_version The mindsdb version used. error Error message info in case of an errror. select_data_query SQL select query to create the datasource. training options Additional training parameters. The datasource TABLE \u00b6 This is a work in progess The [integration_name] TABLE \u00b6 This is a work in progress The model table \u00b6 This is a work in progress The below list contains the column names of the model table. Note that target_variable_ will be the name of the target variable column. target_variable_original - The original value of the target variable. target_variable_min - Lower bound of the predicted value. target_variable_max - Upper bound of the predicted value. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound , confidence_upper_bound , anomaly , truth . select_data_query - SQL select query to create the datasource.","title":"Table Structure (Schema)"},{"location":"sql/table-structure/#table-structure","text":"","title":"Table Structure"},{"location":"sql/table-structure/#general-structure","text":"On startup the mindsdb database will contain 2 tables: predictors and datasources SHOW TABLES ; On execution, you should get: + ---------------------------+ | Tables_in_mindsdb | + ---------------------------+ | predictors | | databases | | integration_name | + ---------------------------+","title":"General Structure"},{"location":"sql/table-structure/#the-predictors-table","text":"All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: Column name Description name The name of the model. status Training status(training, complete, error). predict The name of the target variable column. accuracy The model accuracy. update_status Trainig update status(up_to_date, updating). mindsdb_version The mindsdb version used. error Error message info in case of an errror. select_data_query SQL select query to create the datasource. training options Additional training parameters.","title":"The predictors TABLE"},{"location":"sql/table-structure/#the-datasource-table","text":"This is a work in progess","title":"The datasource TABLE"},{"location":"sql/table-structure/#the-integration_name-table","text":"This is a work in progress","title":"The [integration_name] TABLE"},{"location":"sql/table-structure/#the-model-table","text":"This is a work in progress The below list contains the column names of the model table. Note that target_variable_ will be the name of the target variable column. target_variable_original - The original value of the target variable. target_variable_min - Lower bound of the predicted value. target_variable_max - Upper bound of the predicted value. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound , confidence_upper_bound , anomaly , truth . select_data_query - SQL select query to create the datasource.","title":"The model table"},{"location":"sql/api/describe/","text":"DESCRIBE Statement \u00b6 Description \u00b6 The DESCRIBE statement is used to display the attributes of an existing model. DESCRIBE ... FEATURES Statement \u00b6 DESCRIBE ... FEATURES Description \u00b6 The DESCRIBE mindsdb.[name_of_your_predictor].features statement is used to display the way that the model encoded the data prior to training. DESCRIBE ... FEATURES Syntax \u00b6 DESCRIBE mindsdb .[ name_of_your_predictor ]. features ; On execution: + --------------+-------------+--------------+-------------+ | column | type | encoder | role | + --------------+-------------+--------------+-------------+ | column_name | column_type | encoder_used | column_role | + --------------+-------------+--------------+-------------+ Where: Description [name_of_your_predictor] Name of the model to be described column Columns used type Type of data infered encoder Encoder used role Role for that column, it can be feature or target DESCRIBE ... FEATURES Example \u00b6 DESCRIBE mindsdb . home_rentals_model . features ; On execution: + ---------------------+-------------+----------------+---------+ | column | type | encoder | role | + ---------------------+-------------+----------------+---------+ | number_of_rooms | categorical | OneHotEncoder | feature | | number_of_bathrooms | binary | BinaryEncoder | feature | | sqft | integer | NumericEncoder | feature | | location | categorical | OneHotEncoder | feature | | days_on_market | integer | NumericEncoder | feature | | neighborhood | categorical | OneHotEncoder | feature | | rental_price | integer | NumericEncoder | target | + ---------------------+-------------+----------------+---------+ DESCRIBE ... MODEL Statement \u00b6 The DESCRIBE mindsdb.[name_of_your_predictor].model statement is used to display the performance of the candidate models. DESCRIBE ... MODEL Syntax \u00b6 DESCRIBE mindsdb .[ name_of_your_predictor ]. model ; On execution: + -----------------+-------------+---------------+----------+ | name | performance | training_time | selected | + -----------------+-------------+---------------+----------+ | candidate_model | performace | training_time | selected | + -----------------+-------------+---------------+----------+ Where: Description [name_of_your_predictor] Name of the model to be described name Name of the candidate_model performance Accuracy From 0 - 1 depending on the type of the model training_time Time elapsed for the model training to be completed selected 1 for the best performing model 0 for the rest DESCRIBE ... MODEL Example \u00b6 DESCRIBE mindsdb . home_rentals_model . model ; On execution: + ------------+--------------------+----------------------+----------+ | name | performance | training_time | selected | + ------------+--------------------+----------------------+----------+ | Neural | 0 . 9861694189913056 | 3 . 1538941860198975 | 0 | | LightGBM | 0 . 9991920992432087 | 15 . 671080827713013 | 1 | | Regression | 0 . 9983390488042778 | 0 . 016761064529418945 | 0 | + ------------+--------------------+----------------------+----------+ DESCRIBE ... ENSEMBLE \u00b6 DESCRIBE ... ENSEMBLE Syntax \u00b6 DESCRIBE mindsdb .[ name_of_your_predictor ]. ensemble ; On execution: + -----------------+ | ensemble | + -----------------+ | { JSON } | + -----------------+ Where: Description ensemble JSON type object describing the parameters used to select best model candidate DESCRIBE ... ENSEMBLE Example \u00b6 DESCRIBE mindsdb . home_rentals_model . ensemble ; On execution: + ----------------------------------------------------------------------+ | ensemble | + ----------------------------------------------------------------------+ | { \"encoders\" : { \"rental_price\" : { \"module\" : \"NumericEncoder\" , \"args\" : { \"is_target\" : \"True\" , \"positive_domain\" : \"$statistical_analysis.positive_domain\" } } , \"number_of_rooms\" : { \"module\" : \"OneHotEncoder\" , \"args\" : {} } , \"number_of_bathrooms\" : { \"module\" : \"BinaryEncoder\" , \"args\" : {} } , \"sqft\" : { \"module\" : \"NumericEncoder\" , \"args\" : {} } , \"location\" : { \"module\" : \"OneHotEncoder\" , \"args\" : {} } , \"days_on_market\" : { \"module\" : \"NumericEncoder\" , \"args\" : {} } , \"neighborhood\" : { \"module\" : \"OneHotEncoder\" , \"args\" : {} } } , \"dtype_dict\" : { \"number_of_rooms\" : \"categorical\" , \"number_of_bathrooms\" : \"binary\" , \"sqft\" : \"integer\" , \"location\" : \"categorical\" , \"days_on_market\" : \"integer\" , \"neighborhood\" : \"categorical\" , \"rental_price\" : \"integer\" } , \"dependency_dict\" : {} , \"model\" : { \"module\" : \"BestOf\" , \"args\" : { \"submodels\" : [ { \"module\" : \"Neural\" , \"args\" : { \"fit_on_dev\" : true , \"stop_after\" : \"$problem_definition.seconds_per_mixer\" , \"search_hyperparameters\" : true } } , { \"module\" : \"LightGBM\" , \"args\" : { \"stop_after\" : \"$problem_definition.seconds_per_mixer\" , \"fit_on_dev\" : true } } , { \"module\" : \"Regression\" , \"args\" : { \"stop_after\" : \"$problem_definition.seconds_per_mixer\" } } ], \"args\" : \"$pred_args\" , \"accuracy_functions\" : \"$accuracy_functions\" , \"ts_analysis\" : null } } , \"problem_definition\" : { \"target\" : \"rental_price\" , \"pct_invalid\" : 2 , \"unbias_target\" : true , \"seconds_per_mixer\" : 57024 . 0 , \"seconds_per_encoder\" : null , \"expected_additional_time\" : 8 . 687719106674194 , \"time_aim\" : 259200 , \"target_weights\" : null , \"positive_domain\" : false , \"timeseries_settings\" : { \"is_timeseries\" : false , \"order_by\" : null , \"window\" : null , \"group_by\" : null , \"use_previous_target\" : true , \"horizon\" : null , \"historical_columns\" : null , \"target_type\" : \"\" , \"allow_incomplete_history\" : true , \"eval_cold_start\" : true , \"interval_periods\" : [] } , \"anomaly_detection\" : false , \"use_default_analysis\" : true , \"ignore_features\" : [], \"fit_on_all\" : true , \"strict_mode\" : true , \"seed_nr\" : 420 } , \"identifiers\" : {} , \"accuracy_functions\" : [ \"r2_score\" ] } | + ----------------------------------------------------------------------+ Unsure what it all means? If you're unsure on how to DESCRIBE your model or understand the results feel free to ask us how to do it on the community Slack workspace .","title":"DESCRIBE"},{"location":"sql/api/describe/#describe-statement","text":"","title":"DESCRIBE Statement"},{"location":"sql/api/describe/#description","text":"The DESCRIBE statement is used to display the attributes of an existing model.","title":"Description"},{"location":"sql/api/describe/#describe-features-statement","text":"","title":"DESCRIBE ... FEATURES Statement"},{"location":"sql/api/describe/#describe-features-description","text":"The DESCRIBE mindsdb.[name_of_your_predictor].features statement is used to display the way that the model encoded the data prior to training.","title":"DESCRIBE ... FEATURES Description"},{"location":"sql/api/describe/#describe-features-syntax","text":"DESCRIBE mindsdb .[ name_of_your_predictor ]. features ; On execution: + --------------+-------------+--------------+-------------+ | column | type | encoder | role | + --------------+-------------+--------------+-------------+ | column_name | column_type | encoder_used | column_role | + --------------+-------------+--------------+-------------+ Where: Description [name_of_your_predictor] Name of the model to be described column Columns used type Type of data infered encoder Encoder used role Role for that column, it can be feature or target","title":"DESCRIBE ... FEATURES Syntax"},{"location":"sql/api/describe/#describe-features-example","text":"DESCRIBE mindsdb . home_rentals_model . features ; On execution: + ---------------------+-------------+----------------+---------+ | column | type | encoder | role | + ---------------------+-------------+----------------+---------+ | number_of_rooms | categorical | OneHotEncoder | feature | | number_of_bathrooms | binary | BinaryEncoder | feature | | sqft | integer | NumericEncoder | feature | | location | categorical | OneHotEncoder | feature | | days_on_market | integer | NumericEncoder | feature | | neighborhood | categorical | OneHotEncoder | feature | | rental_price | integer | NumericEncoder | target | + ---------------------+-------------+----------------+---------+","title":"DESCRIBE ... FEATURES Example"},{"location":"sql/api/describe/#describe-model-statement","text":"The DESCRIBE mindsdb.[name_of_your_predictor].model statement is used to display the performance of the candidate models.","title":"DESCRIBE ... MODEL Statement"},{"location":"sql/api/describe/#describe-model-syntax","text":"DESCRIBE mindsdb .[ name_of_your_predictor ]. model ; On execution: + -----------------+-------------+---------------+----------+ | name | performance | training_time | selected | + -----------------+-------------+---------------+----------+ | candidate_model | performace | training_time | selected | + -----------------+-------------+---------------+----------+ Where: Description [name_of_your_predictor] Name of the model to be described name Name of the candidate_model performance Accuracy From 0 - 1 depending on the type of the model training_time Time elapsed for the model training to be completed selected 1 for the best performing model 0 for the rest","title":"DESCRIBE ... MODEL Syntax"},{"location":"sql/api/describe/#describe-model-example","text":"DESCRIBE mindsdb . home_rentals_model . model ; On execution: + ------------+--------------------+----------------------+----------+ | name | performance | training_time | selected | + ------------+--------------------+----------------------+----------+ | Neural | 0 . 9861694189913056 | 3 . 1538941860198975 | 0 | | LightGBM | 0 . 9991920992432087 | 15 . 671080827713013 | 1 | | Regression | 0 . 9983390488042778 | 0 . 016761064529418945 | 0 | + ------------+--------------------+----------------------+----------+","title":"DESCRIBE ... MODEL Example"},{"location":"sql/api/describe/#describe-ensemble","text":"","title":"DESCRIBE ... ENSEMBLE"},{"location":"sql/api/describe/#describe-ensemble-syntax","text":"DESCRIBE mindsdb .[ name_of_your_predictor ]. ensemble ; On execution: + -----------------+ | ensemble | + -----------------+ | { JSON } | + -----------------+ Where: Description ensemble JSON type object describing the parameters used to select best model candidate","title":"DESCRIBE ... ENSEMBLE Syntax"},{"location":"sql/api/describe/#describe-ensemble-example","text":"DESCRIBE mindsdb . home_rentals_model . ensemble ; On execution: + ----------------------------------------------------------------------+ | ensemble | + ----------------------------------------------------------------------+ | { \"encoders\" : { \"rental_price\" : { \"module\" : \"NumericEncoder\" , \"args\" : { \"is_target\" : \"True\" , \"positive_domain\" : \"$statistical_analysis.positive_domain\" } } , \"number_of_rooms\" : { \"module\" : \"OneHotEncoder\" , \"args\" : {} } , \"number_of_bathrooms\" : { \"module\" : \"BinaryEncoder\" , \"args\" : {} } , \"sqft\" : { \"module\" : \"NumericEncoder\" , \"args\" : {} } , \"location\" : { \"module\" : \"OneHotEncoder\" , \"args\" : {} } , \"days_on_market\" : { \"module\" : \"NumericEncoder\" , \"args\" : {} } , \"neighborhood\" : { \"module\" : \"OneHotEncoder\" , \"args\" : {} } } , \"dtype_dict\" : { \"number_of_rooms\" : \"categorical\" , \"number_of_bathrooms\" : \"binary\" , \"sqft\" : \"integer\" , \"location\" : \"categorical\" , \"days_on_market\" : \"integer\" , \"neighborhood\" : \"categorical\" , \"rental_price\" : \"integer\" } , \"dependency_dict\" : {} , \"model\" : { \"module\" : \"BestOf\" , \"args\" : { \"submodels\" : [ { \"module\" : \"Neural\" , \"args\" : { \"fit_on_dev\" : true , \"stop_after\" : \"$problem_definition.seconds_per_mixer\" , \"search_hyperparameters\" : true } } , { \"module\" : \"LightGBM\" , \"args\" : { \"stop_after\" : \"$problem_definition.seconds_per_mixer\" , \"fit_on_dev\" : true } } , { \"module\" : \"Regression\" , \"args\" : { \"stop_after\" : \"$problem_definition.seconds_per_mixer\" } } ], \"args\" : \"$pred_args\" , \"accuracy_functions\" : \"$accuracy_functions\" , \"ts_analysis\" : null } } , \"problem_definition\" : { \"target\" : \"rental_price\" , \"pct_invalid\" : 2 , \"unbias_target\" : true , \"seconds_per_mixer\" : 57024 . 0 , \"seconds_per_encoder\" : null , \"expected_additional_time\" : 8 . 687719106674194 , \"time_aim\" : 259200 , \"target_weights\" : null , \"positive_domain\" : false , \"timeseries_settings\" : { \"is_timeseries\" : false , \"order_by\" : null , \"window\" : null , \"group_by\" : null , \"use_previous_target\" : true , \"horizon\" : null , \"historical_columns\" : null , \"target_type\" : \"\" , \"allow_incomplete_history\" : true , \"eval_cold_start\" : true , \"interval_periods\" : [] } , \"anomaly_detection\" : false , \"use_default_analysis\" : true , \"ignore_features\" : [], \"fit_on_all\" : true , \"strict_mode\" : true , \"seed_nr\" : 420 } , \"identifiers\" : {} , \"accuracy_functions\" : [ \"r2_score\" ] } | + ----------------------------------------------------------------------+ Unsure what it all means? If you're unsure on how to DESCRIBE your model or understand the results feel free to ask us how to do it on the community Slack workspace .","title":"DESCRIBE ... ENSEMBLE Example"},{"location":"sql/api/drop/","text":"DROP PREDICTOR Statement \u00b6 Description \u00b6 The DROP PREDICTOR statement is used to delete the model table: Syntax \u00b6 DROP PREDICTOR [ predictor_name ]; On execution: Query OK , 0 rows affected ( 0 . 058 sec ) Where: Description [predictor_name] Name of the model to be deleted Validation \u00b6 SELECT name FROM mindsdb . predictors WHERE name = '[predictor_name]' ; On execution: Empty set ( 0 . 026 sec ) Example \u00b6 The following SQL statement drops the model table called home_rentals_model . Given the followig query to list all predictors by name SELECT name FROM mindsdb . predictors ; Resulting in a table with 2 rows: + ---------------------+ | name | + ---------------------+ | other_model | + ---------------------+ | home_rentals_model | + ---------------------+ Execute the DROP PREDICTOR statement as: DROP PREDICTOR home_rentals_model ; On execution: Query OK , 0 rows affected ( 0 . 058 sec ) Validate that the model has been deleted by listing again all predictors by name: SELECT name FROM mindsdb . predictors ; Resulting in a table with only one row, and without the dropped model: + ---------------------+ | name | + ---------------------+ | other_model | + ---------------------+","title":"DROP PREDICTOR"},{"location":"sql/api/drop/#drop-predictor-statement","text":"","title":"DROP PREDICTOR Statement"},{"location":"sql/api/drop/#description","text":"The DROP PREDICTOR statement is used to delete the model table:","title":"Description"},{"location":"sql/api/drop/#syntax","text":"DROP PREDICTOR [ predictor_name ]; On execution: Query OK , 0 rows affected ( 0 . 058 sec ) Where: Description [predictor_name] Name of the model to be deleted","title":"Syntax"},{"location":"sql/api/drop/#validation","text":"SELECT name FROM mindsdb . predictors WHERE name = '[predictor_name]' ; On execution: Empty set ( 0 . 026 sec )","title":"Validation"},{"location":"sql/api/drop/#example","text":"The following SQL statement drops the model table called home_rentals_model . Given the followig query to list all predictors by name SELECT name FROM mindsdb . predictors ; Resulting in a table with 2 rows: + ---------------------+ | name | + ---------------------+ | other_model | + ---------------------+ | home_rentals_model | + ---------------------+ Execute the DROP PREDICTOR statement as: DROP PREDICTOR home_rentals_model ; On execution: Query OK , 0 rows affected ( 0 . 058 sec ) Validate that the model has been deleted by listing again all predictors by name: SELECT name FROM mindsdb . predictors ; Resulting in a table with only one row, and without the dropped model: + ---------------------+ | name | + ---------------------+ | other_model | + ---------------------+","title":"Example"},{"location":"sql/api/insert/","text":"INSERT INTO Statement \u00b6 Description \u00b6 The INSERT INTO statement is used to fill a table with the result of subselect. commonly used to persist predictions into the database Syntax \u00b6 INSERT INTO [ integration_name ].[ table_name ] [ SELECT ...] It performs a subselect [ SELECT ...] and gets data from it there after it performs INSERT INTO TABLE [ table_name ] of integration [integration_name] Example \u00b6 In this example we want to persist the predictions into a table int1 . tbl1 . Given the following schema: int1 \u2514\u2500\u2500 tbl1 mindsdb \u2514\u2500\u2500 predictor_name int2 \u2514\u2500\u2500 tbl2 Where: Description int1 Integration for the table to be created in tbl1 Table to be created predictor_name Name of the model to be used int2 Database to be used as a source in the inner SELECT tbl2 Table to be used as a source. In order to achive the desired result we could execute the following query: INSERT INTO int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' )","title":"INSERT INTO"},{"location":"sql/api/insert/#insert-into-statement","text":"","title":"INSERT INTO Statement"},{"location":"sql/api/insert/#description","text":"The INSERT INTO statement is used to fill a table with the result of subselect. commonly used to persist predictions into the database","title":"Description"},{"location":"sql/api/insert/#syntax","text":"INSERT INTO [ integration_name ].[ table_name ] [ SELECT ...] It performs a subselect [ SELECT ...] and gets data from it there after it performs INSERT INTO TABLE [ table_name ] of integration [integration_name]","title":"Syntax"},{"location":"sql/api/insert/#example","text":"In this example we want to persist the predictions into a table int1 . tbl1 . Given the following schema: int1 \u2514\u2500\u2500 tbl1 mindsdb \u2514\u2500\u2500 predictor_name int2 \u2514\u2500\u2500 tbl2 Where: Description int1 Integration for the table to be created in tbl1 Table to be created predictor_name Name of the model to be used int2 Database to be used as a source in the inner SELECT tbl2 Table to be used as a source. In order to achive the desired result we could execute the following query: INSERT INTO int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' )","title":"Example"},{"location":"sql/api/join/","text":"JOIN Statement \u00b6 Description \u00b6 The JOIN clause is used to combine rows from the database table and the model table on a related column. This can be very helpful to get bulk predictions. The basic syntax for joining from the data table and model is: Syntax \u00b6 SELECT t .[ column_name ], p .[ column_name ] ... FROM [ integration_name ].[ table ] AS t JOIN mindsdb .[ predictor_name ] AS p On execution: + -----------------+-----------------+ | t .[ column_name ] | p .[ column_name ] | + -----------------+-----------------+ | t .[ value ] | p . value | + -----------------+-----------------+ Where: Description [integration_name].[table] Name of the table te be used as input for the prediction mindsdb.[predictor_name] Name of the model to be used to predict p.value prediction value Example \u00b6 The following SQL statement joins the home_rentals data with the home_rentals_model predicted price: SELECT t . rental_price as real_price , m . rental_price as predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals as t JOIN mindsdb . home_rentals_model as m LIMIT 100 + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | | 4382 | 4388 | 3 | 2 | 816 | poor | 25 | | 2269 | 2272 | 0 | 1 | 461 | great | 6 | | 2284 | 2272 | 1 | 1 | 333 | great | 6 | | 5420 | 5437 | 3 | 2 | 1124 | great | 9 | | 5016 | 4998 | 3 | 2 | 1204 | good | 7 | | 1421 | 1427 | 0 | 1 | 538 | poor | 43 | | 3476 | 3466 | 2 | 1 | 890 | good | 6 | | 5271 | 5255 | 3 | 2 | 975 | great | 6 | | 3001 | 2993 | 2 | 1 | 564 | good | 13 | | 4682 | 4692 | 3 | 2 | 953 | good | 10 | | 1783 | 1738 | 1 | 1 | 493 | poor | 24 | | 1548 | 1543 | 1 | 1 | 601 | poor | 47 | | 1492 | 1491 | 0 | 1 | 191 | good | 12 | | 2431 | 2419 | 0 | 1 | 511 | great | 1 | | 4237 | 4257 | 3 | 2 | 916 | poor | 36 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ Example Time Series \u00b6 Having a time series predictor trained via: CREATE PREDICTOR mindsdb . house_sales_model FROM example_db ( SELECT * FROM demo_data . house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms , type -- as the target is quarterly, we will look back two years to forecast the next one WINDOW 8 HORIZON 4 ; You can query it and get the forecast predictions like: SELECT m . saledate as date , m . ma as forecast FROM mindsdb . house_sales_model as m JOIN example_db . demo_data . house_sales as t WHERE t . saledate > LATEST AND t . type = 'house' LIMIT 4 ;","title":"JOIN"},{"location":"sql/api/join/#join-statement","text":"","title":"JOIN Statement"},{"location":"sql/api/join/#description","text":"The JOIN clause is used to combine rows from the database table and the model table on a related column. This can be very helpful to get bulk predictions. The basic syntax for joining from the data table and model is:","title":"Description"},{"location":"sql/api/join/#syntax","text":"SELECT t .[ column_name ], p .[ column_name ] ... FROM [ integration_name ].[ table ] AS t JOIN mindsdb .[ predictor_name ] AS p On execution: + -----------------+-----------------+ | t .[ column_name ] | p .[ column_name ] | + -----------------+-----------------+ | t .[ value ] | p . value | + -----------------+-----------------+ Where: Description [integration_name].[table] Name of the table te be used as input for the prediction mindsdb.[predictor_name] Name of the model to be used to predict p.value prediction value","title":"Syntax"},{"location":"sql/api/join/#example","text":"The following SQL statement joins the home_rentals data with the home_rentals_model predicted price: SELECT t . rental_price as real_price , m . rental_price as predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals as t JOIN mindsdb . home_rentals_model as m LIMIT 100 + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | | 4382 | 4388 | 3 | 2 | 816 | poor | 25 | | 2269 | 2272 | 0 | 1 | 461 | great | 6 | | 2284 | 2272 | 1 | 1 | 333 | great | 6 | | 5420 | 5437 | 3 | 2 | 1124 | great | 9 | | 5016 | 4998 | 3 | 2 | 1204 | good | 7 | | 1421 | 1427 | 0 | 1 | 538 | poor | 43 | | 3476 | 3466 | 2 | 1 | 890 | good | 6 | | 5271 | 5255 | 3 | 2 | 975 | great | 6 | | 3001 | 2993 | 2 | 1 | 564 | good | 13 | | 4682 | 4692 | 3 | 2 | 953 | good | 10 | | 1783 | 1738 | 1 | 1 | 493 | poor | 24 | | 1548 | 1543 | 1 | 1 | 601 | poor | 47 | | 1492 | 1491 | 0 | 1 | 191 | good | 12 | | 2431 | 2419 | 0 | 1 | 511 | great | 1 | | 4237 | 4257 | 3 | 2 | 916 | poor | 36 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+","title":"Example"},{"location":"sql/api/join/#example-time-series","text":"Having a time series predictor trained via: CREATE PREDICTOR mindsdb . house_sales_model FROM example_db ( SELECT * FROM demo_data . house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms , type -- as the target is quarterly, we will look back two years to forecast the next one WINDOW 8 HORIZON 4 ; You can query it and get the forecast predictions like: SELECT m . saledate as date , m . ma as forecast FROM mindsdb . house_sales_model as m JOIN example_db . demo_data . house_sales as t WHERE t . saledate > LATEST AND t . type = 'house' LIMIT 4 ;","title":"Example Time Series"},{"location":"sql/api/publish/","text":"PUBLISH statement \u00b6","title":"PUBLISH statement"},{"location":"sql/api/publish/#publish-statement","text":"","title":"PUBLISH statement"},{"location":"sql/api/retrain/","text":"RETRAIN Statement \u00b6 Description \u00b6 The RETRAIN statement is used to retrain old predictors. The predictor is updated to leverage any new data in optimizing its predictive capabilities, without necessarily taking as long to train as starting from scratch. The basic syntax for retraining the predictors is: Syntax \u00b6 RETRAIN mindsdb .[ predictor_name ]; On execution: Query OK , 0 rows affected ( 0 . 058 sec ) Validation \u00b6 SELECT name , update_status FROM mindsdb . predictors WHERE name = '[predictor_name]' ; On execution: + ------------------+---------------+ | name | update_status | + ------------------+---------------+ | [ predictor_name ] | up_to_date | + ------------------+---------------+ Where: Description [predictor_name] Name of the model to be retrained update_status Column from mindsdb . predictors that informs if the model can be retrained or not Example \u00b6 Validating Prior Status \u00b6 SELECT name , update_status FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; On execution: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | available | + --------------------+---------------+ Note the value for update_status is available Retraining model \u00b6 RETRAIN home_rentals_model ; On execution: Query OK , 0 rows affected ( 0 . 058 sec ) Validating Resulting Status \u00b6 SELECT name , update_status FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; On execution: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | up_to_date | + --------------------+---------------+","title":"RETRAIN"},{"location":"sql/api/retrain/#retrain-statement","text":"","title":"RETRAIN Statement"},{"location":"sql/api/retrain/#description","text":"The RETRAIN statement is used to retrain old predictors. The predictor is updated to leverage any new data in optimizing its predictive capabilities, without necessarily taking as long to train as starting from scratch. The basic syntax for retraining the predictors is:","title":"Description"},{"location":"sql/api/retrain/#syntax","text":"RETRAIN mindsdb .[ predictor_name ]; On execution: Query OK , 0 rows affected ( 0 . 058 sec )","title":"Syntax"},{"location":"sql/api/retrain/#validation","text":"SELECT name , update_status FROM mindsdb . predictors WHERE name = '[predictor_name]' ; On execution: + ------------------+---------------+ | name | update_status | + ------------------+---------------+ | [ predictor_name ] | up_to_date | + ------------------+---------------+ Where: Description [predictor_name] Name of the model to be retrained update_status Column from mindsdb . predictors that informs if the model can be retrained or not","title":"Validation"},{"location":"sql/api/retrain/#example","text":"","title":"Example"},{"location":"sql/api/retrain/#validating-prior-status","text":"SELECT name , update_status FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; On execution: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | available | + --------------------+---------------+ Note the value for update_status is available","title":"Validating Prior Status"},{"location":"sql/api/retrain/#retraining-model","text":"RETRAIN home_rentals_model ; On execution: Query OK , 0 rows affected ( 0 . 058 sec )","title":"Retraining model"},{"location":"sql/api/retrain/#validating-resulting-status","text":"SELECT name , update_status FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; On execution: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | up_to_date | + --------------------+---------------+","title":"Validating Resulting Status"},{"location":"sql/api/select/","text":"SELECT statement \u00b6 Description \u00b6 The SELECT statement is used to get a predictions from the model table. The data is not persistent and is returned on the fly as a result-set. The basic syntax for selecting from the model is: Syntax \u00b6 SELECT [ target_variable ], [ target_variable ] _explain FROM mindsdb .[ predictor_name ] WHERE [ column ] = [ value ] AND [ column ] = [ value ]; Grammar matters Ensure that there are no spaces between the column name, equal sign and value. Ensure to not use any quotations for numerical values and singular quotes for strings On execution, you should get: + ----------+----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | [ column ] | [ column ] | [ target_variable ] | [ target_variable ] _explain | + ----------+----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | [ value ] | [ value ] | [ predicted_value ] | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + ----------+----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Where: Expressions Description [target_variable] Name of the column to be predicted [target_variable] _explain JSON object that contains additional information as confidence_lower_bound , confidence_upper_bound , anomaly , truth . [predictor_name] Name of the model to be used to make the prediction WHERE [ column ] = [ value ] AND ... WHERE clause used to pass the input data to make the prediction Example \u00b6 The following SQL statement selects a rental_price prediction from the home_rentals_model for a property that has the attributes named after the WHERE expression: SELECT location , neighborhood , days_on_market , rental_price , rental_price_explain FROM mindsdb . home_rentals_model1 WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, + ----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | location | neighborhood | days_on_market | rental_price | rental_price_explain | + ----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | good | downtown | 10 | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + ----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Bulk predictions You can also make bulk predictions by joining a table with your model: SELECT t . rental_price as real_price , m . rental_price as predicted_price , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals as t JOIN mindsdb . home_rentals_model as m limit 100","title":"SELECT"},{"location":"sql/api/select/#select-statement","text":"","title":"SELECT statement"},{"location":"sql/api/select/#description","text":"The SELECT statement is used to get a predictions from the model table. The data is not persistent and is returned on the fly as a result-set. The basic syntax for selecting from the model is:","title":"Description"},{"location":"sql/api/select/#syntax","text":"SELECT [ target_variable ], [ target_variable ] _explain FROM mindsdb .[ predictor_name ] WHERE [ column ] = [ value ] AND [ column ] = [ value ]; Grammar matters Ensure that there are no spaces between the column name, equal sign and value. Ensure to not use any quotations for numerical values and singular quotes for strings On execution, you should get: + ----------+----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | [ column ] | [ column ] | [ target_variable ] | [ target_variable ] _explain | + ----------+----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | [ value ] | [ value ] | [ predicted_value ] | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + ----------+----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Where: Expressions Description [target_variable] Name of the column to be predicted [target_variable] _explain JSON object that contains additional information as confidence_lower_bound , confidence_upper_bound , anomaly , truth . [predictor_name] Name of the model to be used to make the prediction WHERE [ column ] = [ value ] AND ... WHERE clause used to pass the input data to make the prediction","title":"Syntax"},{"location":"sql/api/select/#example","text":"The following SQL statement selects a rental_price prediction from the home_rentals_model for a property that has the attributes named after the WHERE expression: SELECT location , neighborhood , days_on_market , rental_price , rental_price_explain FROM mindsdb . home_rentals_model1 WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, + ----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | location | neighborhood | days_on_market | rental_price | rental_price_explain | + ----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | good | downtown | 10 | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + ----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Bulk predictions You can also make bulk predictions by joining a table with your model: SELECT t . rental_price as real_price , m . rental_price as predicted_price , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals as t JOIN mindsdb . home_rentals_model as m limit 100","title":"Example"},{"location":"sql/api/select_files/","text":"SELECT files .[ file ] Statement \u00b6 Description \u00b6 The SELECT from files .[ file ] statement is used to select a [ file ] as a datasource. The main use is to create a predictor from a file that has been uploaded to MindsDB via the MindsDB Editor . Before using the SELECT files .[ file ] Make sure to upload the file via the MindsDB Editor Syntax \u00b6 SELECT * FROM files .[ file_name ]; On execution: + --------+--------+--------+--------+ | column | column | column | column | + --------+--------+--------+--------+ | value | value | value | value | + --------+--------+--------+--------+ Where: Description [file_name] Name of file uploaded to mindsDB via the [MindsDB SQL Editor ](/connect/mindsdb_editor/) column Name of the column depending on the file uploaded value Value depending on the file uploaded Example \u00b6 This example will show how to upload a file to MindsDB Cloud and use it to create a predictor. Upload file to MindsDB Editor \u00b6 Connect to the MindsDB Editor Navigate to Add Data located on the right navigation bar identified by a plug icon. Click on the tab Files and the card Import File Name your file in Table name . Click on Save and Continue . Select the file as datasource \u00b6 SELECT * FROM files . home_rentals Limit 10 ; + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | 0 | 1 | 484 , 8 | great | 10 | 2271 | south_side | 2271 | | 1 | 1 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 | 1 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 | 1 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 | 2 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 | 1 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 | 2 | 1190 | poor | 58 | 4463 | westbrae | 4123 . 812 | | 1 | 1 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 | 1 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 | 1 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ Create a predictor from file \u00b6 Query: CREATE PREDICTOR mindsdb .[ predictor_name ] FROM files ( SELECT * FROM [ file_name ]) PREDICT [ target_variable ]; Example CREATE PREDICTOR mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ;","title":"SELECT from files"},{"location":"sql/api/select_files/#select-filesfile-statement","text":"","title":"SELECT files.[file] Statement"},{"location":"sql/api/select_files/#description","text":"The SELECT from files .[ file ] statement is used to select a [ file ] as a datasource. The main use is to create a predictor from a file that has been uploaded to MindsDB via the MindsDB Editor . Before using the SELECT files .[ file ] Make sure to upload the file via the MindsDB Editor","title":"Description"},{"location":"sql/api/select_files/#syntax","text":"SELECT * FROM files .[ file_name ]; On execution: + --------+--------+--------+--------+ | column | column | column | column | + --------+--------+--------+--------+ | value | value | value | value | + --------+--------+--------+--------+ Where: Description [file_name] Name of file uploaded to mindsDB via the [MindsDB SQL Editor ](/connect/mindsdb_editor/) column Name of the column depending on the file uploaded value Value depending on the file uploaded","title":"Syntax"},{"location":"sql/api/select_files/#example","text":"This example will show how to upload a file to MindsDB Cloud and use it to create a predictor.","title":"Example"},{"location":"sql/api/select_files/#upload-file-to-mindsdb-editor","text":"Connect to the MindsDB Editor Navigate to Add Data located on the right navigation bar identified by a plug icon. Click on the tab Files and the card Import File Name your file in Table name . Click on Save and Continue .","title":"Upload file to MindsDB Editor"},{"location":"sql/api/select_files/#select-the-file-as-datasource","text":"SELECT * FROM files . home_rentals Limit 10 ; + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | 0 | 1 | 484 , 8 | great | 10 | 2271 | south_side | 2271 | | 1 | 1 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 | 1 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 | 1 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 | 2 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 | 1 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 | 2 | 1190 | poor | 58 | 4463 | westbrae | 4123 . 812 | | 1 | 1 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 | 1 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 | 1 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+","title":"Select the file as datasource"},{"location":"sql/api/select_files/#create-a-predictor-from-file","text":"Query: CREATE PREDICTOR mindsdb .[ predictor_name ] FROM files ( SELECT * FROM [ file_name ]) PREDICT [ target_variable ]; Example CREATE PREDICTOR mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ;","title":"Create a predictor from file"},{"location":"sql/api/stream/","text":"Work in progress This documentation is in progress. If you want to get access to the beta version, reach out to us on Slack .","title":"Stream"},{"location":"sql/api/use/","text":"USE statement \u00b6 The use integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to preview or SELECT from your database. If you haven't created a datasource after connecting to your database check out the simple steps explained here . Preview the data \u00b6 To connect to your database use the created datasource: use integration_name Then, simply SELECT from the tables: SELECT * FROM table_name ;","title":"USE statement"},{"location":"sql/api/use/#use-statement","text":"The use integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to preview or SELECT from your database. If you haven't created a datasource after connecting to your database check out the simple steps explained here .","title":"USE statement"},{"location":"sql/api/use/#preview-the-data","text":"To connect to your database use the created datasource: use integration_name Then, simply SELECT from the tables: SELECT * FROM table_name ;","title":"Preview the data"},{"location":"sql/create/databases/","text":"CREATE DATABASE Statement \u00b6 Description \u00b6 MindsDB enables connections to your favorite databases, data warehouses, data lakes, via the CREATE DATABASE syntax. Our MindsDB SQL API supports creating a database connection by passing any credentials needed by each type of system that you are connecting to. Syntax \u00b6 CREATE DATABASE [ datasource_name ] WITH engine = [ engine_string ], parameters = { \"key\" : \"value\" , ... } ; On execution, you should get: Query OK , 0 rows affected ( x . xxx sec ) Where: Description [datasource_name] Identifier for the datasource to be created [engine_string] Engine to be selected depending on the database connection parameters { \"key\" : \"value\" } object with the conection parametes especific for each engine Example \u00b6 Here is a concrete example on how to connect to a MySQL database. CREATE DATABASE mysql_datasource WITH engine = 'mariadb' , parameters = { \"user\" : \"root\" , \"port\" : 3307 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"my_database\" } ; On execution: Query OK , 0 rows affected ( 8 . 878 sec ) Listing Linked DATABASES \u00b6 You can list linked databases as follows: SHOW DATABASES ; On execution: + --------------------+ | Database | + --------------------+ | information_schema | | mindsdb | | files | | views | | example_db | + --------------------+ Getting Linked DATABASES Metadata \u00b6 You can also get metadata about the linked databases in mindsdb.datasources : SELECT * FROM mindsdb . datasources ; On execution: + ------------+---------------+--------------+------+-----------+ | name | database_type | host | port | user | + ------------+---------------+--------------+------+-----------+ | example_db | postgres | 3 . 220 . 66 . 106 | 5432 | demo_user | + ------------+---------------+--------------+------+-----------+ Supported Integrations \u00b6 Snowflake \u00b6 CREATE DATABASE snowflake_datasource WITH engine = 'snowflake' , parameters = { \"user\" : \"user\" , \"port\" : 443 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"snowflake\" , \"account\" : \"account\" , \"schema\" : \"public\" , \"protocol\" : \"https\" , \"warehouse\" : \"warehouse\" } ; Singlestore \u00b6 CREATE DATABASE singlestore_datasource WITH engine = 'singlestore' , parameters = { \"user\" : \"root\" , \"port\" : 3306 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"singlestore\" } ; MySQL \u00b6 CREATE DATABASE mysql_datasource WITH engine = 'mysql' , parameters = { \"user\" : \"root\" , \"port\" : 3306 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"mysql\" } ; ClickHouse \u00b6 CREATE DATABASE clickhouse_datasource WITH engine = 'clickhouse' , parameters = { \"user\" : \"default\" , \"port\" : 9000 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"default\" } ; PostgreSQL \u00b6 CREATE DATABASE psql_datasource WITH engine = 'postgres' , parameters = { \"user\" : \"postgres\" , \"port\" : 5432 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"postgres\" } ; Cassandra \u00b6 CREATE DATABASE psql_datasource WITH engine = 'cassandra' , parameters = { \"user\" : \"cassandra\" , \"port\" : 9042 , \"password\" : \"cassandra\" , \"host\" : \"127.0.0.1\" , \"database\" : \"keyspace\" } ; MariaDB \u00b6 CREATE DATABASE maria_datasource WITH engine = 'mariadb' , parameters = { \"user\" : \"root\" , \"port\" : 3306 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"mariadb\" } ; Microsoft SQL Server \u00b6 CREATE DATABASE mssql_datasource WITH engine = 'mssql' , parameters = { \"user\" : \"sa\" , \"port\" : 1433 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"master\" } ; Scylladb \u00b6 CREATE DATABASE scylladb_datasource WITH engine = 'scylladb' , parameters = { \"user\" : \"scylladb\" , \"port\" : 9042 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"scylladb\" } ; Trino \u00b6 CREATE DATABASE trino_datasource WITH engine = 'trinodb' , parameters = { \"user\" : \"trino\" , \"port\" : 8080 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"catalog\" : \"default\" , \"schema\" : \"test\" } ; QuestDB \u00b6 CREATE DATABASE questdb_datasource WITH engine = 'questdb' , parameters = { \"user\" : \"admin\" , \"port\" : 8812 , \"password\" : \"quest\" , \"host\" : \"127.0.0.1\" , \"database\" : \"qdb\" } ; Work in progress Note this feature is in beta version. If you have additional questions about other supported datasources or you expirience some issues reach out to us on Slack or open GitHub issue.","title":"DATABASE"},{"location":"sql/create/databases/#create-database-statement","text":"","title":"CREATE DATABASE Statement"},{"location":"sql/create/databases/#description","text":"MindsDB enables connections to your favorite databases, data warehouses, data lakes, via the CREATE DATABASE syntax. Our MindsDB SQL API supports creating a database connection by passing any credentials needed by each type of system that you are connecting to.","title":"Description"},{"location":"sql/create/databases/#syntax","text":"CREATE DATABASE [ datasource_name ] WITH engine = [ engine_string ], parameters = { \"key\" : \"value\" , ... } ; On execution, you should get: Query OK , 0 rows affected ( x . xxx sec ) Where: Description [datasource_name] Identifier for the datasource to be created [engine_string] Engine to be selected depending on the database connection parameters { \"key\" : \"value\" } object with the conection parametes especific for each engine","title":"Syntax"},{"location":"sql/create/databases/#example","text":"Here is a concrete example on how to connect to a MySQL database. CREATE DATABASE mysql_datasource WITH engine = 'mariadb' , parameters = { \"user\" : \"root\" , \"port\" : 3307 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"my_database\" } ; On execution: Query OK , 0 rows affected ( 8 . 878 sec )","title":"Example"},{"location":"sql/create/databases/#listing-linked-databases","text":"You can list linked databases as follows: SHOW DATABASES ; On execution: + --------------------+ | Database | + --------------------+ | information_schema | | mindsdb | | files | | views | | example_db | + --------------------+","title":"Listing Linked DATABASES"},{"location":"sql/create/databases/#getting-linked-databases-metadata","text":"You can also get metadata about the linked databases in mindsdb.datasources : SELECT * FROM mindsdb . datasources ; On execution: + ------------+---------------+--------------+------+-----------+ | name | database_type | host | port | user | + ------------+---------------+--------------+------+-----------+ | example_db | postgres | 3 . 220 . 66 . 106 | 5432 | demo_user | + ------------+---------------+--------------+------+-----------+","title":"Getting Linked DATABASES Metadata"},{"location":"sql/create/databases/#supported-integrations","text":"","title":"Supported Integrations"},{"location":"sql/create/databases/#snowflake","text":"CREATE DATABASE snowflake_datasource WITH engine = 'snowflake' , parameters = { \"user\" : \"user\" , \"port\" : 443 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"snowflake\" , \"account\" : \"account\" , \"schema\" : \"public\" , \"protocol\" : \"https\" , \"warehouse\" : \"warehouse\" } ;","title":"Snowflake"},{"location":"sql/create/databases/#singlestore","text":"CREATE DATABASE singlestore_datasource WITH engine = 'singlestore' , parameters = { \"user\" : \"root\" , \"port\" : 3306 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"singlestore\" } ;","title":"Singlestore"},{"location":"sql/create/databases/#mysql","text":"CREATE DATABASE mysql_datasource WITH engine = 'mysql' , parameters = { \"user\" : \"root\" , \"port\" : 3306 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"mysql\" } ;","title":"MySQL"},{"location":"sql/create/databases/#clickhouse","text":"CREATE DATABASE clickhouse_datasource WITH engine = 'clickhouse' , parameters = { \"user\" : \"default\" , \"port\" : 9000 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"default\" } ;","title":"ClickHouse"},{"location":"sql/create/databases/#postgresql","text":"CREATE DATABASE psql_datasource WITH engine = 'postgres' , parameters = { \"user\" : \"postgres\" , \"port\" : 5432 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"postgres\" } ;","title":"PostgreSQL"},{"location":"sql/create/databases/#cassandra","text":"CREATE DATABASE psql_datasource WITH engine = 'cassandra' , parameters = { \"user\" : \"cassandra\" , \"port\" : 9042 , \"password\" : \"cassandra\" , \"host\" : \"127.0.0.1\" , \"database\" : \"keyspace\" } ;","title":"Cassandra"},{"location":"sql/create/databases/#mariadb","text":"CREATE DATABASE maria_datasource WITH engine = 'mariadb' , parameters = { \"user\" : \"root\" , \"port\" : 3306 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"mariadb\" } ;","title":"MariaDB"},{"location":"sql/create/databases/#microsoft-sql-server","text":"CREATE DATABASE mssql_datasource WITH engine = 'mssql' , parameters = { \"user\" : \"sa\" , \"port\" : 1433 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"master\" } ;","title":"Microsoft SQL Server"},{"location":"sql/create/databases/#scylladb","text":"CREATE DATABASE scylladb_datasource WITH engine = 'scylladb' , parameters = { \"user\" : \"scylladb\" , \"port\" : 9042 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"database\" : \"scylladb\" } ;","title":"Scylladb"},{"location":"sql/create/databases/#trino","text":"CREATE DATABASE trino_datasource WITH engine = 'trinodb' , parameters = { \"user\" : \"trino\" , \"port\" : 8080 , \"password\" : \"Mimzo3i-mxt@9CpThpBj\" , \"host\" : \"127.0.0.1\" , \"catalog\" : \"default\" , \"schema\" : \"test\" } ;","title":"Trino"},{"location":"sql/create/databases/#questdb","text":"CREATE DATABASE questdb_datasource WITH engine = 'questdb' , parameters = { \"user\" : \"admin\" , \"port\" : 8812 , \"password\" : \"quest\" , \"host\" : \"127.0.0.1\" , \"database\" : \"qdb\" } ; Work in progress Note this feature is in beta version. If you have additional questions about other supported datasources or you expirience some issues reach out to us on Slack or open GitHub issue.","title":"QuestDB"},{"location":"sql/create/predictor/","text":"CREATE PREDICTOR Statement \u00b6 Description \u00b6 The CREATE PREDICTOR statement is used to train a new model. The basic syntax for training a model is: Syntax \u00b6 CREATE PREDICTOR mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ column_name , ...] FROM [ table_name ]) PREDICT [ target_column ] On execution, you should get: Query OK , 0 rows affected ( x . xxx sec ) Where: Expressions Description [predictor_name] Name of the model to be created [integration_name] is the name of the intergration created via CREATE DATABASE or file upload (SELECT [column_name, ...] FROM [table_name]) SELECT statement for selecting the data to be used for training and validation PREDICT [target_column] where target_column is the column name of the target variable. Checking the status of the model After you run the CREATE PREDICTOR statement, you can check the status of the training model, by selecting from the mindsdb . predictors SELECT * FROM mindsdb . predictors WHE RE name = '[predictor_name]' ; Example \u00b6 This example shows how you can train a Machine Learning model called home_rentals_model to predict the rental prices for real estate properties inside the dataset. CREATE PREDICTOR mindsdb . home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; On execution: Query OK , 0 rows affected ( 8 . 878 sec ) To check the predictor status query the mindsdb . predictors : SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; On execution, + -----------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + -----------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ | home_rentals123 | complete | 0 . 9991920992432087 | rental_price | up_to_date | 22 . 5 . 1 . 0 | NULL | | | + -----------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ ... USING Statement \u00b6 ... USING Description \u00b6 In MindsDB, the underlying AutoML models are based on Lightwood. This library generates models automatically based on the data and a declarative problem definition, but the default configuration can be overridden. The USING ... statement provides the option to configure a model to be trained with specific options. ... USING Statement Syntax \u00b6 CREATE PREDICTOR mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ column_name , ...] FROM [ table_name ]) PREDICT [ target_column ] USING [ parameter_key ] = [ 'parameter_value' ] parameter key Description encoders Grants access to configure how each column is encoded.By default, the AutoML engine will try to get the best match for the data. To learn more about how encoders work and their options, go here . model Allows you to specify what type of Machine Learning algorithm to learn from the encoder data. To learn more about all the model options, go here . Other keys supported by lightwood in JsonAI The most common usecases for configuring predictors will be listed and explained in the example below. To see all options available in detail, you should checkout the lightwood docs about JsonAI ... USING encoders Key \u00b6 Grants access to configure how each column is encoded. To learn more about how encoders work and their options, go here . ... USING encoders .[ column_name ]. module = 'value' ; By default, the AutoML engine will try to get the best match for the data. ... USING model Key \u00b6 Allows you to specify what type of Machine Learning algorithm to learn from the encoder data. To learn more about all the model options, go here ... USING model . args = '{\"key\": value}' ; ... USING Example \u00b6 We will use the home rentals dataset, specifying particular encoders for some of the columns and a LightGBM model. CREATE PREDICTOR mindsdb . home_rentals_predictor FROM my_db_integration ( SELECT * FROM home_rentals ) PREDICT rental_price USING encoders . location . module = 'CategoricalAutoEncoder' , encoders . rental_price . module = 'NumericEncoder' , encoders . rental_price . args . positive_domain = 'True' , model . args = '{\"submodels\":[ {\"module\": \"LightGBM\", \"args\": { \"stop_after\": 12, \"fit_on_dev\": true } } ]}' ; CREATE PREDICTOR From file \u00b6 CREATE PREDICTOR Description \u00b6 To train a model using a file: CREATE PREDICTOR Syntax \u00b6 CREATE PREDICTOR mindsdb .[ predictor_name ] FROM files ( SELECT * FROM [ file_name ]) PREDICT target_variable ; Where: Description [predictor_name] Name of the model to be created [file_name] Name of the file uploaded via the MindsDB editor (SELECT * FROM [file_name]) SELECT statement for selecting the data to be used for traning and validation target_variable target_column is the column name of the target variable. On execution, Query OK , 0 rows affected ( 8 . 878 sec ) CREATE PREDICTOR Example \u00b6 CREATE PREDICTOR mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ; CREATE PREDICTOR For Time Series Models \u00b6 CREATE PREDICTOR For Time Series Models Description \u00b6 To train a timeseries model, MindsDB provides additional statements. CREATE PREDICTOR For Time Series Models Syntax \u00b6 CREATE PREDICTOR mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ sequential_column ], [ partition_column ], [ other_column ], [ target_column ] FROM [ table_name ]) PREDICT [ target_column ] ORDER BY [ sequantial_column ] GROUP BY [ partition_column ] WINDOW [ int ] HORIZON [ int ]; Where: Expressions Description ORDER BY [sequantial_column] Defines the column that the time series will be order by. These can be a date, or anything that defines the sequence of events. GROUP BY [partition_column] (optional) Groups the rows that make a partition, for example, if you want to forecast inventory for all items in a store, you can partition the data by product_id, meaning that each product_id has its own time series. WINDOW [int] Specifies the number [int] of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be interpreted like \"Always use the previous 10 rows\". HORIZON [int] (optional) keyword specifies the number of future predictions, default value is 1 On execution, Query OK , 0 rows affected ( 8 . 878 sec ) Getting a prediction of a Time Series model Due to the nature of Time Series Forecasting you will need to use the JOIN statement to get results. CREATE PREDICTOR For Time Series Models Example \u00b6 CREATE PREDICTOR mindsdb . inventory_model FROM db_integration ( SELECT * FROM inventory ) as inventory PREDICT units_in_inventory as predicted_units_in_inventory ORDER BY date , GROUP BY product_id , WINDOW 20 HORIZON 7","title":"PREDICTOR"},{"location":"sql/create/predictor/#create-predictor-statement","text":"","title":"CREATE PREDICTOR Statement"},{"location":"sql/create/predictor/#description","text":"The CREATE PREDICTOR statement is used to train a new model. The basic syntax for training a model is:","title":"Description"},{"location":"sql/create/predictor/#syntax","text":"CREATE PREDICTOR mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ column_name , ...] FROM [ table_name ]) PREDICT [ target_column ] On execution, you should get: Query OK , 0 rows affected ( x . xxx sec ) Where: Expressions Description [predictor_name] Name of the model to be created [integration_name] is the name of the intergration created via CREATE DATABASE or file upload (SELECT [column_name, ...] FROM [table_name]) SELECT statement for selecting the data to be used for training and validation PREDICT [target_column] where target_column is the column name of the target variable. Checking the status of the model After you run the CREATE PREDICTOR statement, you can check the status of the training model, by selecting from the mindsdb . predictors SELECT * FROM mindsdb . predictors WHE RE name = '[predictor_name]' ;","title":"Syntax"},{"location":"sql/create/predictor/#example","text":"This example shows how you can train a Machine Learning model called home_rentals_model to predict the rental prices for real estate properties inside the dataset. CREATE PREDICTOR mindsdb . home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; On execution: Query OK , 0 rows affected ( 8 . 878 sec ) To check the predictor status query the mindsdb . predictors : SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; On execution, + -----------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + -----------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+ | home_rentals123 | complete | 0 . 9991920992432087 | rental_price | up_to_date | 22 . 5 . 1 . 0 | NULL | | | + -----------------+----------+--------------------+--------------+---------------+-----------------+-------+-------------------+------------------+","title":"Example"},{"location":"sql/create/predictor/#using-statement","text":"","title":"... USING Statement"},{"location":"sql/create/predictor/#usingdescription","text":"In MindsDB, the underlying AutoML models are based on Lightwood. This library generates models automatically based on the data and a declarative problem definition, but the default configuration can be overridden. The USING ... statement provides the option to configure a model to be trained with specific options.","title":"... USINGDescription"},{"location":"sql/create/predictor/#using-statement-syntax","text":"CREATE PREDICTOR mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ column_name , ...] FROM [ table_name ]) PREDICT [ target_column ] USING [ parameter_key ] = [ 'parameter_value' ] parameter key Description encoders Grants access to configure how each column is encoded.By default, the AutoML engine will try to get the best match for the data. To learn more about how encoders work and their options, go here . model Allows you to specify what type of Machine Learning algorithm to learn from the encoder data. To learn more about all the model options, go here . Other keys supported by lightwood in JsonAI The most common usecases for configuring predictors will be listed and explained in the example below. To see all options available in detail, you should checkout the lightwood docs about JsonAI","title":"... USING Statement Syntax"},{"location":"sql/create/predictor/#using-encoders-key","text":"Grants access to configure how each column is encoded. To learn more about how encoders work and their options, go here . ... USING encoders .[ column_name ]. module = 'value' ; By default, the AutoML engine will try to get the best match for the data.","title":"... USING encoders Key"},{"location":"sql/create/predictor/#using-model-key","text":"Allows you to specify what type of Machine Learning algorithm to learn from the encoder data. To learn more about all the model options, go here ... USING model . args = '{\"key\": value}' ;","title":"... USING model Key"},{"location":"sql/create/predictor/#using-example","text":"We will use the home rentals dataset, specifying particular encoders for some of the columns and a LightGBM model. CREATE PREDICTOR mindsdb . home_rentals_predictor FROM my_db_integration ( SELECT * FROM home_rentals ) PREDICT rental_price USING encoders . location . module = 'CategoricalAutoEncoder' , encoders . rental_price . module = 'NumericEncoder' , encoders . rental_price . args . positive_domain = 'True' , model . args = '{\"submodels\":[ {\"module\": \"LightGBM\", \"args\": { \"stop_after\": 12, \"fit_on_dev\": true } } ]}' ;","title":"... USING Example"},{"location":"sql/create/predictor/#create-predictor-from-file","text":"","title":"CREATE PREDICTOR From file"},{"location":"sql/create/predictor/#create-predictor-description","text":"To train a model using a file:","title":"CREATE PREDICTOR Description"},{"location":"sql/create/predictor/#create-predictor-syntax","text":"CREATE PREDICTOR mindsdb .[ predictor_name ] FROM files ( SELECT * FROM [ file_name ]) PREDICT target_variable ; Where: Description [predictor_name] Name of the model to be created [file_name] Name of the file uploaded via the MindsDB editor (SELECT * FROM [file_name]) SELECT statement for selecting the data to be used for traning and validation target_variable target_column is the column name of the target variable. On execution, Query OK , 0 rows affected ( 8 . 878 sec )","title":"CREATE PREDICTOR Syntax"},{"location":"sql/create/predictor/#create-predictor-example","text":"CREATE PREDICTOR mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ;","title":"CREATE PREDICTOR Example"},{"location":"sql/create/predictor/#create-predictor-for-time-series-models","text":"","title":"CREATE PREDICTOR For Time Series Models"},{"location":"sql/create/predictor/#create-predictor-for-time-series-models-description","text":"To train a timeseries model, MindsDB provides additional statements.","title":"CREATE PREDICTOR For Time Series Models Description"},{"location":"sql/create/predictor/#create-predictor-for-time-series-models-syntax","text":"CREATE PREDICTOR mindsdb .[ predictor_name ] FROM [ integration_name ] ( SELECT [ sequential_column ], [ partition_column ], [ other_column ], [ target_column ] FROM [ table_name ]) PREDICT [ target_column ] ORDER BY [ sequantial_column ] GROUP BY [ partition_column ] WINDOW [ int ] HORIZON [ int ]; Where: Expressions Description ORDER BY [sequantial_column] Defines the column that the time series will be order by. These can be a date, or anything that defines the sequence of events. GROUP BY [partition_column] (optional) Groups the rows that make a partition, for example, if you want to forecast inventory for all items in a store, you can partition the data by product_id, meaning that each product_id has its own time series. WINDOW [int] Specifies the number [int] of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be interpreted like \"Always use the previous 10 rows\". HORIZON [int] (optional) keyword specifies the number of future predictions, default value is 1 On execution, Query OK , 0 rows affected ( 8 . 878 sec ) Getting a prediction of a Time Series model Due to the nature of Time Series Forecasting you will need to use the JOIN statement to get results.","title":"CREATE PREDICTOR For Time Series Models Syntax"},{"location":"sql/create/predictor/#create-predictor-for-time-series-models-example","text":"CREATE PREDICTOR mindsdb . inventory_model FROM db_integration ( SELECT * FROM inventory ) as inventory PREDICT units_in_inventory as predicted_units_in_inventory ORDER BY date , GROUP BY product_id , WINDOW 20 HORIZON 7","title":"CREATE PREDICTOR For Time Series Models Example"},{"location":"sql/create/table/","text":"CREATE TABLE Statement \u00b6 Description \u00b6 The CREATE TABLE is used to create a table and fill it with the result of a subselect, usually used to materialize predictions into tables. Syntax \u00b6 CREATE [ { REPLACE } ] TABLE [ integration_name ].[ table_name ] [ SELECT ...] It performs a subselect [ SELECT ...] and gets data from it, thereafter it creates a table [ table_name ] in [ integration_name ] . lastly it performs an INSERT INTO [ integration_name ].[ table_name ] with the contents of the [ SELECT ...] REPLACE If REPLACE is indicated then [ integration_name ].[ table_name ] will be Dropped Example \u00b6 In this example we want to persist the predictions into a table int1 . tbl1 . Given the following schema: int1 \u2514\u2500\u2500 tbl1 mindsdb \u2514\u2500\u2500 predictor_name int2 \u2514\u2500\u2500 tbl2 Where: Description int1 Integration for the table to be created in tbl1 Table to be created predictor_name Name of the model to be used int2 Database to be used as a source in the inner SELECT tbl2 Table to be used as a source. In order to achive the desired result we could execute the following query: CREATE TABLE int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' )","title":"TABLE"},{"location":"sql/create/table/#create-table-statement","text":"","title":"CREATE TABLE Statement"},{"location":"sql/create/table/#description","text":"The CREATE TABLE is used to create a table and fill it with the result of a subselect, usually used to materialize predictions into tables.","title":"Description"},{"location":"sql/create/table/#syntax","text":"CREATE [ { REPLACE } ] TABLE [ integration_name ].[ table_name ] [ SELECT ...] It performs a subselect [ SELECT ...] and gets data from it, thereafter it creates a table [ table_name ] in [ integration_name ] . lastly it performs an INSERT INTO [ integration_name ].[ table_name ] with the contents of the [ SELECT ...] REPLACE If REPLACE is indicated then [ integration_name ].[ table_name ] will be Dropped","title":"Syntax"},{"location":"sql/create/table/#example","text":"In this example we want to persist the predictions into a table int1 . tbl1 . Given the following schema: int1 \u2514\u2500\u2500 tbl1 mindsdb \u2514\u2500\u2500 predictor_name int2 \u2514\u2500\u2500 tbl2 Where: Description int1 Integration for the table to be created in tbl1 Table to be created predictor_name Name of the model to be used int2 Database to be used as a source in the inner SELECT tbl2 Table to be used as a source. In order to achive the desired result we could execute the following query: CREATE TABLE int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' )","title":"Example"},{"location":"sql/create/view/","text":"CREATE VIEW Statement \u00b6 Description \u00b6 In MindsDB, an AI Table is a virtual table based on the result-set of the SQL Statement that JOIN s a table data with the predictions of a model. An AI Table can be created using the CREATE VIEW statement. Syntax \u00b6 CREATE VIEW mindsdb .[ ai_table_name ] as ( SELECT a .[ column_name1 ], a .[ column_name2 ], a .[ column_name3 ], p .[ model_column ] as model_column FROM [ integration_name ].[ table_name ] as a JOIN mindsdb .[ predictor_name ] as p ); Where: Expressions Description [ai_table_name] Name of the view to be created [column_name1], [column_name2] ... Name of the columns to be joined, input for the model to make a prediction [model_column] name of the target column for the predictions [integration_name].[table_name] where integration_name is the linked database and has it's table_name [predictor_name] Name of the model to be used to generate the predictions Example \u00b6 Having executed a SQL query for training a home_rentals_model that learns to predict the rental_price value given other features of a real estate listing: CREATE PREDICTOR mindsdb . home_rentals_model FROM integration_name ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; Once trained, we can JOIN any input data with the trained model and store the results as an AI Table using the CREATE VIEW syntax. Let's pass some of the expected input columns (in this case, sqft , number_of_bathrooms , location ) to the model and join the predicted rental_price values: CREATE VIEW mindsdb . home_rentals as ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price as price FROM mysql_db . home_rentals as a JOIN mindsdb . home_rentals_model as p ); Note that in this example, we pass part of the same data that was used to train as a test query, but usually you would create an AI table to store predictions for new data.","title":"VIEW"},{"location":"sql/create/view/#create-view-statement","text":"","title":"CREATE VIEW Statement"},{"location":"sql/create/view/#description","text":"In MindsDB, an AI Table is a virtual table based on the result-set of the SQL Statement that JOIN s a table data with the predictions of a model. An AI Table can be created using the CREATE VIEW statement.","title":"Description"},{"location":"sql/create/view/#syntax","text":"CREATE VIEW mindsdb .[ ai_table_name ] as ( SELECT a .[ column_name1 ], a .[ column_name2 ], a .[ column_name3 ], p .[ model_column ] as model_column FROM [ integration_name ].[ table_name ] as a JOIN mindsdb .[ predictor_name ] as p ); Where: Expressions Description [ai_table_name] Name of the view to be created [column_name1], [column_name2] ... Name of the columns to be joined, input for the model to make a prediction [model_column] name of the target column for the predictions [integration_name].[table_name] where integration_name is the linked database and has it's table_name [predictor_name] Name of the model to be used to generate the predictions","title":"Syntax"},{"location":"sql/create/view/#example","text":"Having executed a SQL query for training a home_rentals_model that learns to predict the rental_price value given other features of a real estate listing: CREATE PREDICTOR mindsdb . home_rentals_model FROM integration_name ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; Once trained, we can JOIN any input data with the trained model and store the results as an AI Table using the CREATE VIEW syntax. Let's pass some of the expected input columns (in this case, sqft , number_of_bathrooms , location ) to the model and join the predicted rental_price values: CREATE VIEW mindsdb . home_rentals as ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price as price FROM mysql_db . home_rentals as a JOIN mindsdb . home_rentals_model as p ); Note that in this example, we pass part of the same data that was used to train as a test query, but usually you would create an AI table to store predictions for new data.","title":"Example"},{"location":"sql/tutorials/ai-tables/","text":"AI Tables Intro \u00b6 There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence. Machine Learning (ML) Lifecycle \u00b6 The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database. Deep Dive into the AI Tables \u00b6 Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; A simple visualization of the data present in the income table is as follows. Querying the income table to get the debt value for a particular income value results in the following. SELECT income , debt FROM income WHERE income = 80000 ; But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; When a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play! Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE PREDICTOR mindsdb . debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows. Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table. SELECT income , debt FROM debt_model WHERE income = 90120 ;","title":"AI Tables Intro"},{"location":"sql/tutorials/ai-tables/#ai-tables-intro","text":"There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence.","title":"AI Tables Intro"},{"location":"sql/tutorials/ai-tables/#machine-learning-ml-lifecycle","text":"The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database.","title":"Machine Learning (ML) Lifecycle"},{"location":"sql/tutorials/ai-tables/#deep-dive-into-the-ai-tables","text":"Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; A simple visualization of the data present in the income table is as follows. Querying the income table to get the debt value for a particular income value results in the following. SELECT income , debt FROM income WHERE income = 80000 ; But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; When a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play! Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE PREDICTOR mindsdb . debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows. Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table. SELECT income , debt FROM debt_model WHERE income = 90120 ;","title":"Deep Dive into the AI Tables"},{"location":"sql/tutorials/bitcoin-forecasting/","text":"Forecast Bitcoin price using MindsDB \u00b6 Level: Easy Dataset: Coinbase 2017-2018 Bitcoin data Bitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk. In this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax caled JSON-AI, but we will not cover it in this article. DISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have. Pre-requisites \u00b6 First, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with Docker or Python . However, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb . In this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier. Second, you need a MySQL client to connect to MindsDB MYSQL API. Connect your database \u00b6 You must first connect MindsDB to the database where the record is stored. In the left navigation, click Database, click ADD DATABASE. And you must provide all the necessary parameters to connect to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction. Connect to MindsDB\u2019s MySQL API \u00b6 In this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data \u00b6 Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and in this tutorial, we will use the Bitcoin table. And let's check what is inside this table. These tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. The column we want to forecast is close price. Create the model \u00b6 Now, to create the model let's move to MindsDB database. and let's see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE PREDICTOR as a new record inside the predictors table, and using this command: CREATE PREDICTOR mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_target as column_alias ORDER BY column_orderby WINDOW num_window HORIZON num_horizon USING { \"is_timeseries\" : \"Yes\" } The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_target (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. column_orderby - The column to order the data, for time series this should be the date/time column. num_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". num_horizon - keyword specifies the number of future predictions. So, use this command to create the models: If there's no error, that means your model is created and training. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the model is finished, it will look like this. The model has been created! and the accuracy is 99%! Create the prediction \u00b6 Now you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; And you need to set these values: - target_variable - The original value of the target variable. - target_variable_confidence - Model confidence score. - target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. - when_data - The data to make the predictions from(WHERE clause params). Finally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy! Conclusions \u00b6 As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself! MindsDB free cloud account is fast to set up and has more than enough to give it a try. Or use the open source version if you want to.","title":"Forecast Bitcoin price using MindsDB"},{"location":"sql/tutorials/bitcoin-forecasting/#forecast-bitcoin-price-using-mindsdb","text":"Level: Easy Dataset: Coinbase 2017-2018 Bitcoin data Bitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk. In this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax caled JSON-AI, but we will not cover it in this article. DISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have.","title":"Forecast Bitcoin price using MindsDB"},{"location":"sql/tutorials/bitcoin-forecasting/#pre-requisites","text":"First, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with Docker or Python . However, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb . In this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier. Second, you need a MySQL client to connect to MindsDB MYSQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/bitcoin-forecasting/#connect-your-database","text":"You must first connect MindsDB to the database where the record is stored. In the left navigation, click Database, click ADD DATABASE. And you must provide all the necessary parameters to connect to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.","title":"Connect your database"},{"location":"sql/tutorials/bitcoin-forecasting/#connect-to-mindsdbs-mysql-api","text":"In this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/bitcoin-forecasting/#data","text":"Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and in this tutorial, we will use the Bitcoin table. And let's check what is inside this table. These tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. The column we want to forecast is close price.","title":"Data"},{"location":"sql/tutorials/bitcoin-forecasting/#create-the-model","text":"Now, to create the model let's move to MindsDB database. and let's see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE PREDICTOR as a new record inside the predictors table, and using this command: CREATE PREDICTOR mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_target as column_alias ORDER BY column_orderby WINDOW num_window HORIZON num_horizon USING { \"is_timeseries\" : \"Yes\" } The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_target (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. column_orderby - The column to order the data, for time series this should be the date/time column. num_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". num_horizon - keyword specifies the number of future predictions. So, use this command to create the models: If there's no error, that means your model is created and training. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the model is finished, it will look like this. The model has been created! and the accuracy is 99%!","title":"Create the model"},{"location":"sql/tutorials/bitcoin-forecasting/#create-the-prediction","text":"Now you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; And you need to set these values: - target_variable - The original value of the target variable. - target_variable_confidence - Model confidence score. - target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. - when_data - The data to make the predictions from(WHERE clause params). Finally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy!","title":"Create the prediction"},{"location":"sql/tutorials/bitcoin-forecasting/#conclusions","text":"As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself! MindsDB free cloud account is fast to set up and has more than enough to give it a try. Or use the open source version if you want to.","title":"Conclusions"},{"location":"sql/tutorials/bodyfat/","text":"Determining Body Fat Percentage \u00b6 Dataset: Body fat prediction Communtiy Author: Contip Machine Learning powered data analysis can be performed quickly and efficiently by MindsDB to enable individuals to make accurate predictions for certain metrics based on a variety of associated values. MindsDB enables you to make predictions automatically using just SQL commands, all the ML workflow is automated, and abstracted as virtual \u201cAI tables\u201d in your database so you may start getting insights from forecasts right away. In this tutorial, we'll be using MindsDB and a MySQL database to predict body fat percentage based on several body part measurement criteria. Pre-requisites \u00b6 Before you start make sure that you've: Visted Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. The dataset being used for this tutorial. Get it from Kaggle . Data Overview \u00b6 For this tutorial, we'll be using the Body Fat Prediction dataset available at Kaggle . Each row represents one person and we'll train an ML model to help us predict an individual's body fat percentage using MindsDB. Below is a short description of each feature of the data: Density: Individual's body density as determined by underwater weighing (float) BodyFat: The individual's determined body fat percentage (float). This is what we want to predict Age: Age of the individual (int) Weight: Weight of the individual in pounds (float) Height: Height of the individual in inches (float) Neck: Circumference of the individual's neck in cm (float) Chest: Circumference of the individual's chest in cm (float) Abdomen: Circumference of the individual's abdomen in cm (float) Hip: Circumference of the individual's hips in cm (float) Thigh: Circumference of the individual's thigh in cm (float) Knee: Circumference of the individual's knee in cm (float) Ankle: Circumference of the individual's ankle in cm (float) Biceps: Circumference of the individual's biceps in cm (float) Forearm: Circumference of the individual's forearm in cm (float) Wrist: Circumference of the individual's wrist in cm (float) Add data to MindsDB GUI \u00b6 MindsDB has a functionality to uploud your data file directly via the GUI where you can immediately query the data and create a machine learning model. The following is the steps to upload your data directly to MindsDB: Access the MindsDB GUI via cloud or local via the URL 127.0.0.1:47334/. Select the button Add data or select the plug icon on the left side bar. The page will navigate to 'Select your data source'. Select the option 'Files'. Select the tab under 'Import a file'. Please note the dataset files should not exceed the maximum size limit which is 10MB. Provide a name for the data file which will be saved as a table. Once you have successfully uploaded the file, you can query the data from the files table to ensure the information pulls through. Run the following syntax: SELECT * FROM files . bodyfat LIMIT 10 ; Once you have confirmed the file has successfully uploaded and the data can be retrieved, we can move on to creating a predictor. Create and train a machine learning model. \u00b6 With the CREATE PREDICTOR statement, we can create a machine learning model: CREATE PREDICTOR mindsdb . predictor_name FROM files ( SELECT column_name , column_name2 FROM file_name ) PREDICT column_name ; The required values that we need to provide are: \u200b - predictor_name (string): The name of the model - integration_name (string): The name of the connection to your database. - column_name (string): The feature you want to predict. For our case, we'll enter the following syntax: CREATE PREDICTOR bodyfat_predictor FROM files ( SELECT * FROM bodyfat ) PREDICT Bodyfat ; Select the Run button or select Shift+Enter to run the syntax. Once is is successful you will receive a message in the console 'Query successfully completed'. You should see output similar to the following: At this point, the predictor will immediately begin training. Check the status of the training by entering the command: SELECT * FROM mindsdb . predictors WHERE name = 'bodyfat_predictor' ; When complete, you should see output similar to the following: As you can see, the predictor training has been completed with an accuracy of approximately 99%. At this point, you have successfully trained an ML model for our Body Fat Prediction dataset! Using SQL Commands to Make Predictions \u00b6 Now, we can query the model and make predictions based on our input data by using SQL statements. Let's imagine an individual aged 25, with a body density of 1.08, a weight of 170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference of 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm, a thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle circumference of 22.8cm, a biceps circumference of 33.3cm, a forearm circumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict this person's body fat percentage by issuing the following command: SELECT BodyFat , BodyFat_confidence , BodyFat_explain FROM mindsdb . bodyfat_predictor WHERE Density = 1 . 08 AND Age = 25 AND Weight = 170 AND Height = 70 AND Neck = 38 . 1 AND Chest = 103 . 5 AND Abdomen = 85 . 4 AND Hip = 102 . 2 AND Thigh = 63 . 0 AND Knee = 39 . 4 AND Ankle = 22 . 8 AND Biceps = 33 . 3 AND Forearm = 28 . 7 AND Wrist = 18 . 3 ; This should return output similar to: As you can see, with around 99% confidence, MindsDB predicted the body fat percentage for this individual at 8.97%. You can at this point feel free to alter the prospective individual's bodypart measurement parameters and make additional prediction queries if you'd like. Making Batch Predictions using the JOIN syntax \u00b6 The above example showed how to make predictions for a single individual's bodyfat, but what if you had a table of bodypart measurements for a number of individuals, and wanted to make predictions for them all? This is possible using the JOIN command , which allows for the combining of rows from a database table and the prediction model table on a related column. The basic syntax to use the JOIN syntax is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our purposes, we'll re-use the original data set, taking the Age, Density, Weight, Height, Neck circumference, Chest circumference, Abdomen circumference, and Hip circumference fields. We'll also include the original BodyFat percentage to compare our predicted values against the originals. Execute the following command: SELECT t . Age , t . Density , t . Weight , t . Height , t . Neck , t . Chest , t . Abdomen , t . Hip , t . BodyFat , p . BodyFat AS predicted_BodyFat FROM bodyfat_integration . bodyfat AS t JOIN mindsdb . bodyfat_predictor AS p LIMIT 5 ; This should return an output table similar to the following: As you can see, a prediction has been generated for each row in the input table. Additionally, our predicted bodyfat percentages align closely with the original values! Note that even though we chose only to display the Age, Density, Weight, Height, Neck, Chest, Abdomen, and Hip measurements in this example, the predicted_BodyFat field was determined by taking into consideration all of the data fields in the original bodyfat table (as this table was JOINed with the bodyfat_predictor table, from which we selected the specified fields). In order to make predictions based ONLY on the specified fields, we would have to create a new table containing only those fields, and JOIN that with the bodyfat_predictor table! You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this check out MindsDB documentation .","title":"Body Fat Prediction"},{"location":"sql/tutorials/bodyfat/#determining-body-fat-percentage","text":"Dataset: Body fat prediction Communtiy Author: Contip Machine Learning powered data analysis can be performed quickly and efficiently by MindsDB to enable individuals to make accurate predictions for certain metrics based on a variety of associated values. MindsDB enables you to make predictions automatically using just SQL commands, all the ML workflow is automated, and abstracted as virtual \u201cAI tables\u201d in your database so you may start getting insights from forecasts right away. In this tutorial, we'll be using MindsDB and a MySQL database to predict body fat percentage based on several body part measurement criteria.","title":"Determining Body Fat Percentage"},{"location":"sql/tutorials/bodyfat/#pre-requisites","text":"Before you start make sure that you've: Visted Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. The dataset being used for this tutorial. Get it from Kaggle .","title":"Pre-requisites"},{"location":"sql/tutorials/bodyfat/#data-overview","text":"For this tutorial, we'll be using the Body Fat Prediction dataset available at Kaggle . Each row represents one person and we'll train an ML model to help us predict an individual's body fat percentage using MindsDB. Below is a short description of each feature of the data: Density: Individual's body density as determined by underwater weighing (float) BodyFat: The individual's determined body fat percentage (float). This is what we want to predict Age: Age of the individual (int) Weight: Weight of the individual in pounds (float) Height: Height of the individual in inches (float) Neck: Circumference of the individual's neck in cm (float) Chest: Circumference of the individual's chest in cm (float) Abdomen: Circumference of the individual's abdomen in cm (float) Hip: Circumference of the individual's hips in cm (float) Thigh: Circumference of the individual's thigh in cm (float) Knee: Circumference of the individual's knee in cm (float) Ankle: Circumference of the individual's ankle in cm (float) Biceps: Circumference of the individual's biceps in cm (float) Forearm: Circumference of the individual's forearm in cm (float) Wrist: Circumference of the individual's wrist in cm (float)","title":"Data Overview"},{"location":"sql/tutorials/bodyfat/#add-data-to-mindsdb-gui","text":"MindsDB has a functionality to uploud your data file directly via the GUI where you can immediately query the data and create a machine learning model. The following is the steps to upload your data directly to MindsDB: Access the MindsDB GUI via cloud or local via the URL 127.0.0.1:47334/. Select the button Add data or select the plug icon on the left side bar. The page will navigate to 'Select your data source'. Select the option 'Files'. Select the tab under 'Import a file'. Please note the dataset files should not exceed the maximum size limit which is 10MB. Provide a name for the data file which will be saved as a table. Once you have successfully uploaded the file, you can query the data from the files table to ensure the information pulls through. Run the following syntax: SELECT * FROM files . bodyfat LIMIT 10 ; Once you have confirmed the file has successfully uploaded and the data can be retrieved, we can move on to creating a predictor.","title":"Add data to MindsDB GUI"},{"location":"sql/tutorials/bodyfat/#create-and-train-a-machine-learning-model","text":"With the CREATE PREDICTOR statement, we can create a machine learning model: CREATE PREDICTOR mindsdb . predictor_name FROM files ( SELECT column_name , column_name2 FROM file_name ) PREDICT column_name ; The required values that we need to provide are: \u200b - predictor_name (string): The name of the model - integration_name (string): The name of the connection to your database. - column_name (string): The feature you want to predict. For our case, we'll enter the following syntax: CREATE PREDICTOR bodyfat_predictor FROM files ( SELECT * FROM bodyfat ) PREDICT Bodyfat ; Select the Run button or select Shift+Enter to run the syntax. Once is is successful you will receive a message in the console 'Query successfully completed'. You should see output similar to the following: At this point, the predictor will immediately begin training. Check the status of the training by entering the command: SELECT * FROM mindsdb . predictors WHERE name = 'bodyfat_predictor' ; When complete, you should see output similar to the following: As you can see, the predictor training has been completed with an accuracy of approximately 99%. At this point, you have successfully trained an ML model for our Body Fat Prediction dataset!","title":"Create and train a machine learning model."},{"location":"sql/tutorials/bodyfat/#using-sql-commands-to-make-predictions","text":"Now, we can query the model and make predictions based on our input data by using SQL statements. Let's imagine an individual aged 25, with a body density of 1.08, a weight of 170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference of 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm, a thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle circumference of 22.8cm, a biceps circumference of 33.3cm, a forearm circumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict this person's body fat percentage by issuing the following command: SELECT BodyFat , BodyFat_confidence , BodyFat_explain FROM mindsdb . bodyfat_predictor WHERE Density = 1 . 08 AND Age = 25 AND Weight = 170 AND Height = 70 AND Neck = 38 . 1 AND Chest = 103 . 5 AND Abdomen = 85 . 4 AND Hip = 102 . 2 AND Thigh = 63 . 0 AND Knee = 39 . 4 AND Ankle = 22 . 8 AND Biceps = 33 . 3 AND Forearm = 28 . 7 AND Wrist = 18 . 3 ; This should return output similar to: As you can see, with around 99% confidence, MindsDB predicted the body fat percentage for this individual at 8.97%. You can at this point feel free to alter the prospective individual's bodypart measurement parameters and make additional prediction queries if you'd like.","title":"Using SQL Commands to Make Predictions"},{"location":"sql/tutorials/bodyfat/#making-batch-predictions-using-the-join-syntax","text":"The above example showed how to make predictions for a single individual's bodyfat, but what if you had a table of bodypart measurements for a number of individuals, and wanted to make predictions for them all? This is possible using the JOIN command , which allows for the combining of rows from a database table and the prediction model table on a related column. The basic syntax to use the JOIN syntax is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our purposes, we'll re-use the original data set, taking the Age, Density, Weight, Height, Neck circumference, Chest circumference, Abdomen circumference, and Hip circumference fields. We'll also include the original BodyFat percentage to compare our predicted values against the originals. Execute the following command: SELECT t . Age , t . Density , t . Weight , t . Height , t . Neck , t . Chest , t . Abdomen , t . Hip , t . BodyFat , p . BodyFat AS predicted_BodyFat FROM bodyfat_integration . bodyfat AS t JOIN mindsdb . bodyfat_predictor AS p LIMIT 5 ; This should return an output table similar to the following: As you can see, a prediction has been generated for each row in the input table. Additionally, our predicted bodyfat percentages align closely with the original values! Note that even though we chose only to display the Age, Density, Weight, Height, Neck, Chest, Abdomen, and Hip measurements in this example, the predicted_BodyFat field was determined by taking into consideration all of the data fields in the original bodyfat table (as this table was JOINed with the bodyfat_predictor table, from which we selected the specified fields). In order to make predictions based ONLY on the specified fields, we would have to create a new table containing only those fields, and JOIN that with the bodyfat_predictor table! You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this check out MindsDB documentation .","title":"Making Batch Predictions using the JOIN syntax"},{"location":"sql/tutorials/byom/","text":"Bring Your Own Model \u00b6 Introduction \u00b6 MindsDB allows you to integrate your own machine learning models into it. In order to do this your model will require some sort of API wrapper, for now we have two API specifications we support: MLflow and Ray Serve . The former supports importing already trained models and predicting with them from mindsdb. The later supports both training and predicting with external models. In order to use custom models there are three mandatory arguments one must past inside the USING statement: - url.predict , this is the url to call for getting predictions from your model - format , this can be either mlflow or ray_serve - dtype_dict , this is a JSON specifying all columns expected by their models, and their respective data types. For now, the mapping supports data types used by lightwood , our AutoML library. There's an additional optional argument if you want to train the model via MindsDB (only for Ray Serve): - url.train , which is the endpoint that will be called to train your model 1. Ray Serve \u00b6 1.1 Simple example - Logistic regression \u00b6 Ray serve is a simple high-throughput service that can wrap over your own ml models. In this example, we will train and predict with an external scikit-learn model. First, let's look at the actual model wrapped inside a class that complies with the above requirements: import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI () ray . init () serve . start ( detached = True ) async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns )) - set ([ target ])) self . feature_cols = feature_cols X = df . loc [:, self . feature_cols ] Y = list ( df [ target ]) self . model = LogisticRegression () self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) X = df . loc [:, self . feature_cols ] predictions = self . model . predict ( X ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ]} return pred_dict MyModel . deploy () while True : time . sleep ( 1 ) The important bits here are having train and predict endpoints. The train endpoint accept two parameters in the JSON sent via POST: - df -- a serialized dictionary that can be converted into a pandas dataframe - target -- the name of the target column The predict endpoint needs only one parameter: - df -- a serialized dictionary that can be converted into a pandas dataframe The training endpoints must return a JSON that contains the keys status set to ok . The predict endpoint must return a dictionary containing the prediction key, storing the predictions. Additional keys can be returned for confidence and confidence intervals. Once you start this RayServe-wrapped model you can train it using a query like this one: CREATE PREDICTOR mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; And you can query predictions as usual, either by conditioning on a subset of input colums: SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or by JOINING to do batch predicions: SELECT tb . number_of_rooms , t . rental_price FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Please note that, if your model is behind a reverse proxy (e.g. nginx) you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB itself can send as much as you'd like and has been stress-tested with over a billion rows. 1.2. Example - Keras NLP model \u00b6 For this example, we will consider a natural language processing (NLP) task where we want to train a neural network with Keras to detect if a tweet is related to a natural disaster (fires, earthquakes, etc.). Please download this dataset to follow the example. The code for the model here is a bit more complex than in section 1.1, but the same rules apply: we create a Ray Server based service that wraps around a Kaggle NLP Model which can be trained and then used for predictions: import re import time import json import string import requests from collections import Counter , defaultdict \u200b import ray from ray import serve \u200b import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk.util import ngrams from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer \u200b from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.initializers import Constant from tensorflow.keras.optimizers import Adam \u200b app = FastAPI () stop = set ( stopwords . words ( 'english' )) \u200b \u200b async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target \u200b \u200b @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 \u200b def __init__ ( self ): self . model = None \u200b @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) \u200b target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) \u200b self . embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) self . embedding_dict [ word ] = vectors f . close () \u200b self . tokenizer_obj = Tokenizer () self . tokenizer_obj . fit_on_texts ( train_corpus ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , self . __class__ . GLOVE_DIM )) \u200b for word , i in tqdm ( word_index . items ()): if i > num_words : continue \u200b emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec \u200b self . model = Sequential () embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 )) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) self . model . add ( Dense ( 1 , activation = 'sigmoid' )) \u200b optimzer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimzer , metrics = [ 'accuracy' ]) \u200b X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) \u200b return { 'status' : 'ok' } \u200b @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) \u200b df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () sub = pd . DataFrame ({ 'target' : y_pre }) \u200b pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ]} return pred_dict \u200b def preprocess_df ( self , df ): df = df [[ 'text' ]] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x )) return df \u200b def remove_URL ( self , text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) \u200b def remove_html ( self , text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) \u200b def remove_punct ( self , text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) \u200b def remove_emoji ( self , text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) \u200b def create_corpus ( self , df ): corpus = [] for tweet in tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus \u200b \u200b if __name__ == '__main__' : \u200b ray . init () serve . start ( detached = True ) \u200b Model . deploy () \u200b while True : time . sleep ( 1 ) We need access to the training data, so we'll create a table called nlp_kaggle_train to load the dataset that the original model uses. And ingest it into a table with the following schema: id INT , keyword VARCHAR ( 255 ), location VARCHAR ( 255 ), text VARCHAR ( 5000 ), target INT Note: specifics of the schema and how to ingest the csv will vary depending on your database. Next, we can register and train the above custom model using the following query: CREATE PREDICTOR mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; Training will take a while given that this model is a neural network rather than a simple logistic regression. You can check its status with the query SELECT * FROM mindsdb.predictors WHERE name = 'byom_ray_serve_nlp'; , much like you'd do with a \"normal\" MindsDB predictor. Once the predictor's status becomes trained we can query it for predictions as usual: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Which would, hopefully, output 1 . Alternatively, we can try out this tweet to expect 0 as an output: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; If your results do not match this example, it could help to train the model for a longer amount of epochs. 2. MLFlow \u00b6 2.1 Simple example - Logistic Regression \u00b6 MLFlow is a tool that you can use to train and serve models, among other features like organizing experiments, tracking metrics, etc. Given there is no way to train an MLflow-wrapped model using its API, you will have to train your models outside of MindsDB by pulling your data manually (i.e. with a script), ideally using a MLflow run or experiment. The first step would be to create a script where you train a model and save it using one of the saving methods that MLflow exposes. For this example, we will use the model in this simple tutorial where the method is mlflow.sklearn.log_model ( here ), given that the model is built with scikit-learn. Once trained, you need to make sure the model is served and listening for input in a URL of your choice (note, this can mean your model can run on a different machine than the one executing MindsDB). Let's assume this URL to be http://localhost:5000/invocations for now. This means you would execute the following command in your terminal, from the directory where the model was stored: mlflow models serve --model-uri runs:/<run-id>/model With <run-id> given in the output of the command python train.py used for actually training the model. Next, we're going to bring this model into MindsDB: CREATE PREDICTOR mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } We can now run predictions as usual, by using the WHERE statement or joining on a data table with an appropriate schema: SELECT ` 1 ` FROM byom_mlflow WHERE ` 0 `= 2 ; 2.2. Advanced example - Keras NLP model \u00b6 Same use case as in section 1.2, be sure to download the dataset to reproduce the steps here. In this case, we will take a look at the best practices when your model needs custom data preprocessing code (which, realistically, will be fairly common). The key difference is that we now need to use the mlflow.pyfunc module to both 1) save the model using mlflow.pyfunc.save_model and 2) subclass mlflow.pyfunc.PythonModel to wrap the model in an MLflow-compatible way that will enable our custom inference logic to be called. Saving the model \u00b6 In the same script where you train the model (which you can find in the final section of 2.2) there should be a call at the end where you actually use mlflow to save every produced artifact: mlflow . pyfunc . save_model ( path = \"nlp_kaggle\" , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) Here, artifacts will be a dictionary with all expected produced outputs when running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. On the other hand, conda_env specifies the environment under which your model should be executed once served in a self-contained conda environment, so it should include all required packages and dependencies. For this example, they look like this: # these will be accessible inside the Model() wrapper artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } # specs for environment that will be created when serving the model conda_env = { 'name' : 'nlp_keras_env' , 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow' , 'cloudpickle' , 'nltk' , 'pandas' , 'numpy' , 'scikit-learn' , 'tqdm' , ], }, ], } Finally, to actually store the model you need to provide the wrapper class that will 1) load all produced artifacts into an accesible \"context\" and 2) implement all required inference logic: class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): # we use paths in the context to load everything self . model_path = context . artifacts [ 'model' ] self . model = load_model ( self . model_path ) with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) def predict ( self , context , model_input ): # preprocess input, tokenize, pad, and call the model df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) As you can see, here we are loading multiple artifacts and using them to guarantee the input data will be in the same format that was used when training. Ideally, you would abstract this even further into a single preprocess method that is called both at training time and inference time. Finally, serving is simple. Go to the directory where you called the above script, and execute mlflow models serve --model-uri ./nlp_kaggle . At this point, the rest is essentially the same as in the previous example. You can link the MLflow model with these SQL statements: CREATE PREDICTOR mindsdb . byom_mlflow_nlp PREDICT ` target ` USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , dtype_dict = { \"text\" : \"rich text\" , \"target\" : \"binary\" } ; To get predictions, you can directly pass input data using the WHERE clause: SELECT target FROM mindsdb . byom_mlflow_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Or you can JOIN with a data table. For this, you should ensure the table actually exists and that the database it belongs to has been connected to your MindsDB instance. For more details, refer to the same steps in the Ray Serve example (section 1.2). SELECT ta . text , tb . target as predicted FROM db_byom . test . nlp_kaggle_test as ta JOIN mindsdb . byom_mlflow_nlp as tb ; Full Script \u00b6 Finally, for reference, here's the full script that trains and saves the model. The model is exactly the same as in section 1.2, so it may seem familiar. import re import pickle import string import mlflow.pyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy as np import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from tensorflow.keras.initializers import Constant from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.models import load_model stop = set ( stopwords . words ( 'english' )) MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def preprocess_df ( df ): df = df [[ 'text' ]] funcs = [ remove_URL , remove_html , remove_emoji , remove_punct ] for fn in funcs : df [ 'text' ] = df [ 'text' ] . apply ( lambda x : fn ( x )) return df def remove_URL ( text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) def remove_html ( text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) def remove_punct ( text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) def create_corpus ( df ): corpus = [] for tweet in tqdm . tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): self . model_path = context . artifacts [ 'model' ] with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) self . model = load_model ( self . model_path ) def predict ( self , context , model_input ): df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) if __name__ == '__main__' : train_model = True model_path = './' tokenizer_path = './tokenizer.pkl' run_name = 'test_run' mlflow_pyfunc_model_path = \"nlp_kaggle\" mlflow . set_tracking_uri ( \"sqlite:///mlflow.db\" ) if train_model : # preprocess data df = pd . read_csv ( './train.csv' ) target = df [[ 'target' ]] target_arr = target . values df = preprocess_df ( df ) train_corpus = create_corpus ( df ) # load embeddings embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) embedding_dict [ word ] = vectors f . close () # generate and save tokenizer tokenizer_obj = Tokenizer () tokenizer_obj . fit_on_texts ( train_corpus ) with open ( tokenizer_path , 'wb' ) as f : pickle . dump ( tokenizer_obj , f ) # tokenize and pad sequences = tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] word_index = tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , GLOVE_DIM )) # fill embedding matrix for word , i in tqdm . tqdm ( word_index . items ()): if i > num_words : continue emb_vec = embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) # generate model model = Sequential () embedding = Embedding ( num_words , GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = MAX_LEN , trainable = False ) model . add ( embedding ) model . add ( SpatialDropout1D ( 0.2 )) model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) optimzer = Adam ( learning_rate = 1e-5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimzer , metrics = [ 'accuracy' ]) # train and save model . fit ( X_train , y_train , batch_size = 4 , epochs = EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) model . save ( model_path ) # save in mlflow format artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } conda_env = { 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow== {} ' . format ( tensorflow . __version__ ), 'cloudpickle== {} ' . format ( cloudpickle . __version__ ), 'nltk== {} ' . format ( nltk . __version__ ), 'pandas== {} ' . format ( pd . __version__ ), 'numpy== {} ' . format ( np . __version__ ), 'scikit-learn== {} ' . format ( sklearn . __version__ ), 'tqdm== {} ' . format ( tqdm . __version__ ) ], }, ], 'name' : 'nlp_keras_env' } # Save and register the MLflow Model with mlflow . start_run ( run_name = run_name ) as run : mlflow . pyfunc . save_model ( path = mlflow_pyfunc_model_path , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) result = mlflow . register_model ( f \"runs:/ { run . info . run_id } / { mlflow_pyfunc_model_path } \" , f \" { mlflow_pyfunc_model_path } \" )","title":"Bring Your Own Model"},{"location":"sql/tutorials/byom/#bring-your-own-model","text":"","title":"Bring Your Own Model"},{"location":"sql/tutorials/byom/#introduction","text":"MindsDB allows you to integrate your own machine learning models into it. In order to do this your model will require some sort of API wrapper, for now we have two API specifications we support: MLflow and Ray Serve . The former supports importing already trained models and predicting with them from mindsdb. The later supports both training and predicting with external models. In order to use custom models there are three mandatory arguments one must past inside the USING statement: - url.predict , this is the url to call for getting predictions from your model - format , this can be either mlflow or ray_serve - dtype_dict , this is a JSON specifying all columns expected by their models, and their respective data types. For now, the mapping supports data types used by lightwood , our AutoML library. There's an additional optional argument if you want to train the model via MindsDB (only for Ray Serve): - url.train , which is the endpoint that will be called to train your model","title":"Introduction"},{"location":"sql/tutorials/byom/#1-ray-serve","text":"","title":"1. Ray Serve"},{"location":"sql/tutorials/byom/#11-simple-example-logistic-regression","text":"Ray serve is a simple high-throughput service that can wrap over your own ml models. In this example, we will train and predict with an external scikit-learn model. First, let's look at the actual model wrapped inside a class that complies with the above requirements: import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI () ray . init () serve . start ( detached = True ) async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns )) - set ([ target ])) self . feature_cols = feature_cols X = df . loc [:, self . feature_cols ] Y = list ( df [ target ]) self . model = LogisticRegression () self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) X = df . loc [:, self . feature_cols ] predictions = self . model . predict ( X ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ]} return pred_dict MyModel . deploy () while True : time . sleep ( 1 ) The important bits here are having train and predict endpoints. The train endpoint accept two parameters in the JSON sent via POST: - df -- a serialized dictionary that can be converted into a pandas dataframe - target -- the name of the target column The predict endpoint needs only one parameter: - df -- a serialized dictionary that can be converted into a pandas dataframe The training endpoints must return a JSON that contains the keys status set to ok . The predict endpoint must return a dictionary containing the prediction key, storing the predictions. Additional keys can be returned for confidence and confidence intervals. Once you start this RayServe-wrapped model you can train it using a query like this one: CREATE PREDICTOR mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; And you can query predictions as usual, either by conditioning on a subset of input colums: SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or by JOINING to do batch predicions: SELECT tb . number_of_rooms , t . rental_price FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Please note that, if your model is behind a reverse proxy (e.g. nginx) you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB itself can send as much as you'd like and has been stress-tested with over a billion rows.","title":"1.1 Simple example - Logistic regression"},{"location":"sql/tutorials/byom/#12-example-keras-nlp-model","text":"For this example, we will consider a natural language processing (NLP) task where we want to train a neural network with Keras to detect if a tweet is related to a natural disaster (fires, earthquakes, etc.). Please download this dataset to follow the example. The code for the model here is a bit more complex than in section 1.1, but the same rules apply: we create a Ray Server based service that wraps around a Kaggle NLP Model which can be trained and then used for predictions: import re import time import json import string import requests from collections import Counter , defaultdict \u200b import ray from ray import serve \u200b import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk.util import ngrams from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer \u200b from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.initializers import Constant from tensorflow.keras.optimizers import Adam \u200b app = FastAPI () stop = set ( stopwords . words ( 'english' )) \u200b \u200b async def parse_req ( request : Request ): data = await request . json () target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ]) df = pd . DataFrame ( di ) return df , target \u200b \u200b @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 \u200b def __init__ ( self ): self . model = None \u200b @app . post ( \"/train\" ) async def train ( self , request : Request ): df , target = await parse_req ( request ) \u200b target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) \u200b self . embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) self . embedding_dict [ word ] = vectors f . close () \u200b self . tokenizer_obj = Tokenizer () self . tokenizer_obj . fit_on_texts ( train_corpus ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , self . __class__ . GLOVE_DIM )) \u200b for word , i in tqdm ( word_index . items ()): if i > num_words : continue \u200b emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec \u200b self . model = Sequential () embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 )) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) self . model . add ( Dense ( 1 , activation = 'sigmoid' )) \u200b optimzer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimzer , metrics = [ 'accuracy' ]) \u200b X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) \u200b return { 'status' : 'ok' } \u200b @app . post ( \"/predict\" ) async def predict ( self , request : Request ): df , _ = await parse_req ( request ) \u200b df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) \u200b sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] \u200b y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () sub = pd . DataFrame ({ 'target' : y_pre }) \u200b pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ]} return pred_dict \u200b def preprocess_df ( self , df ): df = df [[ 'text' ]] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x )) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x )) return df \u200b def remove_URL ( self , text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) \u200b def remove_html ( self , text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) \u200b def remove_punct ( self , text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) \u200b def remove_emoji ( self , text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) \u200b def create_corpus ( self , df ): corpus = [] for tweet in tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus \u200b \u200b if __name__ == '__main__' : \u200b ray . init () serve . start ( detached = True ) \u200b Model . deploy () \u200b while True : time . sleep ( 1 ) We need access to the training data, so we'll create a table called nlp_kaggle_train to load the dataset that the original model uses. And ingest it into a table with the following schema: id INT , keyword VARCHAR ( 255 ), location VARCHAR ( 255 ), text VARCHAR ( 5000 ), target INT Note: specifics of the schema and how to ingest the csv will vary depending on your database. Next, we can register and train the above custom model using the following query: CREATE PREDICTOR mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; Training will take a while given that this model is a neural network rather than a simple logistic regression. You can check its status with the query SELECT * FROM mindsdb.predictors WHERE name = 'byom_ray_serve_nlp'; , much like you'd do with a \"normal\" MindsDB predictor. Once the predictor's status becomes trained we can query it for predictions as usual: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Which would, hopefully, output 1 . Alternatively, we can try out this tweet to expect 0 as an output: SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; If your results do not match this example, it could help to train the model for a longer amount of epochs.","title":"1.2. Example - Keras NLP model"},{"location":"sql/tutorials/byom/#2-mlflow","text":"","title":"2. MLFlow"},{"location":"sql/tutorials/byom/#21-simple-example-logistic-regression","text":"MLFlow is a tool that you can use to train and serve models, among other features like organizing experiments, tracking metrics, etc. Given there is no way to train an MLflow-wrapped model using its API, you will have to train your models outside of MindsDB by pulling your data manually (i.e. with a script), ideally using a MLflow run or experiment. The first step would be to create a script where you train a model and save it using one of the saving methods that MLflow exposes. For this example, we will use the model in this simple tutorial where the method is mlflow.sklearn.log_model ( here ), given that the model is built with scikit-learn. Once trained, you need to make sure the model is served and listening for input in a URL of your choice (note, this can mean your model can run on a different machine than the one executing MindsDB). Let's assume this URL to be http://localhost:5000/invocations for now. This means you would execute the following command in your terminal, from the directory where the model was stored: mlflow models serve --model-uri runs:/<run-id>/model With <run-id> given in the output of the command python train.py used for actually training the model. Next, we're going to bring this model into MindsDB: CREATE PREDICTOR mindsdb . byom_mlflow PREDICT ` 1 ` -- `1` is the target column name USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , data_dtype = { \"0\" : \"integer\" , \"1\" : \"integer\" } We can now run predictions as usual, by using the WHERE statement or joining on a data table with an appropriate schema: SELECT ` 1 ` FROM byom_mlflow WHERE ` 0 `= 2 ;","title":"2.1 Simple example - Logistic Regression"},{"location":"sql/tutorials/byom/#22-advanced-example-keras-nlp-model","text":"Same use case as in section 1.2, be sure to download the dataset to reproduce the steps here. In this case, we will take a look at the best practices when your model needs custom data preprocessing code (which, realistically, will be fairly common). The key difference is that we now need to use the mlflow.pyfunc module to both 1) save the model using mlflow.pyfunc.save_model and 2) subclass mlflow.pyfunc.PythonModel to wrap the model in an MLflow-compatible way that will enable our custom inference logic to be called.","title":"2.2. Advanced example - Keras NLP model"},{"location":"sql/tutorials/byom/#saving-the-model","text":"In the same script where you train the model (which you can find in the final section of 2.2) there should be a call at the end where you actually use mlflow to save every produced artifact: mlflow . pyfunc . save_model ( path = \"nlp_kaggle\" , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) Here, artifacts will be a dictionary with all expected produced outputs when running the training phase. In this case, we want both a model and a tokenizer to preprocess the input text. On the other hand, conda_env specifies the environment under which your model should be executed once served in a self-contained conda environment, so it should include all required packages and dependencies. For this example, they look like this: # these will be accessible inside the Model() wrapper artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } # specs for environment that will be created when serving the model conda_env = { 'name' : 'nlp_keras_env' , 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow' , 'cloudpickle' , 'nltk' , 'pandas' , 'numpy' , 'scikit-learn' , 'tqdm' , ], }, ], } Finally, to actually store the model you need to provide the wrapper class that will 1) load all produced artifacts into an accesible \"context\" and 2) implement all required inference logic: class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): # we use paths in the context to load everything self . model_path = context . artifacts [ 'model' ] self . model = load_model ( self . model_path ) with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) def predict ( self , context , model_input ): # preprocess input, tokenize, pad, and call the model df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) As you can see, here we are loading multiple artifacts and using them to guarantee the input data will be in the same format that was used when training. Ideally, you would abstract this even further into a single preprocess method that is called both at training time and inference time. Finally, serving is simple. Go to the directory where you called the above script, and execute mlflow models serve --model-uri ./nlp_kaggle . At this point, the rest is essentially the same as in the previous example. You can link the MLflow model with these SQL statements: CREATE PREDICTOR mindsdb . byom_mlflow_nlp PREDICT ` target ` USING url . predict = 'http://localhost:5000/invocations' , format = 'mlflow' , dtype_dict = { \"text\" : \"rich text\" , \"target\" : \"binary\" } ; To get predictions, you can directly pass input data using the WHERE clause: SELECT target FROM mindsdb . byom_mlflow_nlp WHERE text = 'The tsunami is coming, seek high ground' ; Or you can JOIN with a data table. For this, you should ensure the table actually exists and that the database it belongs to has been connected to your MindsDB instance. For more details, refer to the same steps in the Ray Serve example (section 1.2). SELECT ta . text , tb . target as predicted FROM db_byom . test . nlp_kaggle_test as ta JOIN mindsdb . byom_mlflow_nlp as tb ;","title":"Saving the model"},{"location":"sql/tutorials/byom/#full-script","text":"Finally, for reference, here's the full script that trains and saves the model. The model is exactly the same as in section 1.2, so it may seem familiar. import re import pickle import string import mlflow.pyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy as np import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from tensorflow.keras.initializers import Constant from tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.models import load_model stop = set ( stopwords . words ( 'english' )) MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def preprocess_df ( df ): df = df [[ 'text' ]] funcs = [ remove_URL , remove_html , remove_emoji , remove_punct ] for fn in funcs : df [ 'text' ] = df [ 'text' ] . apply ( lambda x : fn ( x )) return df def remove_URL ( text ): url = re . compile ( r 'https?://\\S+|www\\.\\S+' ) return url . sub ( r '' , text ) def remove_html ( text ): html = re . compile ( r '<.*?>' ) return html . sub ( r '' , text ) def remove_punct ( text ): table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( text ): emoji_pattern = re . compile ( \"[\" u \" \\U0001F600 - \\U0001F64F \" # emoticons u \" \\U0001F300 - \\U0001F5FF \" # symbols & pictographs u \" \\U0001F680 - \\U0001F6FF \" # transport & map symbols u \" \\U0001F1E0 - \\U0001F1FF \" # flags (iOS) u \" \\U00002702 - \\U000027B0 \" u \" \\U000024C2 - \\U0001F251 \" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r '' , text ) def create_corpus ( df ): corpus = [] for tweet in tqdm . tqdm ( df [ 'text' ]): words = [ word . lower () for word in word_tokenize ( tweet ) if (( word . isalpha () == 1 ) & ( word not in stop ))] corpus . append ( words ) return corpus class Model ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): self . model_path = context . artifacts [ 'model' ] with open ( context . artifacts [ 'tokenizer_path' ], 'rb' ) as f : self . tokenizer = pickle . load ( f ) self . model = load_model ( self . model_path ) def predict ( self , context , model_input ): df = preprocess_df ( model_input ) corpus = create_corpus ( df ) sequences = self . tokenizer . texts_to_sequences ( corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten () . tolist () return list ( y_pre ) if __name__ == '__main__' : train_model = True model_path = './' tokenizer_path = './tokenizer.pkl' run_name = 'test_run' mlflow_pyfunc_model_path = \"nlp_kaggle\" mlflow . set_tracking_uri ( \"sqlite:///mlflow.db\" ) if train_model : # preprocess data df = pd . read_csv ( './train.csv' ) target = df [[ 'target' ]] target_arr = target . values df = preprocess_df ( df ) train_corpus = create_corpus ( df ) # load embeddings embedding_dict = {} with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split () word = values [ 0 ] vectors = np . asarray ( values [ 1 :], 'float32' ) embedding_dict [ word ] = vectors f . close () # generate and save tokenizer tokenizer_obj = Tokenizer () tokenizer_obj . fit_on_texts ( train_corpus ) with open ( tokenizer_path , 'wb' ) as f : pickle . dump ( tokenizer_obj , f ) # tokenize and pad sequences = tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [: df . shape [ 0 ]] word_index = tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros (( num_words , GLOVE_DIM )) # fill embedding matrix for word , i in tqdm . tqdm ( word_index . items ()): if i > num_words : continue emb_vec = embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) # generate model model = Sequential () embedding = Embedding ( num_words , GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ), input_length = MAX_LEN , trainable = False ) model . add ( embedding ) model . add ( SpatialDropout1D ( 0.2 )) model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) optimzer = Adam ( learning_rate = 1e-5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = optimzer , metrics = [ 'accuracy' ]) # train and save model . fit ( X_train , y_train , batch_size = 4 , epochs = EPOCHS , validation_data = ( X_test , y_test ), verbose = 2 ) model . save ( model_path ) # save in mlflow format artifacts = { 'model' : model_path , 'tokenizer_path' : tokenizer_path , } conda_env = { 'channels' : [ 'defaults' ], 'dependencies' : [ 'python=3.8' , 'pip' , { 'pip' : [ 'mlflow' , 'tensorflow== {} ' . format ( tensorflow . __version__ ), 'cloudpickle== {} ' . format ( cloudpickle . __version__ ), 'nltk== {} ' . format ( nltk . __version__ ), 'pandas== {} ' . format ( pd . __version__ ), 'numpy== {} ' . format ( np . __version__ ), 'scikit-learn== {} ' . format ( sklearn . __version__ ), 'tqdm== {} ' . format ( tqdm . __version__ ) ], }, ], 'name' : 'nlp_keras_env' } # Save and register the MLflow Model with mlflow . start_run ( run_name = run_name ) as run : mlflow . pyfunc . save_model ( path = mlflow_pyfunc_model_path , python_model = Model (), conda_env = conda_env , artifacts = artifacts ) result = mlflow . register_model ( f \"runs:/ { run . info . run_id } / { mlflow_pyfunc_model_path } \" , f \" { mlflow_pyfunc_model_path } \" )","title":"Full Script"},{"location":"sql/tutorials/crop-prediction/","text":"Crop Recomendation \u00b6 Dataset: Crop recomendation Data Communtiy Author: pixpack Modern agriculture is becoming very dependent on technology. From advanced machinery to specially selected crops. All the technology produces a lot of data that can be used for better adjustment of the farming process. One use case of machine learning in agriculture could be the selection of the best crop for a specific field to maximize the potential yield. Such problems are often called Classification Problems in machine learning. With MindsDB you can easily make automated machine learning predictions straight from your existing database. Even without advanced ML engineering skills, you can start leveraging predictive models that help you make better business decisions. In this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB . Pre-requisites \u00b6 Before you start make sure you have: Access to MindsDB. In this tutorial, we will use MindsDB Cloud GUI . If you want you can also deploy mindsdb on your premises, Check out the installation guide for Docker or PyPi . Downloaded the dataset. You can get it from Kaggle . Add your file to MindsDB \u00b6 MindsDB can integrates with many databases, in most scenarios your data will be stored in a database, if you decide to load this dataset into your database of choice, please follow instructions here as to how to connect mindsdb to your database. In this tutorial, we will be adding the dataset directly to MindsDB's GUI. For this example MindsDB Cloud GUI will be used.If you need to create an account you can find the guide on how to do it here . Alternatively, you can also use MindsDB's local deployment and access the GUI in your browser with 127.0.0.1:47334 . The first step will be to access MindsDB cloud where we will also make use of the SQL Editor: Once you are logged onto the Cloud GUI,navigate to either the Add Data button or the plug icon on the left side bar to select it. The screen will navigate to the Select your datasource page, select the option Files. Select Import Files. Click on Import a File to select your dataset from your local drive. Your dataset should be a maximum size of 10MB. Under Table name type in the name you would like to give your dataset which will be stored in MindsDB files. Once the dataset has been successfully uploaded into a table, you can query the dataset directly from the table. In the SQL Editor,type in the following syntax and select the button Run or Shift+Enter to execute the code: SELECT * FROM files . crops ; This confirms that the dataset has been successfully uploaded with all its rows. Create a predictor \u00b6 Now we can create a machine learning model with crops columns serving as features, and MindsDB takes care of the rest of the ML workflow automatically. There is a way to get your hands into the insides of the model to fine tune it, but we will not cover it in this tutorial. In the SQL Editor, type in the below syntax to create and train a machine learning predictive model: CREATE PREDICTOR crop_predictor FROM files ( SELECT * FROM crops ) PREDICT label as crop_type ; Select the button Run or Shift+Enter to execute the code. If the predictor is successfully created the console will display a message Query successfully completed . Now the predictor will begin training. You can check the status of the predictor with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'crop_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. You are now done with creating the predictor! \u2728 Make predictions \u00b6 In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT label FROM mindsdb . crop_predictor WHERE N = 77 and P = 52 and K = 17 and temperature = 24 and humidity = 20 . 74 and ph = 5 . 71 and rainfall = 75 . 82 ; As we have used a real data point from our dataset we can verify the prediction. N, P, K, temperature, humidity, ph, rainfall, label 77, 52, 17, 24.86374934, 65.7420046, 5.714799723, 75.82270467, maize As you can see, the model correctly predicted the most appropriate crop type for our field. OK, we made a prediction using a single query, but what if you want to make a batch prediction for a large set of data in your database? In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how it works. Use the following command to create the batch prediction and execute with the Run button. SELECT collected_data . N , collected_data . P , collected_data . K , collected_data . temperature , collected_data . humidity , collected_data . ph , collected_data . rainfall , predictions . label as predicted_crop_type FROM crops_integration . crops AS collected_data JOIN mindsdb . crop_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table. You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more check out other tutorials and MindsDB documentation .","title":"Crop Recommendation"},{"location":"sql/tutorials/crop-prediction/#crop-recomendation","text":"Dataset: Crop recomendation Data Communtiy Author: pixpack Modern agriculture is becoming very dependent on technology. From advanced machinery to specially selected crops. All the technology produces a lot of data that can be used for better adjustment of the farming process. One use case of machine learning in agriculture could be the selection of the best crop for a specific field to maximize the potential yield. Such problems are often called Classification Problems in machine learning. With MindsDB you can easily make automated machine learning predictions straight from your existing database. Even without advanced ML engineering skills, you can start leveraging predictive models that help you make better business decisions. In this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB .","title":"Crop Recomendation"},{"location":"sql/tutorials/crop-prediction/#pre-requisites","text":"Before you start make sure you have: Access to MindsDB. In this tutorial, we will use MindsDB Cloud GUI . If you want you can also deploy mindsdb on your premises, Check out the installation guide for Docker or PyPi . Downloaded the dataset. You can get it from Kaggle .","title":"Pre-requisites"},{"location":"sql/tutorials/crop-prediction/#add-your-file-to-mindsdb","text":"MindsDB can integrates with many databases, in most scenarios your data will be stored in a database, if you decide to load this dataset into your database of choice, please follow instructions here as to how to connect mindsdb to your database. In this tutorial, we will be adding the dataset directly to MindsDB's GUI. For this example MindsDB Cloud GUI will be used.If you need to create an account you can find the guide on how to do it here . Alternatively, you can also use MindsDB's local deployment and access the GUI in your browser with 127.0.0.1:47334 . The first step will be to access MindsDB cloud where we will also make use of the SQL Editor: Once you are logged onto the Cloud GUI,navigate to either the Add Data button or the plug icon on the left side bar to select it. The screen will navigate to the Select your datasource page, select the option Files. Select Import Files. Click on Import a File to select your dataset from your local drive. Your dataset should be a maximum size of 10MB. Under Table name type in the name you would like to give your dataset which will be stored in MindsDB files. Once the dataset has been successfully uploaded into a table, you can query the dataset directly from the table. In the SQL Editor,type in the following syntax and select the button Run or Shift+Enter to execute the code: SELECT * FROM files . crops ; This confirms that the dataset has been successfully uploaded with all its rows.","title":"Add your file to MindsDB"},{"location":"sql/tutorials/crop-prediction/#create-a-predictor","text":"Now we can create a machine learning model with crops columns serving as features, and MindsDB takes care of the rest of the ML workflow automatically. There is a way to get your hands into the insides of the model to fine tune it, but we will not cover it in this tutorial. In the SQL Editor, type in the below syntax to create and train a machine learning predictive model: CREATE PREDICTOR crop_predictor FROM files ( SELECT * FROM crops ) PREDICT label as crop_type ; Select the button Run or Shift+Enter to execute the code. If the predictor is successfully created the console will display a message Query successfully completed . Now the predictor will begin training. You can check the status of the predictor with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'crop_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. You are now done with creating the predictor! \u2728","title":"Create a predictor"},{"location":"sql/tutorials/crop-prediction/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT label FROM mindsdb . crop_predictor WHERE N = 77 and P = 52 and K = 17 and temperature = 24 and humidity = 20 . 74 and ph = 5 . 71 and rainfall = 75 . 82 ; As we have used a real data point from our dataset we can verify the prediction. N, P, K, temperature, humidity, ph, rainfall, label 77, 52, 17, 24.86374934, 65.7420046, 5.714799723, 75.82270467, maize As you can see, the model correctly predicted the most appropriate crop type for our field. OK, we made a prediction using a single query, but what if you want to make a batch prediction for a large set of data in your database? In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how it works. Use the following command to create the batch prediction and execute with the Run button. SELECT collected_data . N , collected_data . P , collected_data . K , collected_data . temperature , collected_data . humidity , collected_data . ph , collected_data . rainfall , predictions . label as predicted_crop_type FROM crops_integration . crops AS collected_data JOIN mindsdb . crop_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table. You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more check out other tutorials and MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/customer-churn/","text":"MindsDB as a Machine Learning framework can help marketing, sales, and customer retention teams determine the best incentive and the right time to make an offer to minimize customer turnover. In this tutorial you will learn how to use SQL queries to train a machine learning model and make predictions in three simple steps: Connect a database with customer's data to MindsDB. Use an CREATE PREDICTOR statement to train the machine learning model automatically. Query predictions with a simple SELECT statement from MindsDB AI Table (this special table returns data from an ML model upon being queried). Using SQL to perform machine learning at the data layer will bring you many benefits like removing unnecessary ETL-ing, seamless integration with your data, and enabling predictive analytics in your BI tool. Let's see how this works with a real world example to predict the probability of churn for a new customer of a telecom company. Note: You can follow up this tutorial by connecting to your own database and using different data - the same workflow applies to most machine learning use cases. Pre-requisites \u00b6 First, you can log into MindsDB Cloud or make sure you have successfully installed MindsDB. Check out the installation guide for Docker or PyPi install. Second, you will need to have mysql-client or DBeaver, MySQL WOrkbench etc installed locally to connect to MySQL API. Log into Mindsdb Cloud \u00b6 The first step will be to log into Mindsdb Cloud , where we will gain access to the MySQL editor to execute SQL syntax. The Upload file feature and the MySQL Editor will make it easy to upload a file, create a predictor and make a prediction. Upload a data file to MindsDB Cloud \u00b6 The datafile can be uploaded directly to Mindsdb Cloud for you to query. Navigate to +Upload file and select the option. Select Upload file to upsert a file from your local files or drag the file onto the 'Upload a file' section. Once the file has been uploaded,name the file you as a table name. Save and continue. The file will be stored as a table in your files table. You can query the file you have uploaded as a table, eg. SELECT * FROM files . churn Data Overview \u00b6 In this tutorial, we will use the customer churn data-set . Each row represents a customer and we will train a machine learning model to help us predict if the customer is going to stop using the company products. Below is a short description of each feature inside the data. CustomerId - Customer ID Gender - Male or Female customer SeniorCitizen - Whether the customer is a senior citizen or not (1, 0) Partner - Whether the customer has a partner or not (Yes, No) Dependents - Whether the customer has dependents or not (Yes, No) Tenure - Number of months the customer has stayed with the company PhoneService - Whether the customer has a phone service or not (Yes, No) MultipleLines - Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService - Customer\u2019s internet service provider (DSL, Fiber optic, No) OnlineSecurity - Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup - Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection - Whether the customer has device protection or not (Yes, No, No internet service) TechSupport - Whether the customer has tech support or not (Yes, No, No internet service) StreamingTv - Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovies - Whether the customer has streaming movies or not (Yes, No, No internet service) Contract - The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling - Whether the customer has paperless billing or not (Yes, No) PaymentMethod - The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges - The monthly charge amount TotalCharges - The total amount charged to the customer Churn - Whether the customer churned or not (Yes or No). This is what we want to predict. Create a predictor \u00b6 We will create a machine learning model with the relevant parameters in order for it to train. For that we are going to use the CREATE PREDICTOR syntax, where we specify what query to train FROM and what we want to learn to PREDICT: CREATE PREDICTOR predictor_name FROM files ( SELECT * FROM file_name ) PREDICT column_name ; The required values that we need to provide are: predictor_name (string) - The name of the model file_name - The name of the table/file you have uploaded. column_name (string) - The feature you want to predict. To train the model that will predict customer churn, run the following syntax: CREATE PREDICTOR customer_churn FROM files ( SELECT * FROM churn ) PREDICT Churn ; What we did here was to create a predictor called customer_churn to predict the Churn and also ignore the gender column as an irrelevant column for the model.The model training has started. You can check the predictors status and will be able to make a prediction once the status shows complete: SELECT * FROM mindsdb . predictors where name = 'customer_churn' ; The next steps would be to query the model and predict the customer churn. Let\u2019s be creative and imagine a customer. Customer will use only DSL service, no phone service and multiple lines, she was with the company for 1 month and has a partner. Add all of this information to the WHERE clause. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' ; With the confidence of around 82% MindsDB predicted that this customer will churn. One important thing to check here is the important_missing_information value, where MindsDB is pointing to the important missing information for giving a more accurate prediction, in this case, Contract, MonthlyCharges, TotalCharges and OnlineBackup. Let\u2019s include those values in the WHERE clause, and run a new query: SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' AND OnlineSecurity = 'No' AND OnlineBackup = 'Yes' AND DeviceProtection = 'No' AND TechSupport = 'No' AND StreamingTV = 'No' AND StreamingMovies = 'No' AND Contract = 'Month-to-month' AND PaperlessBilling = 'Yes' AND PaymentMethod = 'Electronic check' AND MonthlyCharges = 29 . 85 AND TotalCharges = 29 . 85 ;","title":"Customer Churn"},{"location":"sql/tutorials/customer-churn/#pre-requisites","text":"First, you can log into MindsDB Cloud or make sure you have successfully installed MindsDB. Check out the installation guide for Docker or PyPi install. Second, you will need to have mysql-client or DBeaver, MySQL WOrkbench etc installed locally to connect to MySQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/customer-churn/#log-into-mindsdb-cloud","text":"The first step will be to log into Mindsdb Cloud , where we will gain access to the MySQL editor to execute SQL syntax. The Upload file feature and the MySQL Editor will make it easy to upload a file, create a predictor and make a prediction.","title":"Log into Mindsdb Cloud"},{"location":"sql/tutorials/customer-churn/#upload-a-data-file-to-mindsdb-cloud","text":"The datafile can be uploaded directly to Mindsdb Cloud for you to query. Navigate to +Upload file and select the option. Select Upload file to upsert a file from your local files or drag the file onto the 'Upload a file' section. Once the file has been uploaded,name the file you as a table name. Save and continue. The file will be stored as a table in your files table. You can query the file you have uploaded as a table, eg. SELECT * FROM files . churn","title":"Upload a data file to MindsDB Cloud"},{"location":"sql/tutorials/customer-churn/#data-overview","text":"In this tutorial, we will use the customer churn data-set . Each row represents a customer and we will train a machine learning model to help us predict if the customer is going to stop using the company products. Below is a short description of each feature inside the data. CustomerId - Customer ID Gender - Male or Female customer SeniorCitizen - Whether the customer is a senior citizen or not (1, 0) Partner - Whether the customer has a partner or not (Yes, No) Dependents - Whether the customer has dependents or not (Yes, No) Tenure - Number of months the customer has stayed with the company PhoneService - Whether the customer has a phone service or not (Yes, No) MultipleLines - Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService - Customer\u2019s internet service provider (DSL, Fiber optic, No) OnlineSecurity - Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup - Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection - Whether the customer has device protection or not (Yes, No, No internet service) TechSupport - Whether the customer has tech support or not (Yes, No, No internet service) StreamingTv - Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovies - Whether the customer has streaming movies or not (Yes, No, No internet service) Contract - The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling - Whether the customer has paperless billing or not (Yes, No) PaymentMethod - The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges - The monthly charge amount TotalCharges - The total amount charged to the customer Churn - Whether the customer churned or not (Yes or No). This is what we want to predict.","title":"Data Overview"},{"location":"sql/tutorials/customer-churn/#create-a-predictor","text":"We will create a machine learning model with the relevant parameters in order for it to train. For that we are going to use the CREATE PREDICTOR syntax, where we specify what query to train FROM and what we want to learn to PREDICT: CREATE PREDICTOR predictor_name FROM files ( SELECT * FROM file_name ) PREDICT column_name ; The required values that we need to provide are: predictor_name (string) - The name of the model file_name - The name of the table/file you have uploaded. column_name (string) - The feature you want to predict. To train the model that will predict customer churn, run the following syntax: CREATE PREDICTOR customer_churn FROM files ( SELECT * FROM churn ) PREDICT Churn ; What we did here was to create a predictor called customer_churn to predict the Churn and also ignore the gender column as an irrelevant column for the model.The model training has started. You can check the predictors status and will be able to make a prediction once the status shows complete: SELECT * FROM mindsdb . predictors where name = 'customer_churn' ; The next steps would be to query the model and predict the customer churn. Let\u2019s be creative and imagine a customer. Customer will use only DSL service, no phone service and multiple lines, she was with the company for 1 month and has a partner. Add all of this information to the WHERE clause. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' ; With the confidence of around 82% MindsDB predicted that this customer will churn. One important thing to check here is the important_missing_information value, where MindsDB is pointing to the important missing information for giving a more accurate prediction, in this case, Contract, MonthlyCharges, TotalCharges and OnlineBackup. Let\u2019s include those values in the WHERE clause, and run a new query: SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' AND OnlineSecurity = 'No' AND OnlineBackup = 'Yes' AND DeviceProtection = 'No' AND TechSupport = 'No' AND StreamingTV = 'No' AND StreamingMovies = 'No' AND Contract = 'Month-to-month' AND PaperlessBilling = 'Yes' AND PaymentMethod = 'Electronic check' AND MonthlyCharges = 29 . 85 AND TotalCharges = 29 . 85 ;","title":"Create a predictor"},{"location":"sql/tutorials/diabetes/","text":"Diabetes Dataset: Diabetes Data Communtiy Author: Chandre Tosca Van Der Westhuizen Diabetes is a metabolic disease that causes high blood sugar and if left untreated can damage your nerves, eyes, kidneys and other organs. It is known as a silent killer, as recent studies have shown that by the year 2040 the world's diabetic patients will reach 642 million. The need to analyze vast medical data to assist in the diagnoses, treatment and management of illnesses is increasing for the medical community. With the rapid development of machine learning, it has been applied to many aspects of medical health and is transforming the healthcare system. The vitality to intelligently transform information into valuable knowledge through machine learning has become more present in biosciences. With the use of predictive models, MindsDB can assist in classifying diabetic and non-diabetic patients or those who pose a high risk. This is just a small showcase on how MindsDB's machine learning will be able to assist in vastly enhancing the reach of illnesses, thereby making it more efficient and can revolutionize businesses and most importantly the health care system. In this tutorial we will be exploring how we can use a machine learning model to classify negative and positive cases for diabetes. MindsDB allows you to train your model from CSV data directly, however for this tutorial you will: Establish a connection between your database and MindsDB via MindsDB GUI(Cloud and local instance). Allow connections to the database using Ngrok. Create a machine learning model using SQL. Make a prediction. Connect your database to MindsDB GUI \u00b6 MindsDB has a functionality to upload your dataset directly to MindsDB. However in this tutorial, you will be shown how to connect your database to MindsDB cloud and local instance. For this example we will be using a local postgres database, therefore we will connect using an ngrok tunnel. Running a Ngrok Tunnel \u00b6 To make our database publicly avaiable, we will use ngrok . The following command can be run in docker or a terminal on your device to set up a ngrok tunnel. ngrok tcp [ db-port ] For this example the port number used is 5432. You should see a similar output: Session Status online Account chandre (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://4.tcp.ngrok.io:13096 -> localhost:5432 The information required is by the forwarded address, next to 'Forwarding' select and copy 4.tcp.ngrok.io:13096 . Once you have copied the information, you can add it to the information requested by the MindsDB GUI which we will get to in a moment. For the next steps we will log into the MindsDB cloud interface and local gui. MindsDB Cloud is perfect if you are unable to install MindsDB on your device. If you are not registered yet, feel free to follow the below guide. If you have already registered, skip the next steps to connect your database. MindsDB GUI- Cloud MySQL Editor & MySQL CLI \u00b6 The next step will be to connect your database to MindsDB. You can visit this link and follow the steps to create a MindsDB Cloud account. Access the MySQL Editor here . If you are using MindsDB through Docker, you can run the command below to start MindsDB: docker run -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb Once you are connected,head over to your browser and in the browser tab type in the host number 127.0.0.1:47334 to access the local MindsDB GUI. Establish connection between your database and MindsDB GUI \u00b6 Once you have accessed the GUI, you will be able to select a database type and enter the required parameters: - On the user interface,select the plug icon on the left sidebar. - A page will populate with different datasources to select from. For this tutorial we are choosing PostgreSQL. The page will automatically direct to the SQL editor defaulting the syntax parameters required to create a connection with PostgreSQL: CREATE DATABASE : This will be the chosen display name for your database. For this tutorial we will choose airbyte. WITH ENGINE : name of the mindsdb handler,in this example it will be Postgres. PARAMETERS = \"user\": Your database user. for this tutorial it is postgres \"password\": Your password. \"host\":host, it can be an ip or an url. For this tutorial it will be the forwarding address 4.tcp.ngrok.io \"port\": \"13096\",common port is 5432, for this tutorial we will use 15076. \"database\":The name of your database (optional). For this tutorial we will be using postgres. Select the button RUN or select Shift+Enter to execute the query. You are now done with connecting MindsDB to your database! \ud83d\ude80 Create a predictor \u00b6 Now we are ready to create our own predictor! We will start by using the MySQL API to connect to MindsDB and with a single SQL command create a predictor. The predictor we will create will be trained to determine negative and positive cases for diabetes. Use the following query to create a predictor that will predict the Class ( positive or negative ) for the specific field parameters. CREATE PREDICTOR diabetes_predictor FROM mindsdb_prediction ( SELECT * FROM diabetes ) PREDICT class ; Select the RUN button,alternatively select Shift+Enter to execute the query. You will receive the message 'Query successfully completed' if the machine learning model is successfully created. The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . predictors WHERE name = 'diabetes_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728 Make predictions \u00b6 In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL query Use the following query using mock data with the predictor. SELECT Class FROM mindsdb . diabetes_predictor WHERE number_of_times_pregnant = 0 AND plasma_glucose_concentration = 135 . 0 AND diastolic_blood_pressure = 65 . 0 AND triceps_skin_fold_thickness = 30 AND two_Hour_serum_insulin = 0 AND body_mass_index = 23 . 5 AND diabetes_pedigree_function = 0 . 366 AND age = 31 ; MindsDB will provide you with results similar to below: The machine learning model predicted the diabetic class to be negative. Viola! We have successfully created and trained a model and made our own prediction. How easy and amazing is MindsDB? \ud83c\udf89 Want to try it out? Sign up for a free MindsDB account Join MindsDB community on Slack and GitHub to ask questions, share and express ideas and thoughts!","title":"Diabetes"},{"location":"sql/tutorials/diabetes/#connect-your-database-to-mindsdb-gui","text":"MindsDB has a functionality to upload your dataset directly to MindsDB. However in this tutorial, you will be shown how to connect your database to MindsDB cloud and local instance. For this example we will be using a local postgres database, therefore we will connect using an ngrok tunnel.","title":"Connect your database to MindsDB GUI"},{"location":"sql/tutorials/diabetes/#running-a-ngrok-tunnel","text":"To make our database publicly avaiable, we will use ngrok . The following command can be run in docker or a terminal on your device to set up a ngrok tunnel. ngrok tcp [ db-port ] For this example the port number used is 5432. You should see a similar output: Session Status online Account chandre (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://4.tcp.ngrok.io:13096 -> localhost:5432 The information required is by the forwarded address, next to 'Forwarding' select and copy 4.tcp.ngrok.io:13096 . Once you have copied the information, you can add it to the information requested by the MindsDB GUI which we will get to in a moment. For the next steps we will log into the MindsDB cloud interface and local gui. MindsDB Cloud is perfect if you are unable to install MindsDB on your device. If you are not registered yet, feel free to follow the below guide. If you have already registered, skip the next steps to connect your database.","title":"Running a Ngrok Tunnel"},{"location":"sql/tutorials/diabetes/#mindsdb-gui-cloud-mysql-editor-mysql-cli","text":"The next step will be to connect your database to MindsDB. You can visit this link and follow the steps to create a MindsDB Cloud account. Access the MySQL Editor here . If you are using MindsDB through Docker, you can run the command below to start MindsDB: docker run -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb Once you are connected,head over to your browser and in the browser tab type in the host number 127.0.0.1:47334 to access the local MindsDB GUI.","title":"MindsDB GUI- Cloud MySQL Editor &amp; MySQL CLI"},{"location":"sql/tutorials/diabetes/#establish-connection-between-your-database-and-mindsdb-gui","text":"Once you have accessed the GUI, you will be able to select a database type and enter the required parameters: - On the user interface,select the plug icon on the left sidebar. - A page will populate with different datasources to select from. For this tutorial we are choosing PostgreSQL. The page will automatically direct to the SQL editor defaulting the syntax parameters required to create a connection with PostgreSQL: CREATE DATABASE : This will be the chosen display name for your database. For this tutorial we will choose airbyte. WITH ENGINE : name of the mindsdb handler,in this example it will be Postgres. PARAMETERS = \"user\": Your database user. for this tutorial it is postgres \"password\": Your password. \"host\":host, it can be an ip or an url. For this tutorial it will be the forwarding address 4.tcp.ngrok.io \"port\": \"13096\",common port is 5432, for this tutorial we will use 15076. \"database\":The name of your database (optional). For this tutorial we will be using postgres. Select the button RUN or select Shift+Enter to execute the query. You are now done with connecting MindsDB to your database! \ud83d\ude80","title":"Establish connection between your database and MindsDB GUI"},{"location":"sql/tutorials/diabetes/#create-a-predictor","text":"Now we are ready to create our own predictor! We will start by using the MySQL API to connect to MindsDB and with a single SQL command create a predictor. The predictor we will create will be trained to determine negative and positive cases for diabetes. Use the following query to create a predictor that will predict the Class ( positive or negative ) for the specific field parameters. CREATE PREDICTOR diabetes_predictor FROM mindsdb_prediction ( SELECT * FROM diabetes ) PREDICT class ; Select the RUN button,alternatively select Shift+Enter to execute the query. You will receive the message 'Query successfully completed' if the machine learning model is successfully created. The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . predictors WHERE name = 'diabetes_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728","title":"Create a predictor"},{"location":"sql/tutorials/diabetes/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL query Use the following query using mock data with the predictor. SELECT Class FROM mindsdb . diabetes_predictor WHERE number_of_times_pregnant = 0 AND plasma_glucose_concentration = 135 . 0 AND diastolic_blood_pressure = 65 . 0 AND triceps_skin_fold_thickness = 30 AND two_Hour_serum_insulin = 0 AND body_mass_index = 23 . 5 AND diabetes_pedigree_function = 0 . 366 AND age = 31 ; MindsDB will provide you with results similar to below: The machine learning model predicted the diabetic class to be negative. Viola! We have successfully created and trained a model and made our own prediction. How easy and amazing is MindsDB? \ud83c\udf89 Want to try it out? Sign up for a free MindsDB account Join MindsDB community on Slack and GitHub to ask questions, share and express ideas and thoughts!","title":"Make predictions"},{"location":"sql/tutorials/heart-disease/","text":"Dataset: Heart Disease Data Communtiy Author: Mohd Talha Cardiovascular disease remains the leading cause of morbidity and mortality according to the National Center for Health Statistics in the United States, and consequently, early diagnosis is of paramount importance. Machine learning technology, a subfield of artificial intelligence, is enabling scientists, clinicians and patients to detect it in the earlier stages and therefore save lives. Until now, building, deploying and maintaining applied machine learning solutions was a complicated and expensive task, because it required skilled personnel and expensive tools. But not only that. A traditional machine learning project requires building integrations with data and applications, that is not only a technical but also an organizational challenge. So what if machine learning can become a part of the standard tools that are already in use? This is exactly the problem that MindsDB is solving. It makes machine learning easy to use by automating and integrating it into the mainstream instruments for data and analytics, namely databases and business intelligence software. It adds an AI \u201cbrain\u201d to databases so that they can learn automatically from existing data, allowing you to generate and visualize predictions using standard data query language like SQL. Lastly, MindsDB is open-source, and anyone can use it for free. In this article, we will show step by step how to use MindsDB inside databases to predict the risk of heart disease for patients. You can follow this tutorial by connecting to your own database and using different data - the same workflow applies to most machine learning use cases. Let\u2019s get started! Pre-requisites \u00b6 If you want to install MindsDB locally, check out the installation guide for Docker or PyPi and you can follow this tutorial. If you are OK with using MindsDB cloud, then simply create a free account and you will be up and running in just one minute. Second, you will need to have a mysql client like DBeaver, MySQL Workbench etc. installed locally to connect to the MindsDB MySQL API. MindsDB contains a SQL Editor which can be accessed on MindsDB cloud or the URL 127.0.0.1:47334/. Data Overview \u00b6 For the example of this tutorial, we will use the heart disease dataset available publicly in Kaggle . Each row represents a patient and we will train a machine learning model to help us predict if the patient is classified as a heart disease patient. Below is a short description of each feature inside the data. age - In Years sex - 1 = Male; 0 = Female cp - chest pain type (4 values) trestbps - Resting blood pressure (in mm Hg on admission to the hospital) chol - Serum cholesterol in mg/dl fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) restecg - Resting electrocardiographic results thalach - Maximum heart rate achieved exang - Exercise induced angina (1 = yes; 0 = no) oldpeak - ST depression induced by exercise relative to rest slope - the slope of the peak exercise ST segment ca - Number of major vessels (0-3) colored by fluoroscopy thal - 1 = normal; 2 = fixed defect; 3 = reversible defect target - 1 or 0 (This is what we will predict) How to use MindsDB \u00b6 MindsDB allows you to automatically create & train machine learning models from the data in your database that you have connected to in the previous step. MindsDB works via MySQL wire protocol, which means you can do all these steps through SQL commands. When it comes to making predictions, SQL queries become even handier, because you can easily make them straight from your existing applications or Business Intelligence tools that already speak SQL. The ML models are available to use immediately after being trained as if they were virtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it works. Connect to your data \u00b6 First, we need to connect MindsDB to the database where the Heart Disease data is stored: - Access MindsDB GUI on either cloud or local via the URL 127.0.0.1:47334/ - On the default page, select the button Add Data or alternatively select the plug icon on the left sidebar. - The 'Select your data source' page will populate for you to choose your database type. For this tutorial we will be selecting the postgres database button. Once you have selected the database type,the page will automatically navigate to the SQL Editor where the syntax to create a database connection will automatically populate for you to enter the required parameters. The required parameters are: CREATE DATABASE display_name --- display name for database. WITH ENGINE = \"postgres\", --- name of the mindsdb handler PARAMETERS = { \"user\": \" \", --- Your database user. \"password\": \" \", --- Your password. \"host\": \" \", --- host, it can be an ip or an url. \"port\": \"5432\", --- common port is 5432. \"database\": \" \" --- The name of your database *optional.} Select the Run button or Shift+Enter to execute the syntax. Once the Database connection is created the console will display a message 'Query successfully completed'. Create a machine learning model. \u00b6 We can create a machine learning predictive model by using simple SQL statements executed in the SQL Editor. To create and train a new machine learning model we will need to use the CREATE PREDICTOR statement: CREATE PREDICTOR mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The required values that we need to provide are: predictor_name (string): The name of the model integration_name (string): The name of the connection to your database. column_name (string): The feature you want to predict. To train the model that will predict the risk of heart disease as target we will run: CREATE PREDICTOR patients_target FROM mindsdb_predictions ( SELECT * FROM heart_disease ) PREDICT target ; Select the Run button or Shift+Enter to execute the syntax. Once the machine learning model is created the console will display a message 'Query successfully completed'. What we did here was to create a predictor called patients_target to predict the presence of heart disease as target . The model has started training. To check if the training has finished you can SELECT the model name from the predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'patients_target' ; The complete status means that the model training has successfully finished. Using SQL Statements to make predictions \u00b6 The next steps would be to query the model and predict the heart disease risk. Let\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level of 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2. Add all of this information to the WHERE clause. SELECT target as prediction , target_confidence as confidence , target_explain as info FROM mindsdb . patients_target WHERE age = 30 AND chol = 177 AND slope = 2 AND thal = 2 ; With a confidence of around 83%, MindsDB predicted a high risk of heart disease for this patient. The above example shows how you can make predictions for a single patient. But what if you have a table in your database with many patients\u2019 diagnosis data, and you want to make predictions for them in bulk? For this purpose, you can join the predictor with such a table. SELECT * FROM mindsdb_predictions . heart_disease AS t JOIN mindsdb . patients_target AS tb WHERE t . thal = 2 ; Now you can even connect the output table to your BI tool and for more convenient visualization of the results using graphs or pivots. Conclusion \u00b6 In this tutorial, you have seen how easy it is to apply machine learning for your predictive needs. MindsDB's innovative open-source technology is making it easy to leverage machine learning for people who are not experts in this field. However, MindsDB is a great tool for ML practitioners as well: if you are a skilled data scientist, you could also benefit from the convenience of deploying custom machine learning solutions within databases by building & configuring models manually through a declarative syntax called JSON-AI . There are other interesting ML use cases where MindsDB is positioned extremely well, like multivariate time-series and real-time data streams, so feel free to check it yourself .","title":"Heart Disease"},{"location":"sql/tutorials/heart-disease/#pre-requisites","text":"If you want to install MindsDB locally, check out the installation guide for Docker or PyPi and you can follow this tutorial. If you are OK with using MindsDB cloud, then simply create a free account and you will be up and running in just one minute. Second, you will need to have a mysql client like DBeaver, MySQL Workbench etc. installed locally to connect to the MindsDB MySQL API. MindsDB contains a SQL Editor which can be accessed on MindsDB cloud or the URL 127.0.0.1:47334/.","title":"Pre-requisites"},{"location":"sql/tutorials/heart-disease/#data-overview","text":"For the example of this tutorial, we will use the heart disease dataset available publicly in Kaggle . Each row represents a patient and we will train a machine learning model to help us predict if the patient is classified as a heart disease patient. Below is a short description of each feature inside the data. age - In Years sex - 1 = Male; 0 = Female cp - chest pain type (4 values) trestbps - Resting blood pressure (in mm Hg on admission to the hospital) chol - Serum cholesterol in mg/dl fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) restecg - Resting electrocardiographic results thalach - Maximum heart rate achieved exang - Exercise induced angina (1 = yes; 0 = no) oldpeak - ST depression induced by exercise relative to rest slope - the slope of the peak exercise ST segment ca - Number of major vessels (0-3) colored by fluoroscopy thal - 1 = normal; 2 = fixed defect; 3 = reversible defect target - 1 or 0 (This is what we will predict)","title":"Data Overview"},{"location":"sql/tutorials/heart-disease/#how-to-use-mindsdb","text":"MindsDB allows you to automatically create & train machine learning models from the data in your database that you have connected to in the previous step. MindsDB works via MySQL wire protocol, which means you can do all these steps through SQL commands. When it comes to making predictions, SQL queries become even handier, because you can easily make them straight from your existing applications or Business Intelligence tools that already speak SQL. The ML models are available to use immediately after being trained as if they were virtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it works.","title":"How to use MindsDB"},{"location":"sql/tutorials/heart-disease/#connect-to-your-data","text":"First, we need to connect MindsDB to the database where the Heart Disease data is stored: - Access MindsDB GUI on either cloud or local via the URL 127.0.0.1:47334/ - On the default page, select the button Add Data or alternatively select the plug icon on the left sidebar. - The 'Select your data source' page will populate for you to choose your database type. For this tutorial we will be selecting the postgres database button. Once you have selected the database type,the page will automatically navigate to the SQL Editor where the syntax to create a database connection will automatically populate for you to enter the required parameters. The required parameters are: CREATE DATABASE display_name --- display name for database. WITH ENGINE = \"postgres\", --- name of the mindsdb handler PARAMETERS = { \"user\": \" \", --- Your database user. \"password\": \" \", --- Your password. \"host\": \" \", --- host, it can be an ip or an url. \"port\": \"5432\", --- common port is 5432. \"database\": \" \" --- The name of your database *optional.} Select the Run button or Shift+Enter to execute the syntax. Once the Database connection is created the console will display a message 'Query successfully completed'.","title":"Connect to your data"},{"location":"sql/tutorials/heart-disease/#create-a-machine-learning-model","text":"We can create a machine learning predictive model by using simple SQL statements executed in the SQL Editor. To create and train a new machine learning model we will need to use the CREATE PREDICTOR statement: CREATE PREDICTOR mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The required values that we need to provide are: predictor_name (string): The name of the model integration_name (string): The name of the connection to your database. column_name (string): The feature you want to predict. To train the model that will predict the risk of heart disease as target we will run: CREATE PREDICTOR patients_target FROM mindsdb_predictions ( SELECT * FROM heart_disease ) PREDICT target ; Select the Run button or Shift+Enter to execute the syntax. Once the machine learning model is created the console will display a message 'Query successfully completed'. What we did here was to create a predictor called patients_target to predict the presence of heart disease as target . The model has started training. To check if the training has finished you can SELECT the model name from the predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'patients_target' ; The complete status means that the model training has successfully finished.","title":"Create a machine learning model."},{"location":"sql/tutorials/heart-disease/#using-sql-statements-to-make-predictions","text":"The next steps would be to query the model and predict the heart disease risk. Let\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level of 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2. Add all of this information to the WHERE clause. SELECT target as prediction , target_confidence as confidence , target_explain as info FROM mindsdb . patients_target WHERE age = 30 AND chol = 177 AND slope = 2 AND thal = 2 ; With a confidence of around 83%, MindsDB predicted a high risk of heart disease for this patient. The above example shows how you can make predictions for a single patient. But what if you have a table in your database with many patients\u2019 diagnosis data, and you want to make predictions for them in bulk? For this purpose, you can join the predictor with such a table. SELECT * FROM mindsdb_predictions . heart_disease AS t JOIN mindsdb . patients_target AS tb WHERE t . thal = 2 ; Now you can even connect the output table to your BI tool and for more convenient visualization of the results using graphs or pivots.","title":"Using SQL Statements to make predictions"},{"location":"sql/tutorials/heart-disease/#conclusion","text":"In this tutorial, you have seen how easy it is to apply machine learning for your predictive needs. MindsDB's innovative open-source technology is making it easy to leverage machine learning for people who are not experts in this field. However, MindsDB is a great tool for ML practitioners as well: if you are a skilled data scientist, you could also benefit from the convenience of deploying custom machine learning solutions within databases by building & configuring models manually through a declarative syntax called JSON-AI . There are other interesting ML use cases where MindsDB is positioned extremely well, like multivariate time-series and real-time data streams, so feel free to check it yourself .","title":"Conclusion"},{"location":"sql/tutorials/home-rentals/","text":"Predicting Home Rental Prices with MindsDB \u00b6 Introduction \u00b6 Follow these steps to create, train and query a machine learning model (predictor) using SQL that predicts the rental_price (label) for new properties given their attributes (features). The Data \u00b6 Connecting the data \u00b6 There are a couple of ways you can get the data to follow trough this tutorial. Connecting as a database via CREATE DATABASE Connecting as a file We have prepared a demo database you can connect to that contains the data to be used example_db . demo_data . home_rentals CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } Now you can run queries directly on the demo database. Let's start by previewing the data we will use to train our predictor: SELECT * FROM example_db . demo_data . home_rentals LIMIT 10 ; You can download the source file as a .CSV here and then upload via MindsDB SQL Editor Navigate to the Upload a file button. Import the file and name it home_rentals Now you can run queries directly on the demo file as if it was a database. Let's start by previewing the data we will use to train our predictor: SELECT * FROM files . home_rentals LIMIT 10 ; From now onwards we will use the table example_db . demo_data . home_rentals make sure you replace it for files.home_rentals if you are connecting the data as a file. Understanding the Data \u00b6 + -----------------+---------------------+------+----------+----------------+----------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + -----------------+---------------------+------+----------+----------------+----------------+--------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + -----------------+---------------------+------+----------+----------------+----------------+--------------+ Where: Column Description Data Type Usage number_of_rooms Number of rooms of a given house [0,1,2,3] integer Feature number_of_bathrooms Number of bathrooms on a given house [1,2] integer Feature sqft Area of a given house in square feet integer Feature location Rating of the location of a given house [poor, great, good] character varying Feature days_on_market Number of days a given house has been open to be rented integer Feature neighborhood Neighborhood a given house is in [alcatraz_ave, westbrae, ..., south_side, thowsand_oaks ] character varying Feature rental_price Price for renting a given house in dollars integer Label Labels and Features A label is the thing we're predicting\u2014the y variable in simple linear regression ... A feature is an input variable\u2014the x variable in simple linear regression .. Training a Predictor Via CREATE PREDICTOR \u00b6 Let's create and train your first machine learning predictor. For that we are going to use the CREATE PREDICTOR syntax, where we specify what sub-query to train FROM (features) and what we want to learn to PREDICT (labels): CREATE PREDICTOR mindsdb . home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; Checking the Status of a Predictor \u00b6 A predictor may take a couple of minutes for the training to complete. You can monitor the status of your predictor by copying and pasting this command into your SQL client: SELECT status FROM mindsdb . predictors WHERE name = 'home_rentals_predictor' ; Here we are selecting the status from the table called mindsdb.predictors and using the where statement to only show the model we have just trained, On execution, you we get: + ----------+ | status | + ----------+ | training | + ----------+ Or after a the model has been trained: + ----------+ | status | + ----------+ | complete | + ----------+ Making Predictions \u00b6 Predictor Status Must be 'complete' Before Making a Prediction Making Predictions Via SELECT \u00b6 Once the predictor's status is complete. You can make predictions by querying the predictor as if it was a normal table: The SELECT syntax will allow you to make a prediction based on features. SELECT rental_price , rental_price_explain FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, you should get: + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Making Batch Predictions Via JOIN \u00b6 You can also make bulk predictions by joining a table with your predictor: SELECT t . rental_price as real_price , m . rental_price as predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals as t JOIN mindsdb . home_rentals_model as m limit 100 + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+","title":"Home Rentals"},{"location":"sql/tutorials/home-rentals/#predicting-home-rental-prices-with-mindsdb","text":"","title":"Predicting Home Rental Prices with MindsDB"},{"location":"sql/tutorials/home-rentals/#introduction","text":"Follow these steps to create, train and query a machine learning model (predictor) using SQL that predicts the rental_price (label) for new properties given their attributes (features).","title":"Introduction"},{"location":"sql/tutorials/home-rentals/#the-data","text":"","title":"The Data"},{"location":"sql/tutorials/home-rentals/#connecting-the-data","text":"There are a couple of ways you can get the data to follow trough this tutorial. Connecting as a database via CREATE DATABASE Connecting as a file We have prepared a demo database you can connect to that contains the data to be used example_db . demo_data . home_rentals CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } Now you can run queries directly on the demo database. Let's start by previewing the data we will use to train our predictor: SELECT * FROM example_db . demo_data . home_rentals LIMIT 10 ; You can download the source file as a .CSV here and then upload via MindsDB SQL Editor Navigate to the Upload a file button. Import the file and name it home_rentals Now you can run queries directly on the demo file as if it was a database. Let's start by previewing the data we will use to train our predictor: SELECT * FROM files . home_rentals LIMIT 10 ; From now onwards we will use the table example_db . demo_data . home_rentals make sure you replace it for files.home_rentals if you are connecting the data as a file.","title":"Connecting the data"},{"location":"sql/tutorials/home-rentals/#understanding-the-data","text":"+ -----------------+---------------------+------+----------+----------------+----------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + -----------------+---------------------+------+----------+----------------+----------------+--------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + -----------------+---------------------+------+----------+----------------+----------------+--------------+ Where: Column Description Data Type Usage number_of_rooms Number of rooms of a given house [0,1,2,3] integer Feature number_of_bathrooms Number of bathrooms on a given house [1,2] integer Feature sqft Area of a given house in square feet integer Feature location Rating of the location of a given house [poor, great, good] character varying Feature days_on_market Number of days a given house has been open to be rented integer Feature neighborhood Neighborhood a given house is in [alcatraz_ave, westbrae, ..., south_side, thowsand_oaks ] character varying Feature rental_price Price for renting a given house in dollars integer Label Labels and Features A label is the thing we're predicting\u2014the y variable in simple linear regression ... A feature is an input variable\u2014the x variable in simple linear regression ..","title":"Understanding the Data"},{"location":"sql/tutorials/home-rentals/#training-a-predictor-via-create-predictor","text":"Let's create and train your first machine learning predictor. For that we are going to use the CREATE PREDICTOR syntax, where we specify what sub-query to train FROM (features) and what we want to learn to PREDICT (labels): CREATE PREDICTOR mindsdb . home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ;","title":"Training a Predictor Via CREATE PREDICTOR"},{"location":"sql/tutorials/home-rentals/#checking-the-status-of-a-predictor","text":"A predictor may take a couple of minutes for the training to complete. You can monitor the status of your predictor by copying and pasting this command into your SQL client: SELECT status FROM mindsdb . predictors WHERE name = 'home_rentals_predictor' ; Here we are selecting the status from the table called mindsdb.predictors and using the where statement to only show the model we have just trained, On execution, you we get: + ----------+ | status | + ----------+ | training | + ----------+ Or after a the model has been trained: + ----------+ | status | + ----------+ | complete | + ----------+","title":"Checking the Status of a Predictor"},{"location":"sql/tutorials/home-rentals/#making-predictions","text":"Predictor Status Must be 'complete' Before Making a Prediction","title":"Making Predictions"},{"location":"sql/tutorials/home-rentals/#making-predictions-via-select","text":"Once the predictor's status is complete. You can make predictions by querying the predictor as if it was a normal table: The SELECT syntax will allow you to make a prediction based on features. SELECT rental_price , rental_price_explain FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, you should get: + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0 . 99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+","title":"Making Predictions Via SELECT"},{"location":"sql/tutorials/home-rentals/#making-batch-predictions-via-join","text":"You can also make bulk predictions by joining a table with your predictor: SELECT t . rental_price as real_price , m . rental_price as predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals as t JOIN mindsdb . home_rentals_model as m limit 100 + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+","title":"Making Batch Predictions Via JOIN"},{"location":"sql/tutorials/house-sales-forecasting/","text":"Forecast quarterly house sales using MindsDB \u00b6 Introduction \u00b6 In this short example we will produce forecasts for a multivariate time series. Data and setup \u00b6 The dataset we will use is the pre-processed version of the \"House Property Sales\" Kaggle competition , in particular, the \"ma_lga_12345.csv\" file, which tracks quarterly moving averages of house sales aggregated by type and amount of bedrooms in each listing. Make sure you have access to a working MindsDB installation (either local or via cloud.mindsdb.com), and either load it into a table in your database of choice or upload the file directly to the special FILES datasource (via SQL or GUI). Once you've done this, proceed to the next step. For the rest of the tutorial we'll assume you've opted for the latter option and uploaded the file with the name HR_MA : SHOW TABLES FROM files; SELECT * FROM files.HR_MA LIMIT 10; Create a time series predictor \u00b6 Now, we can specify that we want to forecast the MA column, which is a moving average of the historical median price for house sales. However, looking at the data you can see several entries for the same date, depending on two factors: how many bedrooms the properties had, and whether properties were \"houses\" or \"units\". This means that we can have up to ten different groupings here (although, if you do some digging, you will find we only actually have seven of the possible ten combinations in practice). MindsDB makes it simple so that we don't need to repeat the predictor creation process for every group there is. Instead, we can just group for both columns and the predictor will learn from all series and enable forecasts for all of them! The command for this is: CREATE PREDICTOR mindsdb.home_sales_model FROM files (SELECT * FROM HR_MA) PREDICT MA ORDER BY saledate GROUP BY bedrooms, type -- as the target is quarterly, we will look back two years to forecast the next one WINDOW 8 HORIZON 4; You can check the status of the predictor: SELECT * FROM mindsdb.predictors where name='home_sales_model'; Generating some forecasts \u00b6 Once the predictor has been successfully trained, you can query it to get forecasts for a given period of time. Usually, you'll want to know what happens right after the latest training data point that was fed, for which we have a special bit of syntax, the \"LATEST\" key word: SELECT m.saledate as date, m.MA as forecast FROM mindsdb.home_sales_model as m JOIN files.HR_MA as t WHERE t.saledate > LATEST AND t.type = 'house' AND t.bedrooms = 2 LIMIT 4; Now, try changing type to unit or bedrooms to any number between 1 to 5, and check how the forecast varies. This is because MindsDB recognizes each grouping as being its own different time series. Next steps \u00b6 We invite you to try this tutorial with your own time series data. If you have any questions or feedback, be sure to join our slack community or make an issue in the github repository!","title":"Forecasting house sales"},{"location":"sql/tutorials/house-sales-forecasting/#forecast-quarterly-house-sales-using-mindsdb","text":"","title":"Forecast quarterly house sales using MindsDB"},{"location":"sql/tutorials/house-sales-forecasting/#introduction","text":"In this short example we will produce forecasts for a multivariate time series.","title":"Introduction"},{"location":"sql/tutorials/house-sales-forecasting/#data-and-setup","text":"The dataset we will use is the pre-processed version of the \"House Property Sales\" Kaggle competition , in particular, the \"ma_lga_12345.csv\" file, which tracks quarterly moving averages of house sales aggregated by type and amount of bedrooms in each listing. Make sure you have access to a working MindsDB installation (either local or via cloud.mindsdb.com), and either load it into a table in your database of choice or upload the file directly to the special FILES datasource (via SQL or GUI). Once you've done this, proceed to the next step. For the rest of the tutorial we'll assume you've opted for the latter option and uploaded the file with the name HR_MA : SHOW TABLES FROM files; SELECT * FROM files.HR_MA LIMIT 10;","title":"Data and setup"},{"location":"sql/tutorials/house-sales-forecasting/#create-a-time-series-predictor","text":"Now, we can specify that we want to forecast the MA column, which is a moving average of the historical median price for house sales. However, looking at the data you can see several entries for the same date, depending on two factors: how many bedrooms the properties had, and whether properties were \"houses\" or \"units\". This means that we can have up to ten different groupings here (although, if you do some digging, you will find we only actually have seven of the possible ten combinations in practice). MindsDB makes it simple so that we don't need to repeat the predictor creation process for every group there is. Instead, we can just group for both columns and the predictor will learn from all series and enable forecasts for all of them! The command for this is: CREATE PREDICTOR mindsdb.home_sales_model FROM files (SELECT * FROM HR_MA) PREDICT MA ORDER BY saledate GROUP BY bedrooms, type -- as the target is quarterly, we will look back two years to forecast the next one WINDOW 8 HORIZON 4; You can check the status of the predictor: SELECT * FROM mindsdb.predictors where name='home_sales_model';","title":"Create a time series predictor"},{"location":"sql/tutorials/house-sales-forecasting/#generating-some-forecasts","text":"Once the predictor has been successfully trained, you can query it to get forecasts for a given period of time. Usually, you'll want to know what happens right after the latest training data point that was fed, for which we have a special bit of syntax, the \"LATEST\" key word: SELECT m.saledate as date, m.MA as forecast FROM mindsdb.home_sales_model as m JOIN files.HR_MA as t WHERE t.saledate > LATEST AND t.type = 'house' AND t.bedrooms = 2 LIMIT 4; Now, try changing type to unit or bedrooms to any number between 1 to 5, and check how the forecast varies. This is because MindsDB recognizes each grouping as being its own different time series.","title":"Generating some forecasts"},{"location":"sql/tutorials/house-sales-forecasting/#next-steps","text":"We invite you to try this tutorial with your own time series data. If you have any questions or feedback, be sure to join our slack community or make an issue in the github repository!","title":"Next steps"},{"location":"sql/tutorials/insurance-cost-prediction/","text":"Predict Insurance Cost using MindsDB \u00b6 Dataset: Medical Cost Personal Data Communtiy Author: Kinie K Kusuma Pre-requisites \u00b6 First, you need MindsDB installed. So please make sure you've visited Getting Started Guide and Getting Started with Cloud . You may start to use MindsDB by installing it locally or you can use the Cloud service. Let\u2019s use the cloud for this tutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API. Can you accurately predict insurance costs? \u00b6 In this tutorial, you will learn how to predict insurance costs using MindsDB. This tutorial is very easy because you don't need to learn any machine learning algorithms, all you need to know is just SQL. The process looks like the following: First we will connect MindsDB to a database with past data so it can learn from it We will use a single SQL command that will tell MindsDB to train its predictor We will use the standard SQL Select statement to get predictions from AI Tables in MindsDB. Like if this data already exists! MindsDB will execute a complete Machine Learning workflow behind the scenes, it will determine data types for each column, normalize and encode it, train and test ML model. All this happens automatically, so it is very cool! Those who want to get their hands dirty with manual hyperparameters optimization, you can also do that with MindsDB using a declarative syntax called JSON-AI. So let's look at how it works using a real use case. For the demo purpose we will use a public dataset from Kaggle, but you are free to follow this tutorial with your own data. Connect your database \u00b6 First, you need to connect MindsDB to the database where the data is stored. Open MindsDB GUI and in the left navigation click on Database, then click on the ADD DATABASE. Here, you need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it differently Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction. Connect to MindsDB\u2019s MySQL API \u00b6 Here I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data \u00b6 Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and because the tutorial is about insurance cost prediction, we will use the insurance table. Let's check what is inside this table. So, these tables have 7 columns: age: The age of the person (integer) sex: Gender (male or female) bmi: Body mass index is a value derived from the mass and height of a person. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and height in meters (float) children: The number of children (integer) smoker: Indicator if the person smoke (yes or no) region: Region where the insured lives (southeast, northeast, southwest or northwest) charges: The insurance cost, this is the target of prediction (float) Create the model \u00b6 Now, to create the model, let's move to the MindsDB database, and see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE PREDICTOR as a new record inside the predictors table, and using this command: CREATE PREDICTOR mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_name (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. So, use this command to create the models: CREATE PREDICTOR insurance_cost_predictor FROM insurance_costs ( SELECT * from insurance ) PREDICT charges ; If there's no error, that means your model is created and training has started. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. SELECT * FROM mindsdb . predictors WHERE name = 'insurance_cost_predictor' ; If the predictor is ready, it will look like this. The model has been created and trained! The reported accuracy is 75%. If you want to have more control over the model, head to lightwood.io to see how that can be customized. Make prediction \u00b6 Now you are in the last step of this tutorial, making the prediction. To make a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE column3 = \"value\" AND column2 = value ; You need to set these values: target_variable - The original value of the target variable. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. when_data - The data to make the predictions from(WHERE clause params). SELECT charges , charges_confidence , charges_explain as info FROM insurance_cost_predictor WHERE age = 20 AND sex = 'male' AND bmi = 33 . 20 AND children = 0 AND smoker = 'no' AND region = 'southeast' ; Finally, we have trained an insurance model using SQL and MindsDB. Conclusions \u00b6 As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself, MindsDB has an option of a free cloud account that is more than enough to give it a try.","title":"Insurance Cost"},{"location":"sql/tutorials/insurance-cost-prediction/#predict-insurance-cost-using-mindsdb","text":"Dataset: Medical Cost Personal Data Communtiy Author: Kinie K Kusuma","title":"Predict Insurance Cost using MindsDB"},{"location":"sql/tutorials/insurance-cost-prediction/#pre-requisites","text":"First, you need MindsDB installed. So please make sure you've visited Getting Started Guide and Getting Started with Cloud . You may start to use MindsDB by installing it locally or you can use the Cloud service. Let\u2019s use the cloud for this tutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/insurance-cost-prediction/#can-you-accurately-predict-insurance-costs","text":"In this tutorial, you will learn how to predict insurance costs using MindsDB. This tutorial is very easy because you don't need to learn any machine learning algorithms, all you need to know is just SQL. The process looks like the following: First we will connect MindsDB to a database with past data so it can learn from it We will use a single SQL command that will tell MindsDB to train its predictor We will use the standard SQL Select statement to get predictions from AI Tables in MindsDB. Like if this data already exists! MindsDB will execute a complete Machine Learning workflow behind the scenes, it will determine data types for each column, normalize and encode it, train and test ML model. All this happens automatically, so it is very cool! Those who want to get their hands dirty with manual hyperparameters optimization, you can also do that with MindsDB using a declarative syntax called JSON-AI. So let's look at how it works using a real use case. For the demo purpose we will use a public dataset from Kaggle, but you are free to follow this tutorial with your own data.","title":"Can you accurately predict insurance costs?"},{"location":"sql/tutorials/insurance-cost-prediction/#connect-your-database","text":"First, you need to connect MindsDB to the database where the data is stored. Open MindsDB GUI and in the left navigation click on Database, then click on the ADD DATABASE. Here, you need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it differently Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.","title":"Connect your database"},{"location":"sql/tutorials/insurance-cost-prediction/#connect-to-mindsdbs-mysql-api","text":"Here I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/insurance-cost-prediction/#data","text":"Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and because the tutorial is about insurance cost prediction, we will use the insurance table. Let's check what is inside this table. So, these tables have 7 columns: age: The age of the person (integer) sex: Gender (male or female) bmi: Body mass index is a value derived from the mass and height of a person. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and height in meters (float) children: The number of children (integer) smoker: Indicator if the person smoke (yes or no) region: Region where the insured lives (southeast, northeast, southwest or northwest) charges: The insurance cost, this is the target of prediction (float)","title":"Data"},{"location":"sql/tutorials/insurance-cost-prediction/#create-the-model","text":"Now, to create the model, let's move to the MindsDB database, and see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE PREDICTOR as a new record inside the predictors table, and using this command: CREATE PREDICTOR mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_name (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. So, use this command to create the models: CREATE PREDICTOR insurance_cost_predictor FROM insurance_costs ( SELECT * from insurance ) PREDICT charges ; If there's no error, that means your model is created and training has started. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. SELECT * FROM mindsdb . predictors WHERE name = 'insurance_cost_predictor' ; If the predictor is ready, it will look like this. The model has been created and trained! The reported accuracy is 75%. If you want to have more control over the model, head to lightwood.io to see how that can be customized.","title":"Create the model"},{"location":"sql/tutorials/insurance-cost-prediction/#make-prediction","text":"Now you are in the last step of this tutorial, making the prediction. To make a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE column3 = \"value\" AND column2 = value ; You need to set these values: target_variable - The original value of the target variable. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. when_data - The data to make the predictions from(WHERE clause params). SELECT charges , charges_confidence , charges_explain as info FROM insurance_cost_predictor WHERE age = 20 AND sex = 'male' AND bmi = 33 . 20 AND children = 0 AND smoker = 'no' AND region = 'southeast' ; Finally, we have trained an insurance model using SQL and MindsDB.","title":"Make prediction"},{"location":"sql/tutorials/insurance-cost-prediction/#conclusions","text":"As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself, MindsDB has an option of a free cloud account that is more than enough to give it a try.","title":"Conclusions"},{"location":"sql/tutorials/mindsdb-superset-snowflake/","text":"Using MindsDB Machine Learning to Solve a Real-World time series Problem \u00b6 Let\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here . Imagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend) This is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types. Let\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results. Set Up MindsDB \u00b6 First things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI . Once an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc. Connect MindsDB to the Data for model training \u00b6 MindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to. Step 1: Getting the Training Data \u00b6 We start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct. show databases ; MindsDB comes with some built-in databases as follows: INFORMATION_SCHEMA stores information about MindsDB, MINDSDB stores metadata about the predictors and allows access to the created predictors as tables, DATASOURCE for connecting to data or uploading files. The SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it. SELECT * FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST LIMIT 100 ; The training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3. Step 2: Training the Predictive Model \u00b6 Let\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database. use mindsdb ; show tables MINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following. SELECT name , status FROM MINDSDB . PREDICTORS ; The process of training a predictive model using MindsDB is as simple as creating a view or a table. CREATE PREDICTOR mindsdb . rides_forecaster_demo FROM snf ( SELECT ROUTE , RIDES , DATE FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01' ) PREDICT RIDES ORDER BY DATE GROUP BY ROUTE WINDOW 10 HORIZON 7 ; Let\u2019s discuss the statement above. We create a predictor table using the CREATE PREDICTOR statement and specifying the database from which the training data comes. The code in yellow selects the filtered training data. After that, we use the PREDICT keyword to define the column whose data we want to forecast. Next, there are standard SQL clauses, such as ORDER BY, GROUP BY, WINDOW, and HORIZON . We use the ORDER BY clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The GROUP BY clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use WINDOW 10. To prepare the forecast of the number of bus rides for the next week, we define HORIZON 7. Now, you can execute the CREATE PREDICTOR statement and wait until your predictive model is complete. The MINDSDB.PREDICTORS table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete. Step 3: Getting the Forecasts \u00b6 We are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it. We consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data. SELECT tb . ROUTE , tb . RIDES AS PREDICTED_RIDES FROM snf . PUBLIC . CTA_BUS_RIDES_LATEST AS ta JOIN mindsdb . rides_forecaster_demo AS tb WHERE ta . ROUTE = \"171\" AND ta . DATE > LATEST LIMIT 7 ; Let\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route. Let\u2019s run the query above to forecast the number of rides for route 171 in the next seven days. Now we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes. Thanks to the special SQL syntax that includes CREATE PREDICTOR, PREDICT, and > LATEST, MindsDB makes it straightforward to run predictors on our chosen data. Now, let\u2019s visualize our predictions. Visualizing the Results using Apache Superset \u00b6 Apache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types. You can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB. Visualizing Data \u00b6 The two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp. Superset lets us visualize the stops_by_route data set as follows. Every bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field. Now, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple). Predictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions. Lastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d. Let\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save . Let\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names: A Value filter on the route column from the forecasts table. A Numerical range filter on the volatility column from the stops_by_route table. Click Save to publish these filters. Let\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001. We could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off. Now, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55. Conclusions: Powerful forecasting with MindsDB, your database, and Superset \u00b6 The combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards. MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud , a hassle-free and fully hosted cloud service for Superset. We encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities.","title":"AI powered forecasts with MindsDB"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#using-mindsdb-machine-learning-to-solve-a-real-world-time-series-problem","text":"Let\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here . Imagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend) This is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types. Let\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results.","title":"Using MindsDB Machine Learning to Solve a Real-World time series Problem"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#set-up-mindsdb","text":"First things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI . Once an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc.","title":"Set Up MindsDB"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#connect-mindsdb-to-the-data-for-model-training","text":"MindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to.","title":"Connect MindsDB to the Data for model training"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-1-getting-the-training-data","text":"We start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct. show databases ; MindsDB comes with some built-in databases as follows: INFORMATION_SCHEMA stores information about MindsDB, MINDSDB stores metadata about the predictors and allows access to the created predictors as tables, DATASOURCE for connecting to data or uploading files. The SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it. SELECT * FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST LIMIT 100 ; The training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3.","title":"Step 1: Getting the Training Data"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-2-training-the-predictive-model","text":"Let\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database. use mindsdb ; show tables MINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following. SELECT name , status FROM MINDSDB . PREDICTORS ; The process of training a predictive model using MindsDB is as simple as creating a view or a table. CREATE PREDICTOR mindsdb . rides_forecaster_demo FROM snf ( SELECT ROUTE , RIDES , DATE FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01' ) PREDICT RIDES ORDER BY DATE GROUP BY ROUTE WINDOW 10 HORIZON 7 ; Let\u2019s discuss the statement above. We create a predictor table using the CREATE PREDICTOR statement and specifying the database from which the training data comes. The code in yellow selects the filtered training data. After that, we use the PREDICT keyword to define the column whose data we want to forecast. Next, there are standard SQL clauses, such as ORDER BY, GROUP BY, WINDOW, and HORIZON . We use the ORDER BY clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The GROUP BY clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use WINDOW 10. To prepare the forecast of the number of bus rides for the next week, we define HORIZON 7. Now, you can execute the CREATE PREDICTOR statement and wait until your predictive model is complete. The MINDSDB.PREDICTORS table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete.","title":"Step 2: Training the Predictive Model"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-3-getting-the-forecasts","text":"We are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it. We consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data. SELECT tb . ROUTE , tb . RIDES AS PREDICTED_RIDES FROM snf . PUBLIC . CTA_BUS_RIDES_LATEST AS ta JOIN mindsdb . rides_forecaster_demo AS tb WHERE ta . ROUTE = \"171\" AND ta . DATE > LATEST LIMIT 7 ; Let\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route. Let\u2019s run the query above to forecast the number of rides for route 171 in the next seven days. Now we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes. Thanks to the special SQL syntax that includes CREATE PREDICTOR, PREDICT, and > LATEST, MindsDB makes it straightforward to run predictors on our chosen data. Now, let\u2019s visualize our predictions.","title":"Step 3: Getting the Forecasts"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#visualizing-the-results-using-apache-superset","text":"Apache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types. You can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB.","title":"Visualizing the Results using Apache Superset"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#visualizing-data","text":"The two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp. Superset lets us visualize the stops_by_route data set as follows. Every bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field. Now, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple). Predictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions. Lastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d. Let\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save . Let\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names: A Value filter on the route column from the forecasts table. A Numerical range filter on the volatility column from the stops_by_route table. Click Save to publish these filters. Let\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001. We could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off. Now, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55.","title":"Visualizing Data"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#conclusions-powerful-forecasting-with-mindsdb-your-database-and-superset","text":"The combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards. MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud , a hassle-free and fully hosted cloud service for Superset. We encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities.","title":"Conclusions: Powerful forecasting with MindsDB, your database, and Superset"},{"location":"sql/tutorials/mushrooms/","text":"Dataset: Mushrooms Community Author: Chandre Tosca Van Der Westhuizen Mushrooms are a fleshy sporocarp of fungi which can either be edible or poisonous. Its usage dates back centuries with ancient Greek, Chinese and African cultures. They can have high nutritional value and medicinal properties which provide great health benefits. On the other side,some of these fungi can be toxic and consuming the wrong mushroom can have deadly consequences. It is important for industries across the world, like the food and health sector, to identify which type of mushrooms are edible and which are poisonous. We will explore how MindsDB's machine learning predictive model can make it easier classifying mushrooms and predicting which is safe to consume and which can make you ill. Pre-requisites \u00b6 To ensure you can complete all the steps, make sure you have access to the following tools: A MindsDB instance. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Downloaded the dataset. You can get it from Kaggle Optional: Access to ngrok. You can check the installation details at the ngrok website . Data Overview \u00b6 The dataset consists of the following information: Attribute Information: (classes: edible=e, poisonous=p) - cap_shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s - cap_surface: fibrous=f,grooves=g,scaly=y,smooth=s - cap_color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y - bruises: bruises=t,no=f - odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s - gill_attachment: attached=a,descending=d,free=f,notched=n - gill_spacing: close=c,crowded=w,distant=d - gill_size: broad=b,narrow=n - gill_color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y - stalk_shape: enlarging=e,tapering=t - stalk_root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=? - stalk_surface_above_ring: fibrous=f,scaly=y,silky=k,smooth=s - stalk_surface_below_ring: fibrous=f,scaly=y,silky=k,smooth=s - stalk_color_above_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y - stalk_color_below_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y - veil_type: partial=p,universal=u - veil_color: brown=n,orange=o,white=w,yellow=y - ring_number: none=n,one=o,two=t - ring_type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z - spore_print_color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y - population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y - habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d Database connection to MindsDB \u00b6 To establish a database connection we will access MindsDB's GUI. MindsDB has a SQL Editor on Cloud and local via the URL 127.0.0.1:47334/. First, we need to connect MindsDB to the database where the Mushrooms data is stored: - Access MindsDB GUI on either cloud or the URL 127.0.0.1:47334/ - On the default page, select the button Add Data or alternatively select the plug icon on the left sidebar. - The 'Select your data source' page will populate for you to choose your database type. For this tutorial we will be selecting the postgres database button. Once you have selected the database type,the page will automatically navigate to the SQL Editor where the syntax to create a database connection will automatically populate for you to enter the required parameters. The required parameters are: CREATE DATABASE display_name --- display name for database. WITH ENGINE = \"postgres\", --- name of the mindsdb handler PARAMETERS = { \"user\": \" \", --- Your database user. \"password\": \" \", --- Your password. \"host\": \" \", --- host, it can be an ip or an url. \"port\": \"5432\", --- common port is 5432. \"database\": \" \" --- The name of your database *optional. } Select the Run button or Shift+Enter to execute the syntax. Once the Database connection is created the console will display a message 'Query successfully completed'. \u200b Please note that some database connections require running a Ngrok tunnel to establish a connection. Run the ngrok command in a terminal: ngrok tcp [ db-port ] for example,if your port number is 5433 you will see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://6.tcp.ngrok.io:14789 -> localhost:5433 The forwarded address information will be required when connecting to MindsDB's GUI. Select and copy the 'Forwarding' information, in this case it is 6.tcp.ngrok.io:14789 , where 6.tcp.ngrok.io will be used for the host parameter and 14789 as the port number. Once the database integration is successful we can query the table from the database to ensure the data pulls through on MindsDB. Create a machine learning model. \u00b6 Now we are ready to create our own predictor. We will start by using the SQL Editor to execute simple SQL syntax to create and train a machine learning predictive model. The predictor we will and be trained to determine if a mushroom is edible or poisonous.The following CREATE PREDICTOR statement is used to create predictors: CREATE PREDICTOR mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The required values that we need to provide are: \u200b - predictor_name (string): The name of the model - integration_name (string): The name of the connection to your database. - column_name (string): The feature you want to predict. Use the following query to create a predictor that will predict the target_class for the specific field parameters. CREATE PREDICTOR mushroom_predictor FROM mindsdb_predictions ( SELECT * FROM mushrooms ) PREDICT class ; Select the Run button or Shift+Enter to execute the syntax. Once the predictor is created the console will display a message 'Query successfully completed'. The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . predictors WHERE name = 'mushroom_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728 Make predictions \u00b6 In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL syntax. Use the following query using mock data with the predictor. SELECT class FROM mindsdb . mushroom_predictor WHERE cap_shape = 'x' AND cap_surface = 's' AND cap_color = 'n' AND bruises = 't' AND odor = 'p' AND gill_attachment = 'f' AND gill_spacing = 'c' AND gill_size = 'n' AND gill_color = 'k' AND stalk_shape = 'e' AND stalk_root = 'e' AND stalk_surface_above_ring = 's' AND stalk_surface_below_ring = 's' AND stalk_color_above_ring = 'w' AND stalk_color_below_ring = 'w' AND veil_type = 'p' AND veil_color = 'w' AND ring_number = 'o' AND ring_type = 'p' AND spore_print_color = 'k' AND population = 's' AND habitat = 'u' ; The result: The machine learning model has predicted that the mushroom is poisonous. We have successfully created and trained a model that can predict if a model is edible or poisonous. Want to try it out for yourself? Sign up for a free MindsDB account and join our community! Engage with MindsDB community on Slack or Github to ask questions, share and express ideas and thoughts! For more check out other tutorials and MindsDB documentation .","title":"Mushrooms Hunting"},{"location":"sql/tutorials/mushrooms/#pre-requisites","text":"To ensure you can complete all the steps, make sure you have access to the following tools: A MindsDB instance. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Downloaded the dataset. You can get it from Kaggle Optional: Access to ngrok. You can check the installation details at the ngrok website .","title":"Pre-requisites"},{"location":"sql/tutorials/mushrooms/#data-overview","text":"The dataset consists of the following information: Attribute Information: (classes: edible=e, poisonous=p) - cap_shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s - cap_surface: fibrous=f,grooves=g,scaly=y,smooth=s - cap_color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y - bruises: bruises=t,no=f - odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s - gill_attachment: attached=a,descending=d,free=f,notched=n - gill_spacing: close=c,crowded=w,distant=d - gill_size: broad=b,narrow=n - gill_color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y - stalk_shape: enlarging=e,tapering=t - stalk_root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=? - stalk_surface_above_ring: fibrous=f,scaly=y,silky=k,smooth=s - stalk_surface_below_ring: fibrous=f,scaly=y,silky=k,smooth=s - stalk_color_above_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y - stalk_color_below_ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y - veil_type: partial=p,universal=u - veil_color: brown=n,orange=o,white=w,yellow=y - ring_number: none=n,one=o,two=t - ring_type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z - spore_print_color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y - population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y - habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","title":"Data Overview"},{"location":"sql/tutorials/mushrooms/#database-connection-to-mindsdb","text":"To establish a database connection we will access MindsDB's GUI. MindsDB has a SQL Editor on Cloud and local via the URL 127.0.0.1:47334/. First, we need to connect MindsDB to the database where the Mushrooms data is stored: - Access MindsDB GUI on either cloud or the URL 127.0.0.1:47334/ - On the default page, select the button Add Data or alternatively select the plug icon on the left sidebar. - The 'Select your data source' page will populate for you to choose your database type. For this tutorial we will be selecting the postgres database button. Once you have selected the database type,the page will automatically navigate to the SQL Editor where the syntax to create a database connection will automatically populate for you to enter the required parameters. The required parameters are: CREATE DATABASE display_name --- display name for database. WITH ENGINE = \"postgres\", --- name of the mindsdb handler PARAMETERS = { \"user\": \" \", --- Your database user. \"password\": \" \", --- Your password. \"host\": \" \", --- host, it can be an ip or an url. \"port\": \"5432\", --- common port is 5432. \"database\": \" \" --- The name of your database *optional. } Select the Run button or Shift+Enter to execute the syntax. Once the Database connection is created the console will display a message 'Query successfully completed'. \u200b Please note that some database connections require running a Ngrok tunnel to establish a connection. Run the ngrok command in a terminal: ngrok tcp [ db-port ] for example,if your port number is 5433 you will see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://6.tcp.ngrok.io:14789 -> localhost:5433 The forwarded address information will be required when connecting to MindsDB's GUI. Select and copy the 'Forwarding' information, in this case it is 6.tcp.ngrok.io:14789 , where 6.tcp.ngrok.io will be used for the host parameter and 14789 as the port number. Once the database integration is successful we can query the table from the database to ensure the data pulls through on MindsDB.","title":"Database connection to MindsDB"},{"location":"sql/tutorials/mushrooms/#create-a-machine-learning-model","text":"Now we are ready to create our own predictor. We will start by using the SQL Editor to execute simple SQL syntax to create and train a machine learning predictive model. The predictor we will and be trained to determine if a mushroom is edible or poisonous.The following CREATE PREDICTOR statement is used to create predictors: CREATE PREDICTOR mindsdb . predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name ; The required values that we need to provide are: \u200b - predictor_name (string): The name of the model - integration_name (string): The name of the connection to your database. - column_name (string): The feature you want to predict. Use the following query to create a predictor that will predict the target_class for the specific field parameters. CREATE PREDICTOR mushroom_predictor FROM mindsdb_predictions ( SELECT * FROM mushrooms ) PREDICT class ; Select the Run button or Shift+Enter to execute the syntax. Once the predictor is created the console will display a message 'Query successfully completed'. The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . predictors WHERE name = 'mushroom_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728","title":"Create a machine learning model."},{"location":"sql/tutorials/mushrooms/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL syntax. Use the following query using mock data with the predictor. SELECT class FROM mindsdb . mushroom_predictor WHERE cap_shape = 'x' AND cap_surface = 's' AND cap_color = 'n' AND bruises = 't' AND odor = 'p' AND gill_attachment = 'f' AND gill_spacing = 'c' AND gill_size = 'n' AND gill_color = 'k' AND stalk_shape = 'e' AND stalk_root = 'e' AND stalk_surface_above_ring = 's' AND stalk_surface_below_ring = 's' AND stalk_color_above_ring = 'w' AND stalk_color_below_ring = 'w' AND veil_type = 'p' AND veil_color = 'w' AND ring_number = 'o' AND ring_type = 'p' AND spore_print_color = 'k' AND population = 's' AND habitat = 'u' ; The result: The machine learning model has predicted that the mushroom is poisonous. We have successfully created and trained a model that can predict if a model is edible or poisonous. Want to try it out for yourself? Sign up for a free MindsDB account and join our community! Engage with MindsDB community on Slack or Github to ask questions, share and express ideas and thoughts! For more check out other tutorials and MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/process-quality/","text":"Dataset: Mining process Data Communtiy Author: pixpack Pre-requisites \u00b6 Before you start make sure that you've: Visited Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. You can get it from Kaggle . Optional- Visual studio. Manufacturing process quality \u00b6 Predicting process result quality is a common task in manufacturing analytics. Manufacturing plants commonly use quality predictions to gain a competitive edge over their competitors, improve their products or increase their customers satisfaction. MindsDB is a tool that can help you solve quality prediction tasks easily and effectively using machine learning. MindsDB abstracts ML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just using normal SQL commands. In this tutorial you will learn how to predict the quality of a mining process using MindsDB . Upload a file \u00b6 Fix headers: sed -e 's/ /_/g' -e 's/\\(.*\\)/\\L\\1/' -e 's/%_//g' MiningProcess_Flotation_Plant_Database.csv > fixed_headers.csv (for Linux/Unix) edit headers manually: change space to underscore , upper case to lower case, remove % from headers (for Windows) Access MindsDB GUI either on cloud or local via URL 127.0.0.1:47334/. Select the Add data button or the plug icon on the left side bar. The page will navigate to the 'Select your data source' page. Select 'Files'. Under 'Import a file' select the tab 'Click here to browse local files' and select your data file. Once the file is uploaded 100%, provide a name for the data file that will be saved as a table. Select the button Save and Continue. You can query the file that has been uploaded to see that the data does pull through. SELECT * from files . file_name ; Connect to MindsDB SQL Sever \u00b6 MindsDB's GUI has a SQL Editor that allows you to create and train predictors and also makes predictions. However you can also do this via the mysql client in a local terminal by accessing MindsDB SQL Server. mysql - h cloud . mindsdb . com --port 3306 -u username@email.com -p USE mindsdb ; Create a predictor \u00b6 In this section you will connect to MindsDB with the MySql API and create a Predictor. It is in MindsDB terms a machine learning model, but all its complexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML expert and want to tweak the model, MindsDB also allows you that (please refer to documentation). Use the following query to create a Predictor that will foretell the silica_concentrate at the end of our mining process. The row number is limited to 5000 to speed up training but you can keep the whole dataset. CREATE PREDICTOR mindsdb . process_quality_predictor FROM files ( SELECT iron_feed , silica_feed , starch_flow , amina_flow , ore_pulp_flow , ore_pulp_ph , ore_pulp_density , flotation_column_01_air_flow , flotation_column_02_air_flow , flotation_column_03_air_flow , flotation_column_04_air_flow , flotation_column_05_air_flow , flotation_column_06_air_flow , flotation_column_07_air_flow , flotation_column_01_level , flotation_column_02_level , flotation_column_03_level , flotation_column_04_level , flotation_column_05_level , flotation_column_06_level , flotation_column_07_level , iron_concentrate , silica_concentrate from process_quality FROM process_quality LIMIT 5000 ) PREDICT silica_concentrate as quality USING ; After creating the Predictor you should see a similar output: Query OK, 0 rows affected (2 min 27.52 sec) Now the Predictor will begin training. You can check the status with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'process_quality_predictor' ; After the Predictor has finished training, you will see a similar output. +-----------------------------+----------+----------+--------------------+-------------------+------------------+ | name | status | accuracy | predict | select_data_query | training_options | +-----------------------------+----------+----------+--------------------+-------------------+------------------+ | process_quality_predictor | complete | 1 | silica_concentrate | | | +-----------------------------+----------+----------+--------------------+-------------------+------------------+ 1 row in set (0.28 sec) As you can see the accuracy of the model is 1 (i.e. 100%). This is the result of using a limited dataset of 5000 rows. In reality when using the whole dataset, you will probably see a more reasonable accuracy. You are now done with creating the predictor! \u2728 Make predictions \u00b6 In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT silica_concentrate , silica_concentrate_confidence , silica_concentrate_explain FROM mindsdb . process_quality_predictor WHERE iron_feed = 48 . 81 AND silica_feed = 25 . 31 AND starch_flow = 2504 . 94 AND amina_flow = 309 . 448 AND ore_pulp_flow = 377 . 6511682692 AND ore_pulp_ph = 10 . 0607 AND ore_pulp_density = 1 . 68676 ; The output should look similar to this. +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | silica_concentrate | silica_concentrate_confidence | Info | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | 1.68 | 0.99 | {\"predicted_value\": \"1.68\", \"confidence\": 0.99, \"confidence_lower_bound\": null, \"confidence_upper_bound\": null, \"anomaly\": null, \"truth\": null} | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.81 sec) As you can see, the model predicted the silica concentrate for our data point. Again we can see a very high confidence due to the limited dataset. When making predictions you can include different fields. As you can notice, we have only included the first 7 fields of our dataset. You are free to test different combinations. In the previous example, we have made a prediction for a single data point. In a real scenario, you might want to make predictions on multiple data points. In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how to make batch predictions. Use the following command to create the batch prediction. SELECT collected_data . iron_feed , collected_data . silica_feed , collected_data . starch_flow , collected_data . amina_flow , collected_data . ore_pulp_flow , collected_data . ore_pulp_ph , collected_data . ore_pulp_density , predictions . silica_concentrate_confidence as confidence , predictions . silica_concentrate as predicted_silica_concentrate FROM process_quality_integration . process_quality AS collected_data JOIN mindsdb . process_quality_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | 58.84 | 11.46 | 3277.34 | 564.209 | 403.242 | 9.88472 | 1.76297 | 0.99 | 2.129567174379606 | | 58.84 | 11.46 | 3333.59 | 565.308 | 401.016 | 9.88543 | 1.76331 | 0.99 | 2.129548423407259 | | 58.84 | 11.46 | 3400.39 | 565.674 | 399.551 | 9.88613 | 1.76366 | 0.99 | 2.130100408285386 | | 58.84 | 11.46 | 3410.55 | 563.843 | 397.559 | 9.88684 | 1.764 | 0.99 | 2.1298757513510136 | | 58.84 | 11.46 | 3408.98 | 559.57 | 401.719 | 9.88755 | 1.76434 | 0.99 | 2.130438907683961 | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this, check out MindsDB documentation .","title":"Process Quality"},{"location":"sql/tutorials/process-quality/#pre-requisites","text":"Before you start make sure that you've: Visited Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. You can get it from Kaggle . Optional- Visual studio.","title":"Pre-requisites"},{"location":"sql/tutorials/process-quality/#manufacturing-process-quality","text":"Predicting process result quality is a common task in manufacturing analytics. Manufacturing plants commonly use quality predictions to gain a competitive edge over their competitors, improve their products or increase their customers satisfaction. MindsDB is a tool that can help you solve quality prediction tasks easily and effectively using machine learning. MindsDB abstracts ML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just using normal SQL commands. In this tutorial you will learn how to predict the quality of a mining process using MindsDB .","title":"Manufacturing process quality"},{"location":"sql/tutorials/process-quality/#upload-a-file","text":"Fix headers: sed -e 's/ /_/g' -e 's/\\(.*\\)/\\L\\1/' -e 's/%_//g' MiningProcess_Flotation_Plant_Database.csv > fixed_headers.csv (for Linux/Unix) edit headers manually: change space to underscore , upper case to lower case, remove % from headers (for Windows) Access MindsDB GUI either on cloud or local via URL 127.0.0.1:47334/. Select the Add data button or the plug icon on the left side bar. The page will navigate to the 'Select your data source' page. Select 'Files'. Under 'Import a file' select the tab 'Click here to browse local files' and select your data file. Once the file is uploaded 100%, provide a name for the data file that will be saved as a table. Select the button Save and Continue. You can query the file that has been uploaded to see that the data does pull through. SELECT * from files . file_name ;","title":"Upload a file"},{"location":"sql/tutorials/process-quality/#connect-to-mindsdb-sql-sever","text":"MindsDB's GUI has a SQL Editor that allows you to create and train predictors and also makes predictions. However you can also do this via the mysql client in a local terminal by accessing MindsDB SQL Server. mysql - h cloud . mindsdb . com --port 3306 -u username@email.com -p USE mindsdb ;","title":"Connect to MindsDB SQL Sever"},{"location":"sql/tutorials/process-quality/#create-a-predictor","text":"In this section you will connect to MindsDB with the MySql API and create a Predictor. It is in MindsDB terms a machine learning model, but all its complexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML expert and want to tweak the model, MindsDB also allows you that (please refer to documentation). Use the following query to create a Predictor that will foretell the silica_concentrate at the end of our mining process. The row number is limited to 5000 to speed up training but you can keep the whole dataset. CREATE PREDICTOR mindsdb . process_quality_predictor FROM files ( SELECT iron_feed , silica_feed , starch_flow , amina_flow , ore_pulp_flow , ore_pulp_ph , ore_pulp_density , flotation_column_01_air_flow , flotation_column_02_air_flow , flotation_column_03_air_flow , flotation_column_04_air_flow , flotation_column_05_air_flow , flotation_column_06_air_flow , flotation_column_07_air_flow , flotation_column_01_level , flotation_column_02_level , flotation_column_03_level , flotation_column_04_level , flotation_column_05_level , flotation_column_06_level , flotation_column_07_level , iron_concentrate , silica_concentrate from process_quality FROM process_quality LIMIT 5000 ) PREDICT silica_concentrate as quality USING ; After creating the Predictor you should see a similar output: Query OK, 0 rows affected (2 min 27.52 sec) Now the Predictor will begin training. You can check the status with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'process_quality_predictor' ; After the Predictor has finished training, you will see a similar output. +-----------------------------+----------+----------+--------------------+-------------------+------------------+ | name | status | accuracy | predict | select_data_query | training_options | +-----------------------------+----------+----------+--------------------+-------------------+------------------+ | process_quality_predictor | complete | 1 | silica_concentrate | | | +-----------------------------+----------+----------+--------------------+-------------------+------------------+ 1 row in set (0.28 sec) As you can see the accuracy of the model is 1 (i.e. 100%). This is the result of using a limited dataset of 5000 rows. In reality when using the whole dataset, you will probably see a more reasonable accuracy. You are now done with creating the predictor! \u2728","title":"Create a predictor"},{"location":"sql/tutorials/process-quality/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT silica_concentrate , silica_concentrate_confidence , silica_concentrate_explain FROM mindsdb . process_quality_predictor WHERE iron_feed = 48 . 81 AND silica_feed = 25 . 31 AND starch_flow = 2504 . 94 AND amina_flow = 309 . 448 AND ore_pulp_flow = 377 . 6511682692 AND ore_pulp_ph = 10 . 0607 AND ore_pulp_density = 1 . 68676 ; The output should look similar to this. +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | silica_concentrate | silica_concentrate_confidence | Info | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | 1.68 | 0.99 | {\"predicted_value\": \"1.68\", \"confidence\": 0.99, \"confidence_lower_bound\": null, \"confidence_upper_bound\": null, \"anomaly\": null, \"truth\": null} | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.81 sec) As you can see, the model predicted the silica concentrate for our data point. Again we can see a very high confidence due to the limited dataset. When making predictions you can include different fields. As you can notice, we have only included the first 7 fields of our dataset. You are free to test different combinations. In the previous example, we have made a prediction for a single data point. In a real scenario, you might want to make predictions on multiple data points. In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how to make batch predictions. Use the following command to create the batch prediction. SELECT collected_data . iron_feed , collected_data . silica_feed , collected_data . starch_flow , collected_data . amina_flow , collected_data . ore_pulp_flow , collected_data . ore_pulp_ph , collected_data . ore_pulp_density , predictions . silica_concentrate_confidence as confidence , predictions . silica_concentrate as predicted_silica_concentrate FROM process_quality_integration . process_quality AS collected_data JOIN mindsdb . process_quality_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | 58.84 | 11.46 | 3277.34 | 564.209 | 403.242 | 9.88472 | 1.76297 | 0.99 | 2.129567174379606 | | 58.84 | 11.46 | 3333.59 | 565.308 | 401.016 | 9.88543 | 1.76331 | 0.99 | 2.129548423407259 | | 58.84 | 11.46 | 3400.39 | 565.674 | 399.551 | 9.88613 | 1.76366 | 0.99 | 2.130100408285386 | | 58.84 | 11.46 | 3410.55 | 563.843 | 397.559 | 9.88684 | 1.764 | 0.99 | 2.1298757513510136 | | 58.84 | 11.46 | 3408.98 | 559.57 | 401.719 | 9.88755 | 1.76434 | 0.99 | 2.130438907683961 | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this, check out MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/spam_emails_tutorial/","text":"Communtiy Author: PWiederspan Pre-requisites \u00b6 This tutorial will be easier to follow if you have first read: - Getting Started Guide - Getting Started with Cloud And have downloaded this dataset. Filtering Spam Emails \u00b6 Anyone with an email address has experienced it, you open your inbox to find a random email promising a cash reward if you click now. Or maybe it\u2019s sneakier, a tracking link for a package you don\u2019t remember ordering. Spam emails are annoying for anyone, they clutter your inbox and send unnecessary push notifications, but they can be security risks as well. Whether a malicious tracking link in an official looking email, or a phishing scam asking you to confirm credentials, Spam emails are a serious concern for any IT department. Luckily, modern email clients have robust spam filters which remove potentially harmful emails from the inbox and label them as spam. Many major email providers use Machine Learning to analyze incoming emails and determine whether or not they are spam. Let's take a look at how MindsDB can be used to do this from a dataset of about 4600 emails. Upload the Dataset File \u00b6 If you are planning on using MindsDB to make predictions based on your own dataset, you probably have that data in a database, in which case you would want to follow the steps described in the Getting Started Guide , mentioned in the prerequisite section, to connect MindsDB to your database. For this tutorial we will be using the MindsDB cloud editor. If you haven\u2019t familiarized yourself with it, now is a great time to do so. From the main page in the editor, select the \u201cUpload File\u201d button in the top right corner. Then from the new page, browse local files and select the correct Spam Emails CSV, name it appropriately (in this tutorial we use spam_predict), and select the \u201cSave and Continue\u201d button. When the upload is complete it should take you back to the main editor page with something like this in the query box: --- MindsDB ships with a filesystem database called 'files' --- Each file you uploaded is saved as a table there. --- --- You can always check the list tables in files as follows: SHOW TABLES FROM files ; Selecting the \u201cRun\u201d button in the top left corner, or pressing Shift+Enter, will run the query and you should see a list of all files uploaded in the bottom section. Create A Predictor \u00b6 For this particular dataset each row represents a single email, with each column representing the frequency of a given word or symbol in that email. There are also columns for the total, average, and longest run of capital letters throughout the email. First we need to switch to using the mindsdb database. USE mindsdb ; We will start by training a model with the CREATE PREDICTOR command using all columns from the dataset we uploaded. CREATE PREDICTOR mindsdb . spam_predictor FROM files ( SELECT * FROM spam_predict ) PREDICT Spam ; We can check the status of the model by querying the name we used as the predictor. As this is a larger dataset, the status may show \u201cGenerating\u201d or \u201cTraining\u201d for a few minutes. Re-run the query to refresh. SELECT * FROM mindsdb . predictors WHERE name = 'spam_predictor' ; When the model has finished training the status will change tp \"complete\". You should see something similar to: name status accuracy predict update_status mindsdb_version error select_data_query training_options spam_predictor complete 0.9580387730450108 Spam up_to_date 22.4.2.1 null You have just created and trained the Machine Learning Model with approximately 96% accuracy! Make Predictions \u00b6 We will now begin to make predictions about whether a given email is spam using the model we just trained. The generic syntax to make predictions is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our prediction we will query the spam, spam_confidence, and spam_explain columns from the spam_predictor model we trained. To simulate a test email we will provide values for several in the WHERE clause representing various word frequencies and capital letter runs. SELECT spam , spam_confidence , spam_explain FROM mindsdb . spam_predictor WHERE word_freq_internet = 2 . 5 AND word_freq_email = 2 AND word_freq_credit = 0 . 9 AND word_freq_money = 0 . 9 AND word_freq_report = 1 . 2 AND word_freq_free = 1 . 2 AND word_freq_your = 0 . 9 AND word_freq_all = 2 . 4 AND capital_run_length_average = 20 AND word_freq_mail = 1 AND capital_run_total = 20 AND capital_run_longest = 20 ; When the query has run you should see an output similar to: spam spam_confidence spam_explain 1 0.956989247311828 {\"predicted_value\": \"1\", \"confidence\": 0.956989247311828, anomaly\": null, \"truth\": null} With this output we can see that MindsDB predicts that this is a spam email with approximately 96% accuracy. You can change these values and add additional word frequency columns to the query as you see fit to get different predictions. Batch Predictions \u00b6 In the example above, we saw how we can use MindsDB to determine if a single email is spam, however, in a lot of cases we want to determine if any email in an inbox is spam. We can do batch predictions using the JOIN command. By joining our spam_predictor table on a query of our dataset, we can see MindsDB prediction for each row alongside the actual spam value for each email. The generic syntax to do this is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); As we are using the original dataset, which contains 58 columns, we will only include a few of them. The important column for the purposes of this tutorial is the \u201cspam\u201d column for both tables, which we can use to check the predictions accuracy. SELECT t . word_freq_internet , t . word_freq_email , t . word_freq_credit , t . word_freq_money , t . word_freq_report , t . capital_run_length_longest , t . spam , p . spam AS predicted_spam FROM files . spam_predict as t JOIN mindsdb . spam_predictor as p ; This should return an output like the following: word_freq_internet word_freq_email word_freq_credit word_freq_money word_freq_report capital_run_length_longest spam spam_predictor 0 0 0 0 0 9989 1 1 0 0 0 0 0 99 1 1 0 0.21 0 0 0 99 1 1 0 0 0 0 0 99 1 1 0 0 0 0 0 99 1 1 With the ability to scroll through all 4600 rows, broken up 100 per page. You can now see predictions for a whole dataset! What we learned \u00b6 In this tutorial we learned how to : - Upload a dataset into MindsDB Cloud - Create a MindsDB Predictor - Train our Model using MindsDB - Make individual predictions by providing sample data - Make batch predictions by joining a Predictor with a Dataset","title":"Email spam"},{"location":"sql/tutorials/spam_emails_tutorial/#pre-requisites","text":"This tutorial will be easier to follow if you have first read: - Getting Started Guide - Getting Started with Cloud And have downloaded this dataset.","title":"Pre-requisites"},{"location":"sql/tutorials/spam_emails_tutorial/#filtering-spam-emails","text":"Anyone with an email address has experienced it, you open your inbox to find a random email promising a cash reward if you click now. Or maybe it\u2019s sneakier, a tracking link for a package you don\u2019t remember ordering. Spam emails are annoying for anyone, they clutter your inbox and send unnecessary push notifications, but they can be security risks as well. Whether a malicious tracking link in an official looking email, or a phishing scam asking you to confirm credentials, Spam emails are a serious concern for any IT department. Luckily, modern email clients have robust spam filters which remove potentially harmful emails from the inbox and label them as spam. Many major email providers use Machine Learning to analyze incoming emails and determine whether or not they are spam. Let's take a look at how MindsDB can be used to do this from a dataset of about 4600 emails.","title":"Filtering Spam Emails"},{"location":"sql/tutorials/spam_emails_tutorial/#upload-the-dataset-file","text":"If you are planning on using MindsDB to make predictions based on your own dataset, you probably have that data in a database, in which case you would want to follow the steps described in the Getting Started Guide , mentioned in the prerequisite section, to connect MindsDB to your database. For this tutorial we will be using the MindsDB cloud editor. If you haven\u2019t familiarized yourself with it, now is a great time to do so. From the main page in the editor, select the \u201cUpload File\u201d button in the top right corner. Then from the new page, browse local files and select the correct Spam Emails CSV, name it appropriately (in this tutorial we use spam_predict), and select the \u201cSave and Continue\u201d button. When the upload is complete it should take you back to the main editor page with something like this in the query box: --- MindsDB ships with a filesystem database called 'files' --- Each file you uploaded is saved as a table there. --- --- You can always check the list tables in files as follows: SHOW TABLES FROM files ; Selecting the \u201cRun\u201d button in the top left corner, or pressing Shift+Enter, will run the query and you should see a list of all files uploaded in the bottom section.","title":"Upload the Dataset File"},{"location":"sql/tutorials/spam_emails_tutorial/#create-a-predictor","text":"For this particular dataset each row represents a single email, with each column representing the frequency of a given word or symbol in that email. There are also columns for the total, average, and longest run of capital letters throughout the email. First we need to switch to using the mindsdb database. USE mindsdb ; We will start by training a model with the CREATE PREDICTOR command using all columns from the dataset we uploaded. CREATE PREDICTOR mindsdb . spam_predictor FROM files ( SELECT * FROM spam_predict ) PREDICT Spam ; We can check the status of the model by querying the name we used as the predictor. As this is a larger dataset, the status may show \u201cGenerating\u201d or \u201cTraining\u201d for a few minutes. Re-run the query to refresh. SELECT * FROM mindsdb . predictors WHERE name = 'spam_predictor' ; When the model has finished training the status will change tp \"complete\". You should see something similar to: name status accuracy predict update_status mindsdb_version error select_data_query training_options spam_predictor complete 0.9580387730450108 Spam up_to_date 22.4.2.1 null You have just created and trained the Machine Learning Model with approximately 96% accuracy!","title":"Create A Predictor"},{"location":"sql/tutorials/spam_emails_tutorial/#make-predictions","text":"We will now begin to make predictions about whether a given email is spam using the model we just trained. The generic syntax to make predictions is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our prediction we will query the spam, spam_confidence, and spam_explain columns from the spam_predictor model we trained. To simulate a test email we will provide values for several in the WHERE clause representing various word frequencies and capital letter runs. SELECT spam , spam_confidence , spam_explain FROM mindsdb . spam_predictor WHERE word_freq_internet = 2 . 5 AND word_freq_email = 2 AND word_freq_credit = 0 . 9 AND word_freq_money = 0 . 9 AND word_freq_report = 1 . 2 AND word_freq_free = 1 . 2 AND word_freq_your = 0 . 9 AND word_freq_all = 2 . 4 AND capital_run_length_average = 20 AND word_freq_mail = 1 AND capital_run_total = 20 AND capital_run_longest = 20 ; When the query has run you should see an output similar to: spam spam_confidence spam_explain 1 0.956989247311828 {\"predicted_value\": \"1\", \"confidence\": 0.956989247311828, anomaly\": null, \"truth\": null} With this output we can see that MindsDB predicts that this is a spam email with approximately 96% accuracy. You can change these values and add additional word frequency columns to the query as you see fit to get different predictions.","title":"Make Predictions"},{"location":"sql/tutorials/spam_emails_tutorial/#batch-predictions","text":"In the example above, we saw how we can use MindsDB to determine if a single email is spam, however, in a lot of cases we want to determine if any email in an inbox is spam. We can do batch predictions using the JOIN command. By joining our spam_predictor table on a query of our dataset, we can see MindsDB prediction for each row alongside the actual spam value for each email. The generic syntax to do this is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); As we are using the original dataset, which contains 58 columns, we will only include a few of them. The important column for the purposes of this tutorial is the \u201cspam\u201d column for both tables, which we can use to check the predictions accuracy. SELECT t . word_freq_internet , t . word_freq_email , t . word_freq_credit , t . word_freq_money , t . word_freq_report , t . capital_run_length_longest , t . spam , p . spam AS predicted_spam FROM files . spam_predict as t JOIN mindsdb . spam_predictor as p ; This should return an output like the following: word_freq_internet word_freq_email word_freq_credit word_freq_money word_freq_report capital_run_length_longest spam spam_predictor 0 0 0 0 0 9989 1 1 0 0 0 0 0 99 1 1 0 0.21 0 0 0 99 1 1 0 0 0 0 0 99 1 1 0 0 0 0 0 99 1 1 With the ability to scroll through all 4600 rows, broken up 100 per page. You can now see predictions for a whole dataset!","title":"Batch Predictions"},{"location":"sql/tutorials/spam_emails_tutorial/#what-we-learned","text":"In this tutorial we learned how to : - Upload a dataset into MindsDB Cloud - Create a MindsDB Predictor - Train our Model using MindsDB - Make individual predictions by providing sample data - Make batch predictions by joining a Predictor with a Dataset","title":"What we learned"}]}